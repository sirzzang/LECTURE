{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (4.5.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install GOT3 package\n",
    "!pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module import\n",
    "import GetOldTweets3 as got\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import urllib\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT 스태틱 메소드\n",
    "def getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "    \"\"\"\n",
    "    Invoke an HTTP query to Twitter.\n",
    "    Should not be used as an API function. A static method.\n",
    "    \"\"\"\n",
    "    url = \"https://twitter.com/i/search/timeline?\"\n",
    "\n",
    "    if not tweetCriteria.topTweets:\n",
    "        url += \"f=tweets&\"\n",
    "\n",
    "    url += (\"vertical=news&q=%s&src=typd&%s\"\n",
    "            \"&include_available_features=1&include_entities=1&max_position=%s\"\n",
    "            \"&reset_error_state=false\")\n",
    "\n",
    "    urlGetData = ''\n",
    "\n",
    "    if hasattr(tweetCriteria, 'querySearch'):\n",
    "        urlGetData += tweetCriteria.querySearch\n",
    "\n",
    "    if hasattr(tweetCriteria, 'excludeWords'):\n",
    "        urlGetData += ' -'.join([''] + tweetCriteria.excludeWords)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'username'):\n",
    "        if not hasattr(tweetCriteria.username, '__iter__'):\n",
    "            tweetCriteria.username = [tweetCriteria.username]\n",
    "\n",
    "        usernames_ = [u.lstrip('@') for u in tweetCriteria.username if u]\n",
    "        tweetCriteria.username = {u.lower() for u in usernames_ if u}\n",
    "\n",
    "        usernames = [' from:'+u for u in sorted(tweetCriteria.username)]\n",
    "        if usernames:\n",
    "            urlGetData += ' OR'.join(usernames)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'within'):\n",
    "        if hasattr(tweetCriteria, 'near'):\n",
    "            urlGetData += ' near:\"%s\" within:%s' % (tweetCriteria.near, tweetCriteria.within)\n",
    "        elif hasattr(tweetCriteria, 'lat') and hasattr(tweetCriteria, 'lon'):\n",
    "            urlGetData += ' geocode:%f,%f,%s' % (tweetCriteria.lat, tweetCriteria.lon, tweetCriteria.within)\n",
    "\n",
    "    if hasattr(tweetCriteria, 'since'):\n",
    "        urlGetData += ' since:' + tweetCriteria.since\n",
    "\n",
    "    if hasattr(tweetCriteria, 'until'):\n",
    "        urlGetData += ' until:' + tweetCriteria.until\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minReplies'):\n",
    "        urlGetData += ' min_replies:' + tweetCriteria.minReplies\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minFaves'):\n",
    "        urlGetData += ' min_faves:' + tweetCriteria.minFaves\n",
    "\n",
    "    if hasattr(tweetCriteria, 'minRetweets'):\n",
    "        urlGetData += ' min_retweets:' + tweetCriteria.minRetweets\n",
    "\n",
    "    if hasattr(tweetCriteria, 'lang'):\n",
    "        urlLang = 'l=' + tweetCriteria.lang + '&'\n",
    "    else:\n",
    "        urlLang = ''\n",
    "    url = url % (urllib.parse.quote(urlGetData.strip()), urlLang, urllib.parse.quote(refreshCursor))\n",
    "    useragent = useragent or TweetManager.user_agents[0]\n",
    "\n",
    "    headers = [\n",
    "        ('Host', \"twitter.com\"),\n",
    "        ('User-Agent', useragent),\n",
    "        ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "        ('Accept-Language', \"en-US,en;q=0.5\"),\n",
    "        ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "        ('Referer', url),\n",
    "        ('Connection', \"keep-alive\")\n",
    "    ]\n",
    "\n",
    "    if proxy:\n",
    "        opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    else:\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    opener.addheaders = headers\n",
    "\n",
    "    # 디버그 옵션 수정\n",
    "    if debug:\n",
    "        print(url)\n",
    "        \n",
    "    # HTTP Request 429 방지 위해 수정\n",
    "    time.sleep(3) \n",
    "\n",
    "    ##### 아래로 에러 핸들링 전부 수정: sys.exit() 대신 pass.\n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        jsonResponse = response.read()\n",
    "\n",
    "    except TimeoutError as e:\n",
    "        print(\"Timeout error\")\n",
    "        print(\"sleep 30\")\n",
    "        time.sleep(30)\n",
    "\n",
    "        # 한 번 더 시도하도록 수정\n",
    "        try:\n",
    "            response = opener.open(url)\n",
    "            jsonResponse = response.read()\n",
    "        except TimeoutError as e:\n",
    "            print(\"Timeout Error again.\")\n",
    "            print(\"Pass Data\")\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occured during an HTTP request:\", str(e))\n",
    "        print(\"Error URL:\", url)\n",
    "        print(\"Try to open in browser: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "        print(\"sleep 30\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        s_json = jsonResponse.decode()\n",
    "    except:\n",
    "        print(\"Invalid response from Twitter\")\n",
    "        print(\"Error URL:\", url)\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            dataJson = json.loads(s_json)\n",
    "        except:\n",
    "            print(\"Error parsing JSON: %s\" % s_json)\n",
    "            print(\"Error URL:\", url)\n",
    "        pass\n",
    "\n",
    "    return dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 에러\n",
    "class NotValidEndDateError(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__('마지막 검색 날짜를 다시 설정하십시오.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setUntil : 마지막 날짜 배제\n",
    "def set_crawl_date(start_date, end_date):\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(str(start_date), \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(str(end_date), \"%Y%m%d\") - datetime.timedelta(days=1)\n",
    "    \n",
    "    if end_date == start_date:\n",
    "        raise NotValidEndDateError\n",
    "    \n",
    "    else:   \n",
    "        print(\"트윗 수집 날짜 설정: {0}부터 {1}까지\".format(start_date, end_date))    \n",
    "        return start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 후 got tweet 객체 반환\n",
    "def crawl_tweets(start_date, end_date, query='Elon Musk', lang='en', debug=False):    \n",
    "    got.manager.TweetManager.getJsonResponse = getJsonResponse\n",
    "    \n",
    "    print(\"========== 트윗 수집 시작: {0} ~ {1} ==========\".format(start_date, end_date))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tweet_criteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                                .setSince(start_date)\\\n",
    "                                                .setUntil(end_date)\\\n",
    "                                                .setLang(lang)\n",
    "    tweets = got.manager.TweetManager.getTweets(tweet_criteria, debug=debug)\n",
    "    \n",
    "    elapsed_time = time.time()-start_time\n",
    "    print(\"수집 완료 : {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "    print(\"총 수집 트윗 개수 : {0}\".format(len(tweets))) \n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got tweet 객체로부터 결과 추출\n",
    "def get_results(tweet_data):\n",
    "    results = []\n",
    "    for tweet in tqdm(tweet_data):\n",
    "        results.append({'url': tweet.permalink,\n",
    "                        'date': tweet.date,\n",
    "                        'text': tweet.text,\n",
    "                        'user': tweet.username,\n",
    "                        'mentions': tweet.mentions,\n",
    "                        'retweets': tweet.retweets,\n",
    "                        'favorites': tweet.favorites,\n",
    "                        'hashtags': tweet.hashtags})\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 결과 저장\n",
    "def save_tweets(tweet_lists, base_file_dir=\"tweets\"):\n",
    "    \n",
    "    if not os.path.exists(base_file_dir):\n",
    "        os.makedirs(base_file_dir)\n",
    "        \n",
    "    with open(f\"{base_file_dir}/tweets_{crawl_start}_{crawl_end}.csv\", \"a\", -1, encoding=\"utf-8\") as f:    \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['url', 'date', 'text', 'user', 'mentions', 'retweets', 'favorites', 'hashtags'])        \n",
    "        for tweet_list in tqdm(tweet_lists):\n",
    "            writer.writerow(list(tweet_list.values()))\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트윗 수집 날짜 설정: 2017-06-01 00:00:00부터 2017-06-02 00:00:00까지\n",
      "========== 트윗 수집 시작: 2017-06-01 ~ 2017-06-02 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4329/4329 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4329/4329 [00:00<00:00, 276343.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집 완료 : 00:10:57\n",
      "총 수집 트윗 개수 : 4329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 진행\n",
    "crawl_start, crawl_end = set_crawl_date(20170601, 20170603)\n",
    "tweet_results = crawl_tweets(crawl_start, crawl_end)\n",
    "tweet_results_lists = get_results(tweet_results)\n",
    "save_tweets(tweet_results_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
