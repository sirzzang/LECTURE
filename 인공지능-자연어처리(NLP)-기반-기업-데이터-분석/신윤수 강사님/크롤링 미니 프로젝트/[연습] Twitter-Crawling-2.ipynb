{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from GetOldTweets3) (4.5.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\sir95\\anaconda3\\envs\\cpu_env\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install GOT3 package\n",
    "!pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module import\n",
    "import GetOldTweets3 as got\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import urllib\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2015-01-01', '2015-01-02')\n",
      "('2015-02-01', '2015-02-02')\n",
      "('2015-03-01', '2015-03-02')\n"
     ]
    }
   ],
   "source": [
    "dateRange = pd.date_range(start='20150101', end='20200601', freq='MS')\n",
    "for date in dateRange[:3]:\n",
    "    print(set_crawl_date(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트윗 수집 날짜 설정: 2015-01-01부터 2015-01-02까지\n",
      "2015-01-01 2015-01-02\n",
      "트윗 수집 날짜 설정: 2015-02-01부터 2015-02-02까지\n",
      "2015-02-01 2015-02-02\n",
      "트윗 수집 날짜 설정: 2015-03-01부터 2015-03-02까지\n",
      "2015-03-01 2015-03-02\n",
      "트윗 수집 날짜 설정: 2015-04-01부터 2015-04-02까지\n",
      "2015-04-01 2015-04-02\n",
      "트윗 수집 날짜 설정: 2015-05-01부터 2015-05-02까지\n",
      "2015-05-01 2015-05-02\n",
      "트윗 수집 날짜 설정: 2015-06-01부터 2015-06-02까지\n",
      "2015-06-01 2015-06-02\n",
      "트윗 수집 날짜 설정: 2015-07-01부터 2015-07-02까지\n",
      "2015-07-01 2015-07-02\n",
      "트윗 수집 날짜 설정: 2015-08-01부터 2015-08-02까지\n",
      "2015-08-01 2015-08-02\n",
      "트윗 수집 날짜 설정: 2015-09-01부터 2015-09-02까지\n",
      "2015-09-01 2015-09-02\n",
      "트윗 수집 날짜 설정: 2015-10-01부터 2015-10-02까지\n",
      "2015-10-01 2015-10-02\n",
      "트윗 수집 날짜 설정: 2015-11-01부터 2015-11-02까지\n",
      "2015-11-01 2015-11-02\n",
      "트윗 수집 날짜 설정: 2015-12-01부터 2015-12-02까지\n",
      "2015-12-01 2015-12-02\n",
      "트윗 수집 날짜 설정: 2016-01-01부터 2016-01-02까지\n",
      "2016-01-01 2016-01-02\n",
      "트윗 수집 날짜 설정: 2016-02-01부터 2016-02-02까지\n",
      "2016-02-01 2016-02-02\n",
      "트윗 수집 날짜 설정: 2016-03-01부터 2016-03-02까지\n",
      "2016-03-01 2016-03-02\n",
      "트윗 수집 날짜 설정: 2016-04-01부터 2016-04-02까지\n",
      "2016-04-01 2016-04-02\n",
      "트윗 수집 날짜 설정: 2016-05-01부터 2016-05-02까지\n",
      "2016-05-01 2016-05-02\n",
      "트윗 수집 날짜 설정: 2016-06-01부터 2016-06-02까지\n",
      "2016-06-01 2016-06-02\n",
      "트윗 수집 날짜 설정: 2016-07-01부터 2016-07-02까지\n",
      "2016-07-01 2016-07-02\n",
      "트윗 수집 날짜 설정: 2016-08-01부터 2016-08-02까지\n",
      "2016-08-01 2016-08-02\n",
      "트윗 수집 날짜 설정: 2016-09-01부터 2016-09-02까지\n",
      "2016-09-01 2016-09-02\n",
      "트윗 수집 날짜 설정: 2016-10-01부터 2016-10-02까지\n",
      "2016-10-01 2016-10-02\n",
      "트윗 수집 날짜 설정: 2016-11-01부터 2016-11-02까지\n",
      "2016-11-01 2016-11-02\n",
      "트윗 수집 날짜 설정: 2016-12-01부터 2016-12-02까지\n",
      "2016-12-01 2016-12-02\n",
      "트윗 수집 날짜 설정: 2017-01-01부터 2017-01-02까지\n",
      "2017-01-01 2017-01-02\n",
      "트윗 수집 날짜 설정: 2017-02-01부터 2017-02-02까지\n",
      "2017-02-01 2017-02-02\n",
      "트윗 수집 날짜 설정: 2017-03-01부터 2017-03-02까지\n",
      "2017-03-01 2017-03-02\n",
      "트윗 수집 날짜 설정: 2017-04-01부터 2017-04-02까지\n",
      "2017-04-01 2017-04-02\n",
      "트윗 수집 날짜 설정: 2017-05-01부터 2017-05-02까지\n",
      "2017-05-01 2017-05-02\n",
      "트윗 수집 날짜 설정: 2017-06-01부터 2017-06-02까지\n",
      "2017-06-01 2017-06-02\n",
      "트윗 수집 날짜 설정: 2017-07-01부터 2017-07-02까지\n",
      "2017-07-01 2017-07-02\n",
      "트윗 수집 날짜 설정: 2017-08-01부터 2017-08-02까지\n",
      "2017-08-01 2017-08-02\n",
      "트윗 수집 날짜 설정: 2017-09-01부터 2017-09-02까지\n",
      "2017-09-01 2017-09-02\n",
      "트윗 수집 날짜 설정: 2017-10-01부터 2017-10-02까지\n",
      "2017-10-01 2017-10-02\n",
      "트윗 수집 날짜 설정: 2017-11-01부터 2017-11-02까지\n",
      "2017-11-01 2017-11-02\n",
      "트윗 수집 날짜 설정: 2017-12-01부터 2017-12-02까지\n",
      "2017-12-01 2017-12-02\n",
      "트윗 수집 날짜 설정: 2018-01-01부터 2018-01-02까지\n",
      "2018-01-01 2018-01-02\n",
      "트윗 수집 날짜 설정: 2018-02-01부터 2018-02-02까지\n",
      "2018-02-01 2018-02-02\n",
      "트윗 수집 날짜 설정: 2018-03-01부터 2018-03-02까지\n",
      "2018-03-01 2018-03-02\n",
      "트윗 수집 날짜 설정: 2018-04-01부터 2018-04-02까지\n",
      "2018-04-01 2018-04-02\n",
      "트윗 수집 날짜 설정: 2018-05-01부터 2018-05-02까지\n",
      "2018-05-01 2018-05-02\n",
      "트윗 수집 날짜 설정: 2018-06-01부터 2018-06-02까지\n",
      "2018-06-01 2018-06-02\n",
      "트윗 수집 날짜 설정: 2018-07-01부터 2018-07-02까지\n",
      "2018-07-01 2018-07-02\n",
      "트윗 수집 날짜 설정: 2018-08-01부터 2018-08-02까지\n",
      "2018-08-01 2018-08-02\n",
      "트윗 수집 날짜 설정: 2018-09-01부터 2018-09-02까지\n",
      "2018-09-01 2018-09-02\n",
      "트윗 수집 날짜 설정: 2018-10-01부터 2018-10-02까지\n",
      "2018-10-01 2018-10-02\n",
      "트윗 수집 날짜 설정: 2018-11-01부터 2018-11-02까지\n",
      "2018-11-01 2018-11-02\n",
      "트윗 수집 날짜 설정: 2018-12-01부터 2018-12-02까지\n",
      "2018-12-01 2018-12-02\n",
      "트윗 수집 날짜 설정: 2019-01-01부터 2019-01-02까지\n",
      "2019-01-01 2019-01-02\n",
      "트윗 수집 날짜 설정: 2019-02-01부터 2019-02-02까지\n",
      "2019-02-01 2019-02-02\n",
      "트윗 수집 날짜 설정: 2019-03-01부터 2019-03-02까지\n",
      "2019-03-01 2019-03-02\n",
      "트윗 수집 날짜 설정: 2019-04-01부터 2019-04-02까지\n",
      "2019-04-01 2019-04-02\n",
      "트윗 수집 날짜 설정: 2019-05-01부터 2019-05-02까지\n",
      "2019-05-01 2019-05-02\n",
      "트윗 수집 날짜 설정: 2019-06-01부터 2019-06-02까지\n",
      "2019-06-01 2019-06-02\n",
      "트윗 수집 날짜 설정: 2019-07-01부터 2019-07-02까지\n",
      "2019-07-01 2019-07-02\n",
      "트윗 수집 날짜 설정: 2019-08-01부터 2019-08-02까지\n",
      "2019-08-01 2019-08-02\n",
      "트윗 수집 날짜 설정: 2019-09-01부터 2019-09-02까지\n",
      "2019-09-01 2019-09-02\n",
      "트윗 수집 날짜 설정: 2019-10-01부터 2019-10-02까지\n",
      "2019-10-01 2019-10-02\n",
      "트윗 수집 날짜 설정: 2019-11-01부터 2019-11-02까지\n",
      "2019-11-01 2019-11-02\n",
      "트윗 수집 날짜 설정: 2019-12-01부터 2019-12-02까지\n",
      "2019-12-01 2019-12-02\n",
      "트윗 수집 날짜 설정: 2020-01-01부터 2020-01-02까지\n",
      "2020-01-01 2020-01-02\n",
      "트윗 수집 날짜 설정: 2020-02-01부터 2020-02-02까지\n",
      "2020-02-01 2020-02-02\n",
      "트윗 수집 날짜 설정: 2020-03-01부터 2020-03-02까지\n",
      "2020-03-01 2020-03-02\n",
      "트윗 수집 날짜 설정: 2020-04-01부터 2020-04-02까지\n",
      "2020-04-01 2020-04-02\n",
      "트윗 수집 날짜 설정: 2020-05-01부터 2020-05-02까지\n",
      "2020-05-01 2020-05-02\n",
      "트윗 수집 날짜 설정: 2020-06-01부터 2020-06-02까지\n",
      "2020-06-01 2020-06-02\n"
     ]
    }
   ],
   "source": [
    "for date in dateRange:\n",
    "    crawl_start, crawl_end = set_crawl_date(date, 1)\n",
    "    print(crawl_start, crawl_end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setUntil : 마지막 날짜 배제\n",
    "def set_crawl_date(start_date, freq=1):    \n",
    "    end_date = start_date + datetime.timedelta(days=freq)\n",
    "    \n",
    "    # timestamp to string format\n",
    "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # check\n",
    "    print(\"트윗 수집 날짜 설정: {0}부터 {1}까지\".format(start_date, end_date))  \n",
    "    \n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT 스태틱 메소드 수정\n",
    "def getJsonResponse(tweetCriteria, refreshCursor, cookieJar, proxy, useragent=None, debug=False):\n",
    "    url = \"https://twitter.com/i/search/timeline?\"\n",
    "\n",
    "    if not tweetCriteria.topTweets:\n",
    "        url += \"f=tweets&\"\n",
    "\n",
    "    url += (\"vertical=news&q=%s&src=typd&%s\"\n",
    "            \"&include_available_features=1&include_entities=1&max_position=%s\"\n",
    "            \"&reset_error_state=false\")\n",
    "\n",
    "    urlGetData = ''\n",
    "    \n",
    "    # url + query search, since, since, until, lang    \n",
    "    urlGetData += tweetCriteria.querySearch\n",
    "    urlGetData += ' since:' + tweetCriteria.since\n",
    "    urlGetData += ' until:' + tweetCriteria.until\n",
    "    urlLang = 'l=' + tweetCriteria.lang + '&'\n",
    "    \n",
    "    # url 설정\n",
    "    url = url % (urllib.parse.quote(urlGetData.strip()), urlLang, urllib.parse.quote(refreshCursor))\n",
    "    useragent = useragent or TweetManager.user_agents[0]\n",
    "\n",
    "    headers = [\n",
    "        ('Host', \"twitter.com\"),\n",
    "        ('User-Agent', useragent),\n",
    "        ('Accept', \"application/json, text/javascript, */*; q=0.01\"),\n",
    "        ('Accept-Language', \"en-US,en;q=0.5\"),\n",
    "        ('X-Requested-With', \"XMLHttpRequest\"),\n",
    "        ('Referer', url),\n",
    "        ('Connection', \"keep-alive\")\n",
    "    ]\n",
    "\n",
    "    if proxy:\n",
    "        opener = urllib.request.build_opener(urllib.request.ProxyHandler({'http': proxy, 'https': proxy}), urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    else:\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookieJar))\n",
    "    opener.addheaders = headers        \n",
    "    \n",
    "    time.sleep(1) # HTTP Request 429 에러 방지.\n",
    "\n",
    "    ##################### 에러 핸들링 #####################\n",
    "    # TimeOut 에러\n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        jsonResponse = response.read()\n",
    "    except TimeoutError as e:\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)            \n",
    "        print(\"Timeout error\")\n",
    "        print(\"30초 정지\")\n",
    "        time.sleep(30)\n",
    "\n",
    "        # 한 번 더 시도\n",
    "        try:\n",
    "            response = opener.open(url)\n",
    "            jsonResponse = response.read()\n",
    "        except TimeoutError as e:\n",
    "            print(\"Timeout error again. 패스.\")\n",
    "            pass\n",
    "\n",
    "    # UrlParse 에러: 다시 시도 X\n",
    "    except Exception as e:\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)\n",
    "            \n",
    "        print(\"HTTP 요청 오류\", str(e))        \n",
    "        print(\"브라우저 오픈: https://twitter.com/search?q=%s&src=typd\" % urllib.parse.quote(urlGetData))\n",
    "        print(\"30초 정지\")\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Json 데이터 오류\n",
    "    try:\n",
    "        s_json = jsonResponse.decode()\n",
    "    except:\n",
    "        print(\"올바르지 못한 응답\")\n",
    "        if debug: # 디버그 옵션\n",
    "            print(\"에러 url 주소:\", url)\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            dataJson = json.loads(s_json)\n",
    "        except: # json 데이터 파싱 오류\n",
    "            if debug: # 디버그 옵션\n",
    "                print(\"에러 url 주소:\", url)                \n",
    "            print(\"JSON: %s\" % s_json)\n",
    "        pass\n",
    "\n",
    "    return dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 에러\n",
    "class NotValidEndDateError(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__('마지막 검색 날짜를 다시 설정하십시오.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setUntil : 마지막 날짜 배제\n",
    "\n",
    "\n",
    "def set_crawl_date(start_date, freq):\n",
    "    start_date = datetime.datetime.strptime(str(start_date), \"\")\n",
    "\n",
    "# def set_crawl_date(start_date, end_date):\n",
    "    \n",
    "#     start_date = datetime.datetime.strptime(str(start_date), \"%Y%m%d\")\n",
    "#     end_date = datetime.datetime.strptime(str(end_date), \"%Y%m%d\") - datetime.timedelta(days=1)\n",
    "    \n",
    "#     if end_date == start_date:\n",
    "#         raise NotValidEndDateError\n",
    "    \n",
    "#     else:   \n",
    "#         print(\"트윗 수집 날짜 설정: {0}부터 {1}까지\".format(start_date, end_date))    \n",
    "#         return start_date.strftime(\"%Y-%m-%d\"), end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got tweet 객체로부터 결과 추출\n",
    "def get_results(tweet_data):\n",
    "    results = []\n",
    "    for tweet in tqdm(tweet_data):\n",
    "        results.append({'url': tweet.permalink,\n",
    "                        'date': tweet.date,\n",
    "                        'text': tweet.text,\n",
    "                        'user': tweet.username,\n",
    "                        'mentions': tweet.mentions,\n",
    "                        'retweets': tweet.retweets,\n",
    "                        'favorites': tweet.favorites,\n",
    "                        'hashtags': tweet.hashtags})\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 결과 저장\n",
    "def save_tweets(tweet_lists, base_file_dir=\"tweets\"):\n",
    "    \n",
    "    if not os.path.exists(base_file_dir):\n",
    "        os.makedirs(base_file_dir)\n",
    "        \n",
    "    with open(f\"{base_file_dir}/tweets_{crawl_start}_{crawl_end}.csv\", \"a\", -1, encoding=\"utf-8\") as f:    \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['url', 'date', 'text', 'user', 'mentions', 'retweets', 'favorites', 'hashtags'])        \n",
    "        for tweet_list in tqdm(tweet_lists):\n",
    "            writer.writerow(list(tweet_list.values()))\n",
    "            \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트윗 수집 날짜 설정: 2015-06-01 00:00:00부터 2015-06-02 00:00:00까지\n",
      "========== 트윗 수집 시작: 2015-06-01 ~ 2015-06-02 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 2054/2054 [00:00<00:00, 347215.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2054/2054 [00:00<00:00, 85733.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집 완료 : 00:02:37\n",
      "총 수집 트윗 개수 : 2054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 진행\n",
    "crawl_start, crawl_end = set_crawl_date(20150601, 20150603)\n",
    "tweet_results = crawl_tweets(crawl_start, crawl_end)\n",
    "tweet_results_lists = get_results(tweet_results)\n",
    "save_tweets(tweet_results_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "class AbstractCrawler(object):\n",
    "    \n",
    "    def __init__(self, url=None, default_headers= None, request_class=None, parser_class=None, config=None, **kwargs):\n",
    "        self.base_url = url\n",
    "    \n",
    "    def _crawl(self, param=None, headers=None):\n",
    "        url = self.make_url(self.base_url, param)\n",
    "        header = self.__make_headers(headers)\n",
    "        resp = request_class(url, header=header).get_data()\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def crawl(self, param, headers):\n",
    "        return self._crawl(param, headers)\n",
    "    \n",
    "    def __make_headers(self,headers):\n",
    "        return self.default_headers.update(headers)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def __make_url(self, base_url, params):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def run(self):\n",
    "        data = self.crawl()\n",
    "        self.save(data)\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def save(self):\n",
    "        # db handling\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
