# -*- coding: utf-8 -*-
"""NLP-IMDB-Kaggle1st-KerasEmbedding-practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GJlWPi05LdN8wUYs-bSxfYZUH33-pgrb
"""

# 모듈 불러 오기
import nltk
nltk.download('stopwords')
nltk.download('punkt')

import numpy as np
import pandas as pd
from collections import Counter
import re
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import math

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import word_tokenize
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout
from tensorflow.keras.layers import Conv1D, GlobalMaxPool1D
from tensorflow.keras.layers import LSTM, Bidirectional
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K

import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

# 경로 설정
root_path = "/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님"
data_path = f"{root_path}/dataset"

# 데이터 로드 및 원본 보존
df_raw = pd.read_csv(f"{data_path}/4-1.labeledTrainData.tsv",
                     header=0,
                     delimiter='\t',
                     quoting=3)
display(df_raw)

# 전처리
def clean_text(x):
    x = BeautifulSoup(x, 'lxml').get_text() # HTML 태그 제거
    x = re.sub("[^a-zA-Z]", " ", x) # 영어 제외 바꾸기
    x = x.lower() # 소문자
    x = [w for w in x.split() if not w in stopwords.words('english')] # 불용어 제거
    x = [PorterStemmer().stem(word) for word in x] # 포터 스테밍으로 어간 추출
    return " ".join(x)

df_raw['text'] = df_raw['review'].apply(lambda x: clean_text(x))
df = df_raw[['text', 'sentiment']]
display(df)

# 긍정 sentiment와 부정 sentiment로 나누기
df_pos = df[df['sentiment'] == 1]
df_neg = df[df['sentiment'] == 0]

# MI 점수 기반 어휘집 생성
def build_vocab(df):
    vocabulary = Counter()

    for text in df['text']:
        for word in list(set(word_tokenize(text))): # set으로 중복 단어 제거
            vocabulary[word] += 1    
    return vocabulary.most_common()

def calc_MI(pos_df, neg_df):

    pos_vocab = build_vocab(pos_df)
    neg_vocab = build_vocab(neg_df)
    
    merge_vocab = {}
    for k, v in pos_vocab:
        merge_vocab[k] = [v, 0]
    for k, v in neg_vocab:
        if k in merge_vocab:
            merge_vocab[k][1] = v
        else:
            merge_vocab[k] = [0, v]
    
    # MI 계산
    clip = 0.00001
    for k, v in merge_vocab.items():
        merge_vocab[k] = (v[0] / (len(pos_df)+len(neg_df))) * math.log(2*(v[0]+clip) / (v[0] + v[1])) + \
                        (v[1] / (len(pos_df)+len(neg_df))) * math.log(2*(v[1]+clip) / (v[0] + v[1]))
    return {k:v for k, v in sorted(merge_vocab.items(), key=lambda x:x[1], reverse=True)}

vocabulary = calc_MI(df_pos, df_neg)
print(f"총 어휘집 크기: {len(vocabulary)}")

# 상위 50퍼센트만 남기기 + OOV 버리기
word2idx = {k:i+1 for i, (k, v) in enumerate(vocabulary.items()) if i <= len(vocabulary)//2}
# word2idx['OOV'] = 0
idx2word = {v:k for k, v in word2idx.items()}

# 토크나이징
print(f"기존 문장 개수: {len(df['text'])}")
tokens = []
for idx, text in enumerate(df['text']):
    temp = []
    delete_indices = []

    for word in text.split():
        try:
            temp.append(word2idx[word])
        except KeyError: # OOV
            continue 
    
    if len(temp) > 0:
        tokens.append(temp)
    else:
        delete_indices.append(idx)
print(f"OOV 없는 문장으로만 남긴 개수: {len(tokens)}")
print(f"삭제해야 할 문장 인덱스: {delete_indices}")

# 문장 패딩
# def check_len(threshold, sentences):
#     cnt = 0
#     for sent in sentences:
#         if len(sent) <= threshold:
#             cnt += 1
    
#     return f'전체 문장 중 길이가 {threshold} 이하인 샘플의 비율: {(cnt/len(sentences))*100}'

# for i in range(10, 300, 10):
#     print(check_len(i, tokens))

MAX_LENGTH = int(input('문장 최대 길이 설정: '))
X_train = pad_sequences(tokens, 
                        maxlen=MAX_LENGTH,
                        padding='post',
                        truncating='post')

X_train = np.array(X_train)
print(f"패딩 후 train data: {X_train.shape}")
print(X_train[0])
print(f"샘플 문장 길이: {len(X_train[0])}")

# 라벨 가져오기
y_train = df['sentiment']
y_train = np.array(y_train)
print(f"train label: {y_train.shape}")

# 트레인 테스트 스플릿
TEST_SPLIT = 0.1
RANDOM_SEED = 42

train_input, test_input, train_label, test_label = train_test_split(X_train, y_train, 
                                                                    test_size=TEST_SPLIT, 
                                                                    random_state=RANDOM_SEED)
# CNN 모델을 빌드한다.
VOCAB_SIZE = X_train.max() + 1
EMB_SIZE = 32
NUM_FILTER = 64

inputX = Input(batch_shape=(None, train_input.shape[1]))
emb = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)(inputX)
emb = Dropout(rate=0.5)(emb)
conv1 = Conv1D(filters=NUM_FILTER, kernel_size=3, activation='relu')(emb)
pool1 = GlobalMaxPool1D()(conv1)

conv2 = Conv1D(filters=NUM_FILTER, kernel_size=4, activation='relu')(emb)
pool2 = GlobalMaxPool1D()(conv2)

conv3 = Conv1D(filters=NUM_FILTER, kernel_size=5, activation='relu')(emb)
pool3 = GlobalMaxPool1D()(conv3)
concat = Concatenate()([pool1, pool2, pool3])

hidden = Dense(64, activation='relu')(concat)
hidden = Dropout(rate=0.5)(hidden)
outputY = Dense(1, activation='sigmoid')(hidden)

model = Model(inputX, outputY)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005))
model.summary()

# 학습
es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
hist = model.fit(train_input, train_label,
                 validation_data = (test_input, test_label),
                 batch_size = 500, 
                 epochs = 30,
                 callbacks=[es])

# Loss history를 그린다
plt.plot(hist.history['loss'], label='Train loss')
plt.plot(hist.history['val_loss'], label = 'Test loss')
plt.legend()
plt.title("Loss history")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

# 시험 데이터로 학습 성능을 평가한다
predicted = model.predict(test_input)
test_pred = np.where(predicted > 0.5, 1, 0)
accuracy = (test_label.reshape(-1, 1) == test_pred).mean()
print("\nAccuracy = %.2f %s" % (accuracy * 100, '%'))
print("WnRocAuc Score = %.4f" % (roc_auc_score(test_label, test_pred)))