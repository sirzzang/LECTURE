{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Seq2Seq + Attention-Chatbot-train-practice",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raumPUyOCxDz",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq + Attention Train\n",
        "\n",
        "> 2. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVfXiwPz7Dx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈 불러오기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed\n",
        "from tensorflow.keras.layers import Permute, Activation, Dot, Concatenate, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils import model_to_dot"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBE5E_jr7o4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 경로 설정\n",
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/dataset\"\n",
        "chatbot_path = f\"{root_path}/Seq2Seq-Chatbot\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3vDzkvP8I7X",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U_VEeKM7vH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 어휘집 사전\n",
        "with open(f\"{data_path}/6-1.vocabulary.pickle\", 'rb') as f:\n",
        "    word2idx, idx2word = pickle.load(f)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF_7BuCj8MtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 데이터: 인코딩, 디코딩 입력, 디코딩 출력\n",
        "with open(f\"{data_path}/6-1.train_data.pickle\", 'rb') as f:\n",
        "    X_train_E, X_train_D, y_train_D = pickle.load(f)\n",
        "\n",
        "# 테스트 데이터: 인코딩, 디코딩 입력, 디코딩 출력\n",
        "with open(f\"{data_path}/6-1.eval_data.pickle\", 'rb') as f:\n",
        "    X_test_E, X_test_D, y_test_D = pickle.load(f)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYXXgh_c8nrN",
        "colab_type": "text"
      },
      "source": [
        "## 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpKTDjpIX7tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Attention(x, y):\n",
        "    '''\n",
        "    x: encoder 출력\n",
        "    y: decoder 출력\n",
        "    return: concatenated tensors\n",
        "    '''\n",
        "    # 1. dot: decoder 매 시점마다 encoder 전체 시점과 dot-product.\n",
        "    score = Dot(axes=(2,2))([y,x])\n",
        "    # 2. softmax\n",
        "    dist = Activation('softmax')(score)\n",
        "    # 3. 가중평균\n",
        "    score = Dot(axes=(3, 1))([tf.expand_dims(dist, 0), x])\n",
        "    # 4. sum\n",
        "    score = tf.reduce_sum(score, axis=1)\n",
        "    \n",
        "    return Concatenate()([score, y]) # 5. concat"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBTshUqA8j6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "73d933a8-4e33-4f40-d0b7-5d04098a1a67"
      },
      "source": [
        "# 모델 파라미터 설정\n",
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = int(input('임베딩 출력 차원 설정: '))\n",
        "LSTM_HIDDEN = int(input('LSTM 은닉 노드 수 설정: '))\n",
        "MODEL_PATH = f\"{chatbot_path}/Seq2Seq_attention.h5\""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "임베딩 출력 차원 설정: 128\n",
            "LSTM 은닉 노드 수 설정: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZwNFaJC9aQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 세션 클리어\n",
        "K.clear_session()\n",
        "\n",
        "# 공통 임베딩 레이어\n",
        "wordEmbedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVfC0Ih79Kip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder 네트워크 구성\n",
        "E_input = Input(batch_shape=(None, X_train_E.shape[1]))\n",
        "E_embed = wordEmbedding(E_input)\n",
        "E_lstm1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "E_lstm2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "ey1, eh1, ec1 = E_lstm1(E_embed)\n",
        "ey2, eh2, ec2 = E_lstm2(ey1) # 1층의 출력을 받는다"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLTk9GCp-AV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder 네트워크 구성\n",
        "D_input = Input(batch_shape=(None, X_train_D.shape[1]))\n",
        "D_embed = wordEmbedding(D_input)\n",
        "D_lstm1 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "D_lstm2 = LSTM(LSTM_HIDDEN, return_sequences=True, return_state=True)\n",
        "dy1, dh1, dc1 = D_lstm1(D_embed, initial_state=[eh1, ec1]) # 초기 상태 설정\n",
        "dy2, dh2, dc2 = D_lstm2(dy1, initial_state=[eh2, ec2]) # 초기 상태 설정"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4lWNFDXLH7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention 네트워크 구성\n",
        "attention = Attention(ey2, dy2)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek-KsBR7aEei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 출력 네트워크\n",
        "y_output = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax')) # 시간 분배\n",
        "y_output = y_output(attention)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nieb_fTIahlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "698f4d84-a1f3-4d2a-9a6c-7ec58f535a73"
      },
      "source": [
        "# 모델 구성\n",
        "model = Model([E_input, D_input], y_output)\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss='sparse_categorical_crossentropy')\n",
        "print(\"========== 모델 전체 구조 확인 ==========\")\n",
        "print(model.summary())\n",
        "SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== 모델 전체 구조 확인 ==========\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 10, 128)      2650240     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 10, 128), (N 131584      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 10, 128), (N 131584      embedding[1][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 10, 128), (N 131584      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 10, 128), (N 131584      lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 10, 10)       0           lstm_3[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 10)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(1, None, 10, 10)]  0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, None, 10, 128 0           tf_op_layer_ExpandDims[0][0]     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum (TensorFlowOpLa [(None, 10, 128)]    0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 256)      0           tf_op_layer_Sum[0][0]            \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 10, 20705)    5321185     concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 8,497,761\n",
            "Trainable params: 8,497,761\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"874pt\" viewBox=\"0.00 0.00 794.50 968.00\" width=\"717pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 964)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-964 790.5,-964 790.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140623844342696 -->\n<g class=\"node\" id=\"node1\">\n<title>140623844342696</title>\n<polygon fill=\"none\" points=\"67.5,-913.5 67.5,-959.5 321.5,-959.5 321.5,-913.5 67.5,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-932.8\">input_2: InputLayer</text>\n<polyline fill=\"none\" points=\"200.5,-913.5 200.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"229.5\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"200.5,-936.5 258.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"229.5\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"258.5,-913.5 258.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-944.3\">[(?, 10)]</text>\n<polyline fill=\"none\" points=\"258.5,-936.5 321.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-921.3\">[(?, 10)]</text>\n</g>\n<!-- 140623855483032 -->\n<g class=\"node\" id=\"node3\">\n<title>140623855483032</title>\n<polygon fill=\"none\" points=\"181.5,-830.5 181.5,-876.5 479.5,-876.5 479.5,-830.5 181.5,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259.5\" y=\"-849.8\">embedding: Embedding</text>\n<polyline fill=\"none\" points=\"337.5,-830.5 337.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"337.5,-853.5 395.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"395.5,-830.5 395.5,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"437.5\" y=\"-861.3\">(?, 10)</text>\n<polyline fill=\"none\" points=\"395.5,-853.5 479.5,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"437.5\" y=\"-838.3\">(?, 10, 128)</text>\n</g>\n<!-- 140623844342696&#45;&gt;140623855483032 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140623844342696-&gt;140623855483032</title>\n<path d=\"M232.3836,-913.3799C248.3128,-903.6583 267.0052,-892.2505 283.7771,-882.0147\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"285.6353,-884.9811 292.3479,-876.784 281.9886,-879.0059 285.6353,-884.9811\" stroke=\"#000000\"/>\n</g>\n<!-- 140623855948968 -->\n<g class=\"node\" id=\"node2\">\n<title>140623855948968</title>\n<polygon fill=\"none\" points=\"339.5,-913.5 339.5,-959.5 593.5,-959.5 593.5,-913.5 339.5,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"406\" y=\"-932.8\">input_1: InputLayer</text>\n<polyline fill=\"none\" points=\"472.5,-913.5 472.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"501.5\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"472.5,-936.5 530.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"501.5\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"530.5,-913.5 530.5,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-944.3\">[(?, 10)]</text>\n<polyline fill=\"none\" points=\"530.5,-936.5 593.5,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-921.3\">[(?, 10)]</text>\n</g>\n<!-- 140623855948968&#45;&gt;140623855483032 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140623855948968-&gt;140623855483032</title>\n<path d=\"M428.6164,-913.3799C412.6872,-903.6583 393.9948,-892.2505 377.2229,-882.0147\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"379.0114,-879.0059 368.6521,-876.784 375.3647,-884.9811 379.0114,-879.0059\" stroke=\"#000000\"/>\n</g>\n<!-- 140622081676344 -->\n<g class=\"node\" id=\"node4\">\n<title>140622081676344</title>\n<polygon fill=\"none\" points=\"259,-747.5 259,-793.5 602,-793.5 602,-747.5 259,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302.5\" y=\"-766.8\">lstm: LSTM</text>\n<polyline fill=\"none\" points=\"346,-747.5 346,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"346,-770.5 404,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"404,-747.5 404,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503\" y=\"-778.3\">(?, 10, 128)</text>\n<polyline fill=\"none\" points=\"404,-770.5 602,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503\" y=\"-755.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n</g>\n<!-- 140623855483032&#45;&gt;140622081676344 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140623855483032-&gt;140622081676344</title>\n<path d=\"M358.3556,-830.3799C369.531,-821.1043 382.556,-810.2936 394.4392,-800.4304\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"396.9875,-802.8639 402.447,-793.784 392.5168,-797.4775 396.9875,-802.8639\" stroke=\"#000000\"/>\n</g>\n<!-- 140623851457448 -->\n<g class=\"node\" id=\"node5\">\n<title>140623851457448</title>\n<polygon fill=\"none\" points=\"52.5,-664.5 52.5,-710.5 410.5,-710.5 410.5,-664.5 52.5,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-683.8\">lstm_2: LSTM</text>\n<polyline fill=\"none\" points=\"154.5,-664.5 154.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"154.5,-687.5 212.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"212.5,-664.5 212.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311.5\" y=\"-695.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n<polyline fill=\"none\" points=\"212.5,-687.5 410.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311.5\" y=\"-672.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n</g>\n<!-- 140623855483032&#45;&gt;140623851457448 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140623855483032-&gt;140623851457448</title>\n<path d=\"M287.3098,-830.2567C273.5205,-820.7738 259.5141,-808.5736 250.5,-794 237.0004,-772.1743 232.4987,-743.3067 231.2356,-721.1324\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"234.7222,-720.7067 230.8504,-710.8447 227.7271,-720.9687 234.7222,-720.7067\" stroke=\"#000000\"/>\n</g>\n<!-- 140622081676344&#45;&gt;140623851457448 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140622081676344-&gt;140623851457448</title>\n<path d=\"M375.3317,-747.4901C350.745,-737.2353 321.6188,-725.0872 296.0287,-714.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"297.3524,-711.1739 286.7756,-710.5547 294.6577,-717.6345 297.3524,-711.1739\" stroke=\"#000000\"/>\n</g>\n<!-- 140622082243328 -->\n<g class=\"node\" id=\"node6\">\n<title>140622082243328</title>\n<polygon fill=\"none\" points=\"428.5,-664.5 428.5,-710.5 786.5,-710.5 786.5,-664.5 428.5,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479.5\" y=\"-683.8\">lstm_1: LSTM</text>\n<polyline fill=\"none\" points=\"530.5,-664.5 530.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559.5\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"530.5,-687.5 588.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559.5\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"588.5,-664.5 588.5,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"687.5\" y=\"-695.3\">(?, 10, 128)</text>\n<polyline fill=\"none\" points=\"588.5,-687.5 786.5,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"687.5\" y=\"-672.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n</g>\n<!-- 140622081676344&#45;&gt;140622082243328 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140622081676344-&gt;140622082243328</title>\n<path d=\"M479.8044,-747.3799C501.2914,-737.304 526.6414,-725.4167 549.068,-714.9003\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"550.5854,-718.0545 558.1534,-710.6399 547.6134,-711.7168 550.5854,-718.0545\" stroke=\"#000000\"/>\n</g>\n<!-- 140623851454872 -->\n<g class=\"node\" id=\"node7\">\n<title>140623851454872</title>\n<polygon fill=\"none\" points=\"113.5,-581.5 113.5,-627.5 471.5,-627.5 471.5,-581.5 113.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-600.8\">lstm_3: LSTM</text>\n<polyline fill=\"none\" points=\"215.5,-581.5 215.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"215.5,-604.5 273.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"273.5,-581.5 273.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-612.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n<polyline fill=\"none\" points=\"273.5,-604.5 471.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-589.3\">[(?, 10, 128), (?, 128), (?, 128)]</text>\n</g>\n<!-- 140623851457448&#45;&gt;140623851454872 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140623851457448-&gt;140623851454872</title>\n<path d=\"M248.4919,-664.3799C254.9812,-655.5502 262.4928,-645.3295 269.4528,-635.8593\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"272.2858,-637.9146 275.3877,-627.784 266.6453,-633.7692 272.2858,-637.9146\" stroke=\"#000000\"/>\n</g>\n<!-- 140622082243328&#45;&gt;140623851454872 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140622082243328-&gt;140623851454872</title>\n<path d=\"M520.1733,-664.4901C479.7184,-653.8306 431.4994,-641.1252 389.8734,-630.1571\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"390.5584,-626.7182 379.9966,-627.5547 388.7748,-633.4872 390.5584,-626.7182\" stroke=\"#000000\"/>\n</g>\n<!-- 140623855484264 -->\n<g class=\"node\" id=\"node8\">\n<title>140623855484264</title>\n<polygon fill=\"none\" points=\"208,-498.5 208,-544.5 499,-544.5 499,-498.5 208,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.5\" y=\"-517.8\">dot: Dot</text>\n<polyline fill=\"none\" points=\"273,-498.5 273,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"273,-521.5 331,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"331,-498.5 331,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415\" y=\"-529.3\">[(?, 10, 128), (?, 10, 128)]</text>\n<polyline fill=\"none\" points=\"331,-521.5 499,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415\" y=\"-506.3\">(?, 10, 10)</text>\n</g>\n<!-- 140622082243328&#45;&gt;140623855484264 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140622082243328-&gt;140623855484264</title>\n<path d=\"M613.1702,-664.2201C616.1402,-646.7303 617.3746,-622.7498 607.5,-604.5\" fill=\"none\" stroke=\"#000000\"/>\n<path d=\"M607.5,-604.5C594.5094,-580.4912 539.7853,-561.0161 484.4431,-546.9801\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"485.0251,-543.5186 474.4768,-544.5109 483.3416,-550.3131 485.0251,-543.5186\" stroke=\"#000000\"/>\n</g>\n<!-- 140623860943840 -->\n<g class=\"node\" id=\"node11\">\n<title>140623860943840</title>\n<polygon fill=\"none\" points=\"196.5,-249.5 196.5,-295.5 510.5,-295.5 510.5,-249.5 196.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-268.8\">dot_1: Dot</text>\n<polyline fill=\"none\" points=\"276.5,-249.5 276.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"276.5,-272.5 334.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"334.5,-249.5 334.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422.5\" y=\"-280.3\">[(1, ?, 10, 10), (?, 10, 128)]</text>\n<polyline fill=\"none\" points=\"334.5,-272.5 510.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422.5\" y=\"-257.3\">(?, ?, 10, 128)</text>\n</g>\n<!-- 140622082243328&#45;&gt;140623860943840 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140622082243328-&gt;140623860943840</title>\n<path d=\"M607.5,-604.5C549.7254,-497.7231 666.3592,-425.1512 588.5,-332 576.8738,-318.0903 546.4591,-306.8043 511.2181,-297.9512\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"511.7046,-294.468 501.162,-295.5186 510.0587,-301.2718 511.7046,-294.468\" stroke=\"#000000\"/>\n</g>\n<!-- 140623851454872&#45;&gt;140623855484264 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140623851454872-&gt;140623855484264</title>\n<path d=\"M309.4919,-581.3799C315.9812,-572.5502 323.4928,-562.3295 330.4528,-552.8593\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"333.2858,-554.9146 336.3877,-544.784 327.6453,-550.7692 333.2858,-554.9146\" stroke=\"#000000\"/>\n</g>\n<!-- 140623860337800 -->\n<g class=\"node\" id=\"node13\">\n<title>140623860337800</title>\n<polygon fill=\"none\" points=\"33.5,-83.5 33.5,-129.5 419.5,-129.5 419.5,-83.5 33.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113.5\" y=\"-102.8\">concatenate: Concatenate</text>\n<polyline fill=\"none\" points=\"193.5,-83.5 193.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"193.5,-106.5 251.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"251.5,-83.5 251.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-114.3\">[(?, 10, 128), (?, 10, 128)]</text>\n<polyline fill=\"none\" points=\"251.5,-106.5 419.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335.5\" y=\"-91.3\">(?, 10, 256)</text>\n</g>\n<!-- 140623851454872&#45;&gt;140623860337800 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140623851454872-&gt;140623860337800</title>\n<path d=\"M230.6372,-581.4497C174.1855,-555.9361 99.5,-508.8819 99.5,-438.5 99.5,-438.5 99.5,-438.5 99.5,-272.5 99.5,-223.0762 101.5277,-205.2985 131.5,-166 140.7939,-153.8142 153.3596,-143.4952 166.3046,-135.0475\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"168.1758,-138.0056 174.8356,-129.7656 164.4908,-132.054 168.1758,-138.0056\" stroke=\"#000000\"/>\n</g>\n<!-- 140622081863576 -->\n<g class=\"node\" id=\"node9\">\n<title>140622081863576</title>\n<polygon fill=\"none\" points=\"216.5,-415.5 216.5,-461.5 490.5,-461.5 490.5,-415.5 216.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-434.8\">activation: Activation</text>\n<polyline fill=\"none\" points=\"355.5,-415.5 355.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"355.5,-438.5 413.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"413.5,-415.5 413.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-446.3\">(?, 10, 10)</text>\n<polyline fill=\"none\" points=\"413.5,-438.5 490.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-423.3\">(?, 10, 10)</text>\n</g>\n<!-- 140623855484264&#45;&gt;140622081863576 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140623855484264-&gt;140622081863576</title>\n<path d=\"M353.5,-498.3799C353.5,-490.1745 353.5,-480.7679 353.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"357.0001,-471.784 353.5,-461.784 350.0001,-471.784 357.0001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140625830276008 -->\n<g class=\"node\" id=\"node10\">\n<title>140625830276008</title>\n<polygon fill=\"none\" points=\"127.5,-332.5 127.5,-378.5 579.5,-378.5 579.5,-332.5 127.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274\" y=\"-351.8\">tf_op_layer_ExpandDims: TensorFlowOpLayer</text>\n<polyline fill=\"none\" points=\"420.5,-332.5 420.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"449.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"420.5,-355.5 478.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"449.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"478.5,-332.5 478.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529\" y=\"-363.3\">(?, 10, 10)</text>\n<polyline fill=\"none\" points=\"478.5,-355.5 579.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529\" y=\"-340.3\">[(1, ?, 10, 10)]</text>\n</g>\n<!-- 140622081863576&#45;&gt;140625830276008 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140622081863576-&gt;140625830276008</title>\n<path d=\"M353.5,-415.3799C353.5,-407.1745 353.5,-397.7679 353.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"357.0001,-388.784 353.5,-378.784 350.0001,-388.784 357.0001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140625830276008&#45;&gt;140623860943840 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140625830276008-&gt;140623860943840</title>\n<path d=\"M353.5,-332.3799C353.5,-324.1745 353.5,-314.7679 353.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"357.0001,-305.784 353.5,-295.784 350.0001,-305.784 357.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140623872024472 -->\n<g class=\"node\" id=\"node12\">\n<title>140623872024472</title>\n<polygon fill=\"none\" points=\"140,-166.5 140,-212.5 541,-212.5 541,-166.5 140,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.5\" y=\"-185.8\">tf_op_layer_Sum: TensorFlowOpLayer</text>\n<polyline fill=\"none\" points=\"385,-166.5 385,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"385,-189.5 443,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"414\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"443,-166.5 443,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"492\" y=\"-197.3\">(?, ?, 10, 128)</text>\n<polyline fill=\"none\" points=\"443,-189.5 541,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"492\" y=\"-174.3\">[(?, 10, 128)]</text>\n</g>\n<!-- 140623860943840&#45;&gt;140623872024472 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140623860943840-&gt;140623872024472</title>\n<path d=\"M349.8788,-249.3799C348.5936,-241.1745 347.1203,-231.7679 345.728,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"349.1522,-222.1219 344.1469,-212.784 342.2365,-223.2052 349.1522,-222.1219\" stroke=\"#000000\"/>\n</g>\n<!-- 140623872024472&#45;&gt;140623860337800 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140623872024472-&gt;140623860337800</title>\n<path d=\"M308.7446,-166.3799C295.7597,-156.9259 280.5843,-145.8772 266.8292,-135.8625\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"268.6248,-132.8405 258.4804,-129.784 264.5047,-138.4995 268.6248,-132.8405\" stroke=\"#000000\"/>\n</g>\n<!-- 140623869896576 -->\n<g class=\"node\" id=\"node14\">\n<title>140623869896576</title>\n<polygon fill=\"none\" points=\"0,-.5 0,-46.5 453,-46.5 453,-.5 0,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-19.8\">time_distributed(dense): TimeDistributed(Dense)</text>\n<polyline fill=\"none\" points=\"296,-.5 296,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"296,-23.5 354,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"354,-.5 354,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403.5\" y=\"-31.3\">(?, 10, 256)</text>\n<polyline fill=\"none\" points=\"354,-23.5 453,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403.5\" y=\"-8.3\">(?, 10, 20705)</text>\n</g>\n<!-- 140623860337800&#45;&gt;140623869896576 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140623860337800-&gt;140623869896576</title>\n<path d=\"M226.5,-83.3799C226.5,-75.1745 226.5,-65.7679 226.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"230.0001,-56.784 226.5,-46.784 223.0001,-56.784 230.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBGECoPCAqap",
        "colab_type": "text"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB7ehYxyAX58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea257f74-d72a-46a2-cb8e-b8319050bf7f"
      },
      "source": [
        "LOAD_MODEL = True\n",
        "\n",
        "if LOAD_MODEL :\n",
        "    model.load_weights(MODEL_PATH)\n",
        "\n",
        "# teacher forcing 학습\n",
        "hist = model.fit([X_train_E, X_train_D], y_train_D,\n",
        "                 batch_size=300,\n",
        "                 epochs=500,\n",
        "                 shuffle=True,\n",
        "                 validation_data=([X_test_E, X_test_D], y_test_D))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "36/36 [==============================] - 4s 98ms/step - loss: 5.0025 - val_loss: 3.6796\n",
            "Epoch 2/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 3.4959 - val_loss: 3.4834\n",
            "Epoch 3/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 3.2408 - val_loss: 3.3400\n",
            "Epoch 4/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 3.0304 - val_loss: 3.2446\n",
            "Epoch 5/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 2.7301 - val_loss: 3.1647\n",
            "Epoch 6/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 2.2290 - val_loss: 3.1439\n",
            "Epoch 7/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 1.6680 - val_loss: 3.2086\n",
            "Epoch 8/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 1.2688 - val_loss: 3.2680\n",
            "Epoch 9/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 1.0555 - val_loss: 3.2711\n",
            "Epoch 10/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.9144 - val_loss: 3.3086\n",
            "Epoch 11/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.7937 - val_loss: 3.2831\n",
            "Epoch 12/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.6848 - val_loss: 3.3187\n",
            "Epoch 13/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.6009 - val_loss: 3.3489\n",
            "Epoch 14/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.5322 - val_loss: 3.3723\n",
            "Epoch 15/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.4761 - val_loss: 3.3986\n",
            "Epoch 16/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.4287 - val_loss: 3.4240\n",
            "Epoch 17/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3918 - val_loss: 3.4480\n",
            "Epoch 18/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3589 - val_loss: 3.4754\n",
            "Epoch 19/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3314 - val_loss: 3.4997\n",
            "Epoch 20/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3024 - val_loss: 3.5296\n",
            "Epoch 21/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.2815 - val_loss: 3.5403\n",
            "Epoch 22/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.2616 - val_loss: 3.5694\n",
            "Epoch 23/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.2437 - val_loss: 3.5818\n",
            "Epoch 24/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.2273 - val_loss: 3.5974\n",
            "Epoch 25/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.2136 - val_loss: 3.6369\n",
            "Epoch 26/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.2020 - val_loss: 3.6334\n",
            "Epoch 27/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1887 - val_loss: 3.6617\n",
            "Epoch 28/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.1769 - val_loss: 3.7074\n",
            "Epoch 29/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1675 - val_loss: 3.6971\n",
            "Epoch 30/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1571 - val_loss: 3.7237\n",
            "Epoch 31/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1501 - val_loss: 3.7253\n",
            "Epoch 32/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1410 - val_loss: 3.7597\n",
            "Epoch 33/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1363 - val_loss: 3.7756\n",
            "Epoch 34/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1325 - val_loss: 3.7900\n",
            "Epoch 35/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1255 - val_loss: 3.8038\n",
            "Epoch 36/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1227 - val_loss: 3.8149\n",
            "Epoch 37/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.1156 - val_loss: 3.8038\n",
            "Epoch 38/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.1071 - val_loss: 3.8492\n",
            "Epoch 39/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0989 - val_loss: 3.8506\n",
            "Epoch 40/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0955 - val_loss: 3.8622\n",
            "Epoch 41/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0923 - val_loss: 3.8522\n",
            "Epoch 42/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0885 - val_loss: 3.8894\n",
            "Epoch 43/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0865 - val_loss: 3.8722\n",
            "Epoch 44/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0836 - val_loss: 3.9065\n",
            "Epoch 45/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0813 - val_loss: 3.8934\n",
            "Epoch 46/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0776 - val_loss: 3.9048\n",
            "Epoch 47/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0743 - val_loss: 3.9262\n",
            "Epoch 48/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0723 - val_loss: 3.9241\n",
            "Epoch 49/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0690 - val_loss: 3.9474\n",
            "Epoch 50/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0684 - val_loss: 3.9418\n",
            "Epoch 51/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0650 - val_loss: 3.9430\n",
            "Epoch 52/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0652 - val_loss: 3.9563\n",
            "Epoch 53/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0638 - val_loss: 3.9595\n",
            "Epoch 54/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0642 - val_loss: 3.9627\n",
            "Epoch 55/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0617 - val_loss: 3.9807\n",
            "Epoch 56/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0605 - val_loss: 3.9566\n",
            "Epoch 57/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0586 - val_loss: 3.9676\n",
            "Epoch 58/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0585 - val_loss: 3.9686\n",
            "Epoch 59/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0562 - val_loss: 4.0071\n",
            "Epoch 60/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0605 - val_loss: 3.9650\n",
            "Epoch 61/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0594 - val_loss: 3.9718\n",
            "Epoch 62/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0564 - val_loss: 3.9505\n",
            "Epoch 63/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0545 - val_loss: 3.9692\n",
            "Epoch 64/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0526 - val_loss: 3.9785\n",
            "Epoch 65/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0469 - val_loss: 3.9989\n",
            "Epoch 66/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0444 - val_loss: 3.9957\n",
            "Epoch 67/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0464 - val_loss: 4.0169\n",
            "Epoch 68/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0456 - val_loss: 4.0086\n",
            "Epoch 69/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0463 - val_loss: 4.0046\n",
            "Epoch 70/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0453 - val_loss: 4.0116\n",
            "Epoch 71/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0440 - val_loss: 4.0043\n",
            "Epoch 72/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0432 - val_loss: 4.0024\n",
            "Epoch 73/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0408 - val_loss: 4.0316\n",
            "Epoch 74/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0443 - val_loss: 3.9962\n",
            "Epoch 75/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0413 - val_loss: 4.0159\n",
            "Epoch 76/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0399 - val_loss: 4.0101\n",
            "Epoch 77/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0408 - val_loss: 4.0097\n",
            "Epoch 78/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0387 - val_loss: 3.9956\n",
            "Epoch 79/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0378 - val_loss: 4.0309\n",
            "Epoch 80/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0372 - val_loss: 4.0242\n",
            "Epoch 81/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0380 - val_loss: 4.0342\n",
            "Epoch 82/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0382 - val_loss: 4.0165\n",
            "Epoch 83/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0378 - val_loss: 4.0222\n",
            "Epoch 84/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0363 - val_loss: 4.0034\n",
            "Epoch 85/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0363 - val_loss: 4.0508\n",
            "Epoch 86/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0366 - val_loss: 4.0294\n",
            "Epoch 87/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0372 - val_loss: 4.0168\n",
            "Epoch 88/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0377 - val_loss: 4.0104\n",
            "Epoch 89/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0391 - val_loss: 4.0494\n",
            "Epoch 90/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0391 - val_loss: 4.0190\n",
            "Epoch 91/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0397 - val_loss: 4.0182\n",
            "Epoch 92/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0358 - val_loss: 4.0512\n",
            "Epoch 93/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0344 - val_loss: 4.0606\n",
            "Epoch 94/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0355 - val_loss: 4.0320\n",
            "Epoch 95/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0349 - val_loss: 4.0365\n",
            "Epoch 96/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0346 - val_loss: 4.0803\n",
            "Epoch 97/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0347 - val_loss: 4.0529\n",
            "Epoch 98/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0351 - val_loss: 4.0205\n",
            "Epoch 99/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0346 - val_loss: 4.0533\n",
            "Epoch 100/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0368 - val_loss: 4.0421\n",
            "Epoch 101/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0346 - val_loss: 4.0522\n",
            "Epoch 102/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0319 - val_loss: 4.0549\n",
            "Epoch 103/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0304 - val_loss: 4.0684\n",
            "Epoch 104/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0315 - val_loss: 4.0708\n",
            "Epoch 105/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0311 - val_loss: 4.0686\n",
            "Epoch 106/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0298 - val_loss: 4.0612\n",
            "Epoch 107/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0300 - val_loss: 4.0364\n",
            "Epoch 108/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0286 - val_loss: 4.0528\n",
            "Epoch 109/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0279 - val_loss: 4.0609\n",
            "Epoch 110/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0271 - val_loss: 4.0829\n",
            "Epoch 111/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0271 - val_loss: 4.0711\n",
            "Epoch 112/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0261 - val_loss: 4.0807\n",
            "Epoch 113/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0277 - val_loss: 4.0761\n",
            "Epoch 114/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0277 - val_loss: 4.0807\n",
            "Epoch 115/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0272 - val_loss: 4.0659\n",
            "Epoch 116/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0262 - val_loss: 4.0739\n",
            "Epoch 117/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0303 - val_loss: 4.0739\n",
            "Epoch 118/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0329 - val_loss: 4.0689\n",
            "Epoch 119/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0310 - val_loss: 4.0880\n",
            "Epoch 120/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0304 - val_loss: 4.0683\n",
            "Epoch 121/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0289 - val_loss: 4.0590\n",
            "Epoch 122/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0283 - val_loss: 4.0585\n",
            "Epoch 123/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0283 - val_loss: 4.0637\n",
            "Epoch 124/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0290 - val_loss: 4.0776\n",
            "Epoch 125/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0282 - val_loss: 4.0776\n",
            "Epoch 126/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0277 - val_loss: 4.0751\n",
            "Epoch 127/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0261 - val_loss: 4.0797\n",
            "Epoch 128/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0261 - val_loss: 4.0991\n",
            "Epoch 129/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0252 - val_loss: 4.0840\n",
            "Epoch 130/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0249 - val_loss: 4.0839\n",
            "Epoch 131/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0250 - val_loss: 4.1198\n",
            "Epoch 132/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0264 - val_loss: 4.0758\n",
            "Epoch 133/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0263 - val_loss: 4.0725\n",
            "Epoch 134/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0250 - val_loss: 4.0596\n",
            "Epoch 135/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0239 - val_loss: 4.1020\n",
            "Epoch 136/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0248 - val_loss: 4.1020\n",
            "Epoch 137/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0252 - val_loss: 4.0786\n",
            "Epoch 138/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0252 - val_loss: 4.0781\n",
            "Epoch 139/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0246 - val_loss: 4.0894\n",
            "Epoch 140/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0247 - val_loss: 4.0769\n",
            "Epoch 141/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0237 - val_loss: 4.1258\n",
            "Epoch 142/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0250 - val_loss: 4.0618\n",
            "Epoch 143/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0256 - val_loss: 4.0777\n",
            "Epoch 144/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0234 - val_loss: 4.0853\n",
            "Epoch 145/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0246 - val_loss: 4.0947\n",
            "Epoch 146/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0259 - val_loss: 4.0621\n",
            "Epoch 147/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0263 - val_loss: 4.0744\n",
            "Epoch 148/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0247 - val_loss: 4.0687\n",
            "Epoch 149/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0240 - val_loss: 4.0421\n",
            "Epoch 150/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0222 - val_loss: 4.0837\n",
            "Epoch 151/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0209 - val_loss: 4.0925\n",
            "Epoch 152/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0208 - val_loss: 4.0884\n",
            "Epoch 153/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0209 - val_loss: 4.1232\n",
            "Epoch 154/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0221 - val_loss: 4.0940\n",
            "Epoch 155/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0210 - val_loss: 4.0997\n",
            "Epoch 156/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0212 - val_loss: 4.1113\n",
            "Epoch 157/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0208 - val_loss: 4.0982\n",
            "Epoch 158/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0206 - val_loss: 4.1506\n",
            "Epoch 159/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0208 - val_loss: 4.1080\n",
            "Epoch 160/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0216 - val_loss: 4.1077\n",
            "Epoch 161/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0230 - val_loss: 4.0876\n",
            "Epoch 162/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0225 - val_loss: 4.0698\n",
            "Epoch 163/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0212 - val_loss: 4.0984\n",
            "Epoch 164/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0212 - val_loss: 4.0663\n",
            "Epoch 165/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0213 - val_loss: 4.0983\n",
            "Epoch 166/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0214 - val_loss: 4.0978\n",
            "Epoch 167/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0231 - val_loss: 4.1023\n",
            "Epoch 168/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0225 - val_loss: 4.0628\n",
            "Epoch 169/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0205 - val_loss: 4.0891\n",
            "Epoch 170/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0191 - val_loss: 4.1002\n",
            "Epoch 171/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0202 - val_loss: 4.1021\n",
            "Epoch 172/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0189 - val_loss: 4.0995\n",
            "Epoch 173/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0194 - val_loss: 4.0553\n",
            "Epoch 174/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0207 - val_loss: 4.1107\n",
            "Epoch 175/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0200 - val_loss: 4.1044\n",
            "Epoch 176/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0204 - val_loss: 4.0967\n",
            "Epoch 177/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0185 - val_loss: 4.1327\n",
            "Epoch 178/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0189 - val_loss: 4.1182\n",
            "Epoch 179/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0209 - val_loss: 4.1569\n",
            "Epoch 180/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0205 - val_loss: 4.1092\n",
            "Epoch 181/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0209 - val_loss: 4.0939\n",
            "Epoch 182/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0206 - val_loss: 4.0966\n",
            "Epoch 183/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0213 - val_loss: 4.0965\n",
            "Epoch 184/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0205 - val_loss: 4.1058\n",
            "Epoch 185/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0197 - val_loss: 4.1189\n",
            "Epoch 186/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0181 - val_loss: 4.0824\n",
            "Epoch 187/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0179 - val_loss: 4.0986\n",
            "Epoch 188/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0163 - val_loss: 4.1144\n",
            "Epoch 189/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0167 - val_loss: 4.1163\n",
            "Epoch 190/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0187 - val_loss: 4.1092\n",
            "Epoch 191/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0185 - val_loss: 4.1312\n",
            "Epoch 192/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0194 - val_loss: 4.1143\n",
            "Epoch 193/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0214 - val_loss: 4.1398\n",
            "Epoch 194/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0211 - val_loss: 4.1122\n",
            "Epoch 195/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0188 - val_loss: 4.1527\n",
            "Epoch 196/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0189 - val_loss: 4.1231\n",
            "Epoch 197/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0180 - val_loss: 4.1383\n",
            "Epoch 198/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0186 - val_loss: 4.1436\n",
            "Epoch 199/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0181 - val_loss: 4.1133\n",
            "Epoch 200/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0169 - val_loss: 4.1213\n",
            "Epoch 201/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0172 - val_loss: 4.1538\n",
            "Epoch 202/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0188 - val_loss: 4.1156\n",
            "Epoch 203/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0214 - val_loss: 4.1146\n",
            "Epoch 204/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0189 - val_loss: 4.1066\n",
            "Epoch 205/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0194 - val_loss: 4.1140\n",
            "Epoch 206/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0178 - val_loss: 4.1314\n",
            "Epoch 207/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0179 - val_loss: 4.1056\n",
            "Epoch 208/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0172 - val_loss: 4.1063\n",
            "Epoch 209/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0164 - val_loss: 4.1108\n",
            "Epoch 210/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0157 - val_loss: 4.1376\n",
            "Epoch 211/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0158 - val_loss: 4.0932\n",
            "Epoch 212/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0157 - val_loss: 4.1419\n",
            "Epoch 213/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0160 - val_loss: 4.1273\n",
            "Epoch 214/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0163 - val_loss: 4.1103\n",
            "Epoch 215/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0162 - val_loss: 4.1298\n",
            "Epoch 216/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0167 - val_loss: 4.1404\n",
            "Epoch 217/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0183 - val_loss: 4.1583\n",
            "Epoch 218/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0188 - val_loss: 4.1473\n",
            "Epoch 219/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0191 - val_loss: 4.0961\n",
            "Epoch 220/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0179 - val_loss: 4.1000\n",
            "Epoch 221/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0180 - val_loss: 4.1009\n",
            "Epoch 222/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0179 - val_loss: 4.1086\n",
            "Epoch 223/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0172 - val_loss: 4.1382\n",
            "Epoch 224/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0177 - val_loss: 4.0874\n",
            "Epoch 225/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0169 - val_loss: 4.0773\n",
            "Epoch 226/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0174 - val_loss: 4.1176\n",
            "Epoch 227/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0160 - val_loss: 4.1209\n",
            "Epoch 228/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0151 - val_loss: 4.1317\n",
            "Epoch 229/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0154 - val_loss: 4.1377\n",
            "Epoch 230/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0155 - val_loss: 4.1408\n",
            "Epoch 231/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0147 - val_loss: 4.1514\n",
            "Epoch 232/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0145 - val_loss: 4.1494\n",
            "Epoch 233/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0138 - val_loss: 4.1441\n",
            "Epoch 234/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0146 - val_loss: 4.1404\n",
            "Epoch 235/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0137 - val_loss: 4.1500\n",
            "Epoch 236/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0150 - val_loss: 4.1340\n",
            "Epoch 237/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0158 - val_loss: 4.1429\n",
            "Epoch 238/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0151 - val_loss: 4.1496\n",
            "Epoch 239/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0168 - val_loss: 4.1349\n",
            "Epoch 240/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0161 - val_loss: 4.1503\n",
            "Epoch 241/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0167 - val_loss: 4.1454\n",
            "Epoch 242/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0160 - val_loss: 4.1489\n",
            "Epoch 243/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0163 - val_loss: 4.1648\n",
            "Epoch 244/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0150 - val_loss: 4.1130\n",
            "Epoch 245/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0142 - val_loss: 4.1336\n",
            "Epoch 246/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0140 - val_loss: 4.1232\n",
            "Epoch 247/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0163 - val_loss: 4.1411\n",
            "Epoch 248/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0185 - val_loss: 4.1080\n",
            "Epoch 249/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0170 - val_loss: 4.2215\n",
            "Epoch 250/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0156 - val_loss: 4.1595\n",
            "Epoch 251/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0148 - val_loss: 4.1733\n",
            "Epoch 252/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0134 - val_loss: 4.1638\n",
            "Epoch 253/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0124 - val_loss: 4.1536\n",
            "Epoch 254/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0131 - val_loss: 4.1848\n",
            "Epoch 255/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0133 - val_loss: 4.1663\n",
            "Epoch 256/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0127 - val_loss: 4.1527\n",
            "Epoch 257/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0130 - val_loss: 4.1555\n",
            "Epoch 258/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0134 - val_loss: 4.1512\n",
            "Epoch 259/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0133 - val_loss: 4.1144\n",
            "Epoch 260/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0127 - val_loss: 4.1349\n",
            "Epoch 261/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0137 - val_loss: 4.1610\n",
            "Epoch 262/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0144 - val_loss: 4.1687\n",
            "Epoch 263/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0139 - val_loss: 4.1571\n",
            "Epoch 264/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0138 - val_loss: 4.1217\n",
            "Epoch 265/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0122 - val_loss: 4.1631\n",
            "Epoch 266/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0122 - val_loss: 4.1202\n",
            "Epoch 267/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0122 - val_loss: 4.1690\n",
            "Epoch 268/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0117 - val_loss: 4.1447\n",
            "Epoch 269/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0120 - val_loss: 4.1553\n",
            "Epoch 270/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0134 - val_loss: 4.0983\n",
            "Epoch 271/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0136 - val_loss: 4.1528\n",
            "Epoch 272/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0129 - val_loss: 4.1407\n",
            "Epoch 273/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0121 - val_loss: 4.1231\n",
            "Epoch 274/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0125 - val_loss: 4.1046\n",
            "Epoch 275/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0118 - val_loss: 4.1393\n",
            "Epoch 276/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0124 - val_loss: 4.1152\n",
            "Epoch 277/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0113 - val_loss: 4.1713\n",
            "Epoch 278/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0126 - val_loss: 4.1241\n",
            "Epoch 279/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0116 - val_loss: 4.1318\n",
            "Epoch 280/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0114 - val_loss: 4.1297\n",
            "Epoch 281/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0118 - val_loss: 4.1493\n",
            "Epoch 282/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0120 - val_loss: 4.1722\n",
            "Epoch 283/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0126 - val_loss: 4.1441\n",
            "Epoch 284/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0130 - val_loss: 4.1022\n",
            "Epoch 285/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0134 - val_loss: 4.1262\n",
            "Epoch 286/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0142 - val_loss: 4.1232\n",
            "Epoch 287/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0143 - val_loss: 4.1259\n",
            "Epoch 288/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0154 - val_loss: 4.1441\n",
            "Epoch 289/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0147 - val_loss: 4.1289\n",
            "Epoch 290/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0140 - val_loss: 4.1065\n",
            "Epoch 291/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0135 - val_loss: 4.1429\n",
            "Epoch 292/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0124 - val_loss: 4.0996\n",
            "Epoch 293/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0113 - val_loss: 4.1498\n",
            "Epoch 294/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0122 - val_loss: 4.1650\n",
            "Epoch 295/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0127 - val_loss: 4.1008\n",
            "Epoch 296/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0126 - val_loss: 4.1682\n",
            "Epoch 297/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0130 - val_loss: 4.1763\n",
            "Epoch 298/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0137 - val_loss: 4.1615\n",
            "Epoch 299/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0139 - val_loss: 4.1575\n",
            "Epoch 300/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0121 - val_loss: 4.1200\n",
            "Epoch 301/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0119 - val_loss: 4.1343\n",
            "Epoch 302/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0119 - val_loss: 4.1002\n",
            "Epoch 303/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0119 - val_loss: 4.1462\n",
            "Epoch 304/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0112 - val_loss: 4.1773\n",
            "Epoch 305/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0119 - val_loss: 4.1503\n",
            "Epoch 306/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0114 - val_loss: 4.1667\n",
            "Epoch 307/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0122 - val_loss: 4.1532\n",
            "Epoch 308/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0115 - val_loss: 4.1679\n",
            "Epoch 309/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0116 - val_loss: 4.1665\n",
            "Epoch 310/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0114 - val_loss: 4.1813\n",
            "Epoch 311/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0124 - val_loss: 4.1934\n",
            "Epoch 312/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0116 - val_loss: 4.2159\n",
            "Epoch 313/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0119 - val_loss: 4.1859\n",
            "Epoch 314/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0111 - val_loss: 4.1711\n",
            "Epoch 315/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0108 - val_loss: 4.1925\n",
            "Epoch 316/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0118 - val_loss: 4.1622\n",
            "Epoch 317/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0118 - val_loss: 4.1624\n",
            "Epoch 318/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0106 - val_loss: 4.1983\n",
            "Epoch 319/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0125 - val_loss: 4.1510\n",
            "Epoch 320/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0118 - val_loss: 4.1144\n",
            "Epoch 321/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0125 - val_loss: 4.1583\n",
            "Epoch 322/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0103 - val_loss: 4.1795\n",
            "Epoch 323/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0110 - val_loss: 4.1712\n",
            "Epoch 324/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0118 - val_loss: 4.1483\n",
            "Epoch 325/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0109 - val_loss: 4.1350\n",
            "Epoch 326/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0115 - val_loss: 4.1330\n",
            "Epoch 327/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0111 - val_loss: 4.1572\n",
            "Epoch 328/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0109 - val_loss: 4.1394\n",
            "Epoch 329/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0106 - val_loss: 4.1433\n",
            "Epoch 330/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0110 - val_loss: 4.1639\n",
            "Epoch 331/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0111 - val_loss: 4.1641\n",
            "Epoch 332/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0108 - val_loss: 4.1698\n",
            "Epoch 333/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0113 - val_loss: 4.1732\n",
            "Epoch 334/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0120 - val_loss: 4.1976\n",
            "Epoch 335/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0121 - val_loss: 4.2056\n",
            "Epoch 336/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0125 - val_loss: 4.1384\n",
            "Epoch 337/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0137 - val_loss: 4.1570\n",
            "Epoch 338/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0138 - val_loss: 4.1658\n",
            "Epoch 339/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0132 - val_loss: 4.1657\n",
            "Epoch 340/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0133 - val_loss: 4.1472\n",
            "Epoch 341/500\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.0125 - val_loss: 4.1774\n",
            "Epoch 342/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0117 - val_loss: 4.1292\n",
            "Epoch 343/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0112 - val_loss: 4.1263\n",
            "Epoch 344/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0104 - val_loss: 4.1871\n",
            "Epoch 345/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0115 - val_loss: 4.1959\n",
            "Epoch 346/500\n",
            "36/36 [==============================] - 3s 73ms/step - loss: 0.0114 - val_loss: 4.1550\n",
            "Epoch 347/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0107 - val_loss: 4.2071\n",
            "Epoch 348/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0113 - val_loss: 4.1974\n",
            "Epoch 349/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0109 - val_loss: 4.2170\n",
            "Epoch 350/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0113 - val_loss: 4.1718\n",
            "Epoch 351/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0102 - val_loss: 4.1832\n",
            "Epoch 352/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0108 - val_loss: 4.1869\n",
            "Epoch 353/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0106 - val_loss: 4.1757\n",
            "Epoch 354/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0105 - val_loss: 4.2226\n",
            "Epoch 355/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0101 - val_loss: 4.2287\n",
            "Epoch 356/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0113 - val_loss: 4.1619\n",
            "Epoch 357/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0109 - val_loss: 4.1859\n",
            "Epoch 358/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0117 - val_loss: 4.1670\n",
            "Epoch 359/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0119 - val_loss: 4.1605\n",
            "Epoch 360/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0105 - val_loss: 4.1627\n",
            "Epoch 361/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.2225\n",
            "Epoch 362/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0091 - val_loss: 4.1896\n",
            "Epoch 363/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0088 - val_loss: 4.2357\n",
            "Epoch 364/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2161\n",
            "Epoch 365/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0091 - val_loss: 4.2523\n",
            "Epoch 366/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.2263\n",
            "Epoch 367/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.2260\n",
            "Epoch 368/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.1675\n",
            "Epoch 369/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0093 - val_loss: 4.2444\n",
            "Epoch 370/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0114 - val_loss: 4.2437\n",
            "Epoch 371/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0113 - val_loss: 4.2371\n",
            "Epoch 372/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0115 - val_loss: 4.1782\n",
            "Epoch 373/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0110 - val_loss: 4.2032\n",
            "Epoch 374/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0105 - val_loss: 4.2289\n",
            "Epoch 375/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0099 - val_loss: 4.2306\n",
            "Epoch 376/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0100 - val_loss: 4.2475\n",
            "Epoch 377/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0116 - val_loss: 4.2169\n",
            "Epoch 378/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0115 - val_loss: 4.2014\n",
            "Epoch 379/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0117 - val_loss: 4.1867\n",
            "Epoch 380/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0118 - val_loss: 4.1604\n",
            "Epoch 381/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0107 - val_loss: 4.1697\n",
            "Epoch 382/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0108 - val_loss: 4.1460\n",
            "Epoch 383/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.1565\n",
            "Epoch 384/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0092 - val_loss: 4.2090\n",
            "Epoch 385/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0092 - val_loss: 4.1818\n",
            "Epoch 386/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.1282\n",
            "Epoch 387/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.1377\n",
            "Epoch 388/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0104 - val_loss: 4.1628\n",
            "Epoch 389/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.1776\n",
            "Epoch 390/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.1826\n",
            "Epoch 391/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0084 - val_loss: 4.1662\n",
            "Epoch 392/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0077 - val_loss: 4.1915\n",
            "Epoch 393/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0081 - val_loss: 4.2187\n",
            "Epoch 394/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0091 - val_loss: 4.1929\n",
            "Epoch 395/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.1975\n",
            "Epoch 396/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0100 - val_loss: 4.1777\n",
            "Epoch 397/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.2133\n",
            "Epoch 398/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0101 - val_loss: 4.2075\n",
            "Epoch 399/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0106 - val_loss: 4.1893\n",
            "Epoch 400/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0106 - val_loss: 4.1998\n",
            "Epoch 401/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0103 - val_loss: 4.2002\n",
            "Epoch 402/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.2967\n",
            "Epoch 403/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0105 - val_loss: 4.2014\n",
            "Epoch 404/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0106 - val_loss: 4.2259\n",
            "Epoch 405/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0103 - val_loss: 4.1339\n",
            "Epoch 406/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0103 - val_loss: 4.1439\n",
            "Epoch 407/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0098 - val_loss: 4.1869\n",
            "Epoch 408/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.1854\n",
            "Epoch 409/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0093 - val_loss: 4.2041\n",
            "Epoch 410/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.2144\n",
            "Epoch 411/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.1776\n",
            "Epoch 412/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.1957\n",
            "Epoch 413/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0093 - val_loss: 4.2028\n",
            "Epoch 414/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.1985\n",
            "Epoch 415/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.1765\n",
            "Epoch 416/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0088 - val_loss: 4.2254\n",
            "Epoch 417/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.2401\n",
            "Epoch 418/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.1982\n",
            "Epoch 419/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0091 - val_loss: 4.1772\n",
            "Epoch 420/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0084 - val_loss: 4.1823\n",
            "Epoch 421/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0090 - val_loss: 4.2111\n",
            "Epoch 422/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0093 - val_loss: 4.1669\n",
            "Epoch 423/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2686\n",
            "Epoch 424/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0092 - val_loss: 4.1669\n",
            "Epoch 425/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0085 - val_loss: 4.2390\n",
            "Epoch 426/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0082 - val_loss: 4.2490\n",
            "Epoch 427/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0092 - val_loss: 4.1876\n",
            "Epoch 428/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0086 - val_loss: 4.2239\n",
            "Epoch 429/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0095 - val_loss: 4.2178\n",
            "Epoch 430/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0102 - val_loss: 4.2109\n",
            "Epoch 431/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0106 - val_loss: 4.2051\n",
            "Epoch 432/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.2122\n",
            "Epoch 433/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0101 - val_loss: 4.2001\n",
            "Epoch 434/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0104 - val_loss: 4.2384\n",
            "Epoch 435/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0101 - val_loss: 4.2364\n",
            "Epoch 436/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0094 - val_loss: 4.2589\n",
            "Epoch 437/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0099 - val_loss: 4.1865\n",
            "Epoch 438/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0105 - val_loss: 4.2003\n",
            "Epoch 439/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0102 - val_loss: 4.2311\n",
            "Epoch 440/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0102 - val_loss: 4.1648\n",
            "Epoch 441/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0108 - val_loss: 4.1723\n",
            "Epoch 442/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0107 - val_loss: 4.1864\n",
            "Epoch 443/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0098 - val_loss: 4.2261\n",
            "Epoch 444/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0108 - val_loss: 4.1447\n",
            "Epoch 445/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0103 - val_loss: 4.1752\n",
            "Epoch 446/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0093 - val_loss: 4.1649\n",
            "Epoch 447/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0086 - val_loss: 4.2050\n",
            "Epoch 448/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0084 - val_loss: 4.2228\n",
            "Epoch 449/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0080 - val_loss: 4.2197\n",
            "Epoch 450/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0079 - val_loss: 4.2192\n",
            "Epoch 451/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0074 - val_loss: 4.2054\n",
            "Epoch 452/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0074 - val_loss: 4.2581\n",
            "Epoch 453/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0076 - val_loss: 4.2767\n",
            "Epoch 454/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0079 - val_loss: 4.2151\n",
            "Epoch 455/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0075 - val_loss: 4.2574\n",
            "Epoch 456/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0077 - val_loss: 4.2219\n",
            "Epoch 457/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0082 - val_loss: 4.2331\n",
            "Epoch 458/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0084 - val_loss: 4.2466\n",
            "Epoch 459/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0092 - val_loss: 4.2367\n",
            "Epoch 460/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.2178\n",
            "Epoch 461/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0098 - val_loss: 4.2146\n",
            "Epoch 462/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.2117\n",
            "Epoch 463/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0092 - val_loss: 4.2375\n",
            "Epoch 464/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0091 - val_loss: 4.2181\n",
            "Epoch 465/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0087 - val_loss: 4.2327\n",
            "Epoch 466/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0096 - val_loss: 4.2330\n",
            "Epoch 467/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0102 - val_loss: 4.1928\n",
            "Epoch 468/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0099 - val_loss: 4.2218\n",
            "Epoch 469/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.1437\n",
            "Epoch 470/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 4.1700\n",
            "Epoch 471/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.2570\n",
            "Epoch 472/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0090 - val_loss: 4.2440\n",
            "Epoch 473/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0087 - val_loss: 4.2253\n",
            "Epoch 474/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2047\n",
            "Epoch 475/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0081 - val_loss: 4.2610\n",
            "Epoch 476/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0077 - val_loss: 4.2433\n",
            "Epoch 477/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0082 - val_loss: 4.1959\n",
            "Epoch 478/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0082 - val_loss: 4.2015\n",
            "Epoch 479/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0081 - val_loss: 4.2096\n",
            "Epoch 480/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0077 - val_loss: 4.2331\n",
            "Epoch 481/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0076 - val_loss: 4.2587\n",
            "Epoch 482/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0081 - val_loss: 4.2848\n",
            "Epoch 483/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0083 - val_loss: 4.2629\n",
            "Epoch 484/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0086 - val_loss: 4.2574\n",
            "Epoch 485/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0090 - val_loss: 4.2050\n",
            "Epoch 486/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0085 - val_loss: 4.2542\n",
            "Epoch 487/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2225\n",
            "Epoch 488/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0081 - val_loss: 4.2823\n",
            "Epoch 489/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0080 - val_loss: 4.2454\n",
            "Epoch 490/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0082 - val_loss: 4.2715\n",
            "Epoch 491/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0084 - val_loss: 4.2929\n",
            "Epoch 492/500\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 0.0092 - val_loss: 4.2822\n",
            "Epoch 493/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0093 - val_loss: 4.2207\n",
            "Epoch 494/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2845\n",
            "Epoch 495/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0089 - val_loss: 4.2479\n",
            "Epoch 496/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0094 - val_loss: 4.2346\n",
            "Epoch 497/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0096 - val_loss: 4.3013\n",
            "Epoch 498/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0098 - val_loss: 4.1883\n",
            "Epoch 499/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0088 - val_loss: 4.3195\n",
            "Epoch 500/500\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.0100 - val_loss: 4.2437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5gH5i-3A9D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9074b825-b793-424a-c7e6-c5f1add50d03"
      },
      "source": [
        "# loss 시각화 : 첫 번째 학습\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Trajectory')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCRhUKcTBKpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 결과 저장\n",
        "model.save_weights(MODEL_PATH)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVA1pVIW_uVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}