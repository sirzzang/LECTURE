# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZE4r86L7CKNiGP8MN3fXCQ6958kip3a1

`'/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/20200818/BERT'` 경로에다가만 풀면 안 된다.
"""

# 모듈 불러오기
import codecs
import tensorflow as tf
from keras_bert import load_trained_model_from_checkpoint
from keras_bert import Tokenizer
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
import numpy as np
from tqdm import tqdm
import pickle
import os


# 경로 설정
pretrained_path = "/content/pretrained_bert"
config_path = os.path.join(pretrained_path, 'bert_config.json')
checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')
vocab_path = os.path.join(pretrained_path, 'vocab.txt')

data_path = "/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/dataset"

# 모델 파라미터 설정
SEQ_LEN = int(input('최대 문장 길이 설정: '))
BATCH_SIZE = int(input('배치 사이즈 설정: '))
EPOCHS = int(input('학습 횟수 설정: '))
LR = float(input('학습률 설정: ')) # 작게 줘야 한다.

# Pre-trained BERT의 Vocabulary
word2idx = {}
with codecs.open(vocab_path, 'r', 'utf8') as reader:
    for line in reader:
        token = line.strip()
        word2idx[token] = len(word2idx)

idx2word = {v:k for v, k in enumerate(word2idx)}

# Pre-trained BERT 모델
model = load_trained_model_from_checkpoint(
        config_path,
        checkpoint_path,
        training=True,
        trainable=True,
        seq_len=SEQ_LEN,
)

print("========== 모델 전체 구조 확인 ==========")
print(model.summary())

# BERT 모델의 토크나이저
tokenizer = Tokenizer(word2idx)

# IMDB 데이터 로드
dataset = tf.keras.utils.get_file(
    fname="aclImdb.tar.gz", 
    origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz", 
    extract=True,
)

# BERT Fine-tuning용 학습 데이터와 시험 데이터 생성
def load_data(path):
    global tokenizer
    indices, sentiments = [], []
    for folder, sentiment in (('neg', 0), ('pos', 1)):
        folder = os.path.join(path, folder)
        for name in tqdm(os.listdir(folder)):
            with open(os.path.join(folder, name), 'r', encoding='UTF8') as reader:
                  text = reader.read()
            ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)
            indices.append(ids)
            sentiments.append(sentiment)
    items = list(zip(indices, sentiments))
    np.random.shuffle(items)
    indices, sentiments = zip(*items)
    indices = np.array(indices)
    mod = indices.shape[0] % BATCH_SIZE
    if mod > 0:
        indices, sentiments = indices[:-mod], sentiments[:-mod]
    return [indices, np.zeros_like(indices)], np.array(sentiments)

# 데이터 로드 혹은 저장
try:
    # 학습, 시험 데이터를 읽어온다.
    with open(f'{data_path}/train_test.pickle', 'rb') as f:
        train_x, train_y, test_x, test_y = pickle.load(f)
except:
    train_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'train')
    test_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'test')
    
    train_x, train_y = load_data(train_path)
    test_x, test_y = load_data(test_path)
    
    # 결과 저장
    with open(f'{data_path}/train_test.pickle', 'wb') as f:
        pickle.dump([train_x, train_y, test_x, test_y], f, pickle.HIGHEST_PROTOCOL)

# 데이터 확인
print([idx2word[k] for k in train_x[0][0]])

# 데이터 원상 복귀하여 확인
print([idx2word[k] for k in train_x[0][0]])
decoded = tokenizer.decode(list(train_x[0][0])) # 오픈소스 패키지에서 제공하는 동일한 메소드: 맨 앞의 [CLS]와 맨 뒤의 [SEP]은 제거
print(decoded)

# 원본 문장을 복원
text = []
for i, t in enumerate(decoded):
    if i != 0 and t[0] != '#':
        text.append('_' + t)
    else:
        text.append(t)
''.join([t.replace('##', '') for t in text]).replace('_', ' ')

# Fine-tuning 모델 생성 후 학습
inputs = model.inputs[:2]
dense = model.get_layer('NSP-Dense').output
outputs = Dense(units=1, activation='sigmoid')(dense)
model = Model(inputs, outputs)

# 모델 학습 환경 설정
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
)

# 추가 학습: Fine-tuning
model.fit(
    train_x,
    train_y,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
)

# 시험 데이터로 정확도 평가
predicts = model.predict(test_x, verbose=True)
pred_y = np.where(predicts > 0.5, 1, 0).reshape(-1,)
print('Accuracy = %.4f' % np.mean(test_y == pred_y))







