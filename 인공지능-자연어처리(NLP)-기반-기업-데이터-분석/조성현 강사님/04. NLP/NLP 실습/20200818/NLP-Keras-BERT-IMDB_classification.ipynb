{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Keras-BERT-IMDB_classification.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSW4KgtWql2-",
        "colab_type": "text"
      },
      "source": [
        "# 설치\n",
        "- 오픈소스: `keras_bert`(https://github.com/CyberZHG/keras-bert).\n",
        "- BERT: BERT tiny 다운로드.\n",
        "    - *오류 주의* : 드라이브 내 폴더에다가 풀면 `.ckpt` 파일 인식하지 못한다.\n",
        "    - path 지정해서 폴더 안에다가 풀 것."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVdtYQtjbwcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "4b9f9407-e057-49d8-dd7b-447b0d693000"
      },
      "source": [
        "!pip install keras-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34143 sha256=074b1becc427154c6c96dbeb333ce6bc3bd4b020fba408397045b53c8ab56ea1\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=499347c8e0113b4bc50be7d2c357897b7a811287910f7859faa69a7ad232b6d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=2d85c2381eb1ae60207dd0225b7b02d1b196f972eed11aca49a53074435127fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15611 sha256=db8e700767a04386d9297a673be32aec90b33f9453c0e05ca2546f4adb5004bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=9ee64ad2762d0ad4546bbdf421e323e1a712aa5cd076833a514e13e2cc824f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=9cb92edd486b26721acafcb286ccc990e1319eb4ed32be27e9f05359c2d071cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4558 sha256=3d19ed55618ab963adeae4b461f1cc0cbfd847a581ba9c276552afef4c611a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=eb456d6d813ce7adced6ca5e00d7058df2b6308e6f5c2295e43f17a8f6ba4654\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaIw26kec0Gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "77844c89-5304-435d-924e-295c599a9abb"
      },
      "source": [
        "!apt install unzip\n",
        "!wget -q https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip\n",
        "!unzip -o uncased_L-2_H-128_A-2.zip -d pretrained_bert"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Archive:  uncased_L-2_H-128_A-2.zip\n",
            "  inflating: pretrained_bert/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: pretrained_bert/bert_config.json  \n",
            "  inflating: pretrained_bert/vocab.txt  \n",
            "  inflating: pretrained_bert/bert_model.ckpt.index  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIB-FNfldO41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import tensorflow as tf\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "from keras_bert import Tokenizer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "os.environ['TF_KERAS'] = '1'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DEKZ2-1rBWx",
        "colab_type": "text"
      },
      "source": [
        "# 기본 설정\n",
        "- 경로 설정\n",
        "- 파라미터 설정\n",
        "    - 강사님 코드 변경: `LOAD_DATA` boolean 변수 삭제.\n",
        "    - 이후 `try ~ except ...` 구문으로 실행."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcCh5pvzd2bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 경로 설정\n",
        "\n",
        "pretrained_path = \"/content/pretrained_bert\"\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
        "\n",
        "data_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/dataset\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Jjocc5eBLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d7fcda2f-e4b8-4855-d5a7-043e54b8af7b"
      },
      "source": [
        "# 모델 파라미터 설정\n",
        "SEQ_LEN = int(input('최대 문장 길이 설정: '))\n",
        "BATCH_SIZE = int(input('배치 사이즈 설정: '))\n",
        "EPOCHS = int(input('학습 횟수 설정: '))\n",
        "LR = 0.001\n",
        "# LOAD_DATA = True # 강사님 코드 변경"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최대 문장 길이 설정: 128\n",
            "배치 사이즈 설정: 128\n",
            "학습 횟수 설정: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvr4vt6rN6G",
        "colab_type": "text"
      },
      "source": [
        "# Pre-trained BERT 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2upLlpPUdXKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vocabulary\n",
        "word2idx = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        word2idx[token] = len(word2idx)\n",
        "\n",
        "idx2word = {v:k for v, k in enumerate(word2idx)}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqddpEQpdXhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9037679a-fbd9-4af5-b0a0-d5e28f50ec3c"
      },
      "source": [
        "# Pre-trained BERT 모델 구조 확인\n",
        "model = load_trained_model_from_checkpoint(\n",
        "        config_path,\n",
        "        checkpoint_path,\n",
        "        training=True,\n",
        "        trainable=True,\n",
        "        seq_len=SEQ_LEN,\n",
        ")\n",
        "\n",
        "print()\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 128, 128), ( 3906816     Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 128)     256         Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 128)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 128)     16384       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 128)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 128)     256         Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 128)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 128)     256         Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 128, 128)     131712      Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 128, 128)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 128, 128)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 128, 128)     256         Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 128)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 128)     256         Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 128, 128)     131712      Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 128, 128)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 128, 128)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 128, 128)     256         Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 128, 128)     16512       Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 128, 128)     256         MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 128)          0           Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 128, 30522)   30522       MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 128)          16512       Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 128, 30522)   0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            258         NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,384,316\n",
            "Trainable params: 4,384,316\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6fZflh4eFoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 토크나이저\n",
        "tokenizer = Tokenizer(word2idx)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc31AiqwrW4L",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 준비\n",
        "- IMDB 데이터 로드.\n",
        "- 학습용, 시험용 데이터 생성.\n",
        "- 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eDXbGT2eH3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9e8e9287-2ff9-484f-b0fc-98531d15137b"
      },
      "source": [
        "# IMDB 데이터를 읽어온다.\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    fname=\"aclImdb.tar.gz\", \n",
        "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "    extract=True,\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ULngITeQ0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT Fine-tuning용 학습 데이터와 시험 데이터를 생성한다.\n",
        "def load_data(path):\n",
        "    global tokenizer\n",
        "    indices, sentiments = [], []\n",
        "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
        "        folder = os.path.join(path, folder)\n",
        "        for name in tqdm(os.listdir(folder)):\n",
        "            with open(os.path.join(folder, name), 'r', encoding='UTF8') as reader:\n",
        "                  text = reader.read()\n",
        "            ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
        "            indices.append(ids)\n",
        "            sentiments.append(sentiment)\n",
        "    items = list(zip(indices, sentiments))\n",
        "    np.random.shuffle(items)\n",
        "    indices, sentiments = zip(*items)\n",
        "    indices = np.array(indices)\n",
        "    mod = indices.shape[0] % BATCH_SIZE\n",
        "    if mod > 0:\n",
        "        indices, sentiments = indices[:-mod], sentiments[:-mod]\n",
        "    return [indices, np.zeros_like(indices)], np.array(sentiments)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI585gYbeTzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if LOAD_DATA:\n",
        "try:\n",
        "    # 학습, 시험 데이터를 읽어온다.\n",
        "    with open(f'{data_path}/train_test.pickle', 'rb') as f:\n",
        "        train_x, train_y, test_x, test_y = pickle.load(f)\n",
        "except:\n",
        "    train_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'train')\n",
        "    test_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'test')\n",
        "    \n",
        "    train_x, train_y = load_data(train_path)\n",
        "    test_x, test_y = load_data(test_path)\n",
        "    \n",
        "    # 결과를 저장한다.\n",
        "    with open(f'{data_path}/train_test.pickle', 'wb') as f:\n",
        "        pickle.dump([train_x, train_y, test_x, test_y], f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp-AZSM-rgRQ",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 확인\n",
        "- 첫 번째 문장 확인.\n",
        "- decode해서 원래 문장 확인."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQP_92Vie6PE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ad94e8ad-d261-4c67-a260-76a505dc7024"
      },
      "source": [
        "# 학습 데이터의 첫 번째 문장을 decode해 본다. 결과는 맨 뒤에 있다.\n",
        "print([idx2word[k] for k in train_x[0][0]])\n",
        "\n",
        "# 아래 명령으로 decode해도 된다. 맨 앞의 [CLS]와 맨 뒤의 [SEP]은 제거된다.\n",
        "decoded = tokenizer.decode(list(train_x[0][0]))\n",
        "print(decoded)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'a', 'drama', 'at', 'its', 'very', 'core', ',', '\"', 'anna', '\"', 'displays', 'that', 'genuine', 'truth', 'that', 'all', 'actors', 'age', ',', 'and', 'sometimes', ',', 'fade', 'away', '.', 'anna', 'is', 'a', 'character', 'that', 'believes', 'america', 'is', 'her', 'safety', 'net', ',', 'her', 'home', ',', 'and', 'it', 'can', 'do', 'her', 'no', 'wrong', 'but', 'she', 'refuses', 'to', 'bel', '##itt', '##le', 'herself', 'to', 'do', 'work', 'she', 'doesn', \"'\", 't', 'believe', 'in', '.', 'she', 'is', 'hard', '-', 'nosed', ',', 'optimistic', ',', 'stubborn', ',', 'and', 'arrogant', 'when', 'it', 'comes', 'to', 'her', 'life', ',', 'yet', 'not', 'afraid', 'to', 'let', 'others', 'in', ',', 'yet', 'drop', 'them', 'at', 'a', 'moments', 'notice', '.', 'anna', 'flip', '-', 'flop', '##s', 'between', 'personalities', ',', 'which', 'makes', 'this', 'film', 'ideal', 'of', 'an', 'aging', 'star', ',', 'but', 'not', 'idea', 'of', 'the', 'viewing', 'audience', '.', '[SEP]']\n",
            "['a', 'drama', 'at', 'its', 'very', 'core', ',', '\"', 'anna', '\"', 'displays', 'that', 'genuine', 'truth', 'that', 'all', 'actors', 'age', ',', 'and', 'sometimes', ',', 'fade', 'away', '.', 'anna', 'is', 'a', 'character', 'that', 'believes', 'america', 'is', 'her', 'safety', 'net', ',', 'her', 'home', ',', 'and', 'it', 'can', 'do', 'her', 'no', 'wrong', 'but', 'she', 'refuses', 'to', 'bel', '##itt', '##le', 'herself', 'to', 'do', 'work', 'she', 'doesn', \"'\", 't', 'believe', 'in', '.', 'she', 'is', 'hard', '-', 'nosed', ',', 'optimistic', ',', 'stubborn', ',', 'and', 'arrogant', 'when', 'it', 'comes', 'to', 'her', 'life', ',', 'yet', 'not', 'afraid', 'to', 'let', 'others', 'in', ',', 'yet', 'drop', 'them', 'at', 'a', 'moments', 'notice', '.', 'anna', 'flip', '-', 'flop', '##s', 'between', 'personalities', ',', 'which', 'makes', 'this', 'film', 'ideal', 'of', 'an', 'aging', 'star', ',', 'but', 'not', 'idea', 'of', 'the', 'viewing', 'audience', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoWwDXldfY48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5740b889-e6ce-4b28-fa0b-ebf5a13ec76b"
      },
      "source": [
        "# delimiter를 제거하고 원본 문장을 복원해 본다.\n",
        "text = []\n",
        "for i, t in enumerate(decoded):\n",
        "    if i != 0 and t[0] != '#':\n",
        "        text.append('_' + t)\n",
        "    else:\n",
        "        text.append(t)\n",
        "''.join([t.replace('##', '') for t in text]).replace('_', ' ')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a drama at its very core , \" anna \" displays that genuine truth that all actors age , and sometimes , fade away . anna is a character that believes america is her safety net , her home , and it can do her no wrong but she refuses to belittle herself to do work she doesn \\' t believe in . she is hard - nosed , optimistic , stubborn , and arrogant when it comes to her life , yet not afraid to let others in , yet drop them at a moments notice . anna flip - flops between personalities , which makes this film ideal of an aging star , but not idea of the viewing audience .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY8DJiEWrk6g",
        "colab_type": "text"
      },
      "source": [
        "# 학습\n",
        "\n",
        "- 원래 우리가 참조한 오픈소스에서는 `RAdam` 쓰도록 했는데, 그거 쓰면 에러난다.\n",
        "\n",
        "```\n",
        "from keras_radam import RAdam\n",
        "optimizer=RAdam(lr=1e-4)\n",
        "# TypeError: __init__() missing 1 required positional argument: 'name' \n",
        "```\n",
        "\n",
        "- 추가 학습: 에폭 1만 잡아서 돌려 본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSpA3YNefbSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine-tuning 모델 생성 후 학습\n",
        "inputs = model.inputs[:2]\n",
        "dense = model.get_layer('NSP-Dense').output\n",
        "outputs = Dense(units=1, activation='sigmoid')(dense)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# 모델 학습 환경 설정\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "#    metrics=['binary_crossentropy'],\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsEM3WSMfbuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "229df612-6e15-4031-c152-00e55a38c2ef"
      },
      "source": [
        "# 추가 학습한다. Fine-tuning\n",
        "model.fit(\n",
        "    train_x,\n",
        "    train_y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195/195 [==============================] - 76s 389ms/step - loss: 0.4862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0f1008940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "espB1Pz5r1NR",
        "colab_type": "text"
      },
      "source": [
        "# 예측 정확도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ricuIPJBr2cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM27v-sAfurr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "842c78af-b290-49e0-d2f4-3cb903093ac5"
      },
      "source": [
        "# 시험 데이터로 정확도를 평가한다.\n",
        "predicts = model.predict(test_x, verbose=True)\n",
        "pred_y = np.where(predicts > 0.5, 1, 0).reshape(-1,)\n",
        "print('Accuracy = %.4f' % np.mean(test_y == pred_y))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "780/780 [==============================] - 35s 45ms/step\n",
            "Accuracy = 0.8079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lXsFWukfvmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srUFNdH2fvCK",
        "colab_type": "text"
      },
      "source": [
        "# ============ 테스트 ============"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcAxmHwifv-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f6826c06-3a13-4e9b-c46e-4c3e6d86acde"
      },
      "source": [
        "print([idx2word[k] for k in train_x[0][0]])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'a', 'drama', 'at', 'its', 'very', 'core', ',', '\"', 'anna', '\"', 'displays', 'that', 'genuine', 'truth', 'that', 'all', 'actors', 'age', ',', 'and', 'sometimes', ',', 'fade', 'away', '.', 'anna', 'is', 'a', 'character', 'that', 'believes', 'america', 'is', 'her', 'safety', 'net', ',', 'her', 'home', ',', 'and', 'it', 'can', 'do', 'her', 'no', 'wrong', 'but', 'she', 'refuses', 'to', 'bel', '##itt', '##le', 'herself', 'to', 'do', 'work', 'she', 'doesn', \"'\", 't', 'believe', 'in', '.', 'she', 'is', 'hard', '-', 'nosed', ',', 'optimistic', ',', 'stubborn', ',', 'and', 'arrogant', 'when', 'it', 'comes', 'to', 'her', 'life', ',', 'yet', 'not', 'afraid', 'to', 'let', 'others', 'in', ',', 'yet', 'drop', 'them', 'at', 'a', 'moments', 'notice', '.', 'anna', 'flip', '-', 'flop', '##s', 'between', 'personalities', ',', 'which', 'makes', 'this', 'film', 'ideal', 'of', 'an', 'aging', 'star', ',', 'but', 'not', 'idea', 'of', 'the', 'viewing', 'audience', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGwsrTYsfwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xto8pBoOfv16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}