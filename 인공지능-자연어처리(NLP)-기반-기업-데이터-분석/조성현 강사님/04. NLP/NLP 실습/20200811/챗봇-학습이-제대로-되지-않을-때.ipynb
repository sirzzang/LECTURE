{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "챗봇 학습이 제대로 되지 않을 때.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeKBdKKmlayg",
        "colab_type": "text"
      },
      "source": [
        " vocabulary가 다르면 학습이 제대도 되어도 좋은 결과가 나지 않을 수 있다. 일단 강사님 코드 실행시켜서 vocabulary 파일을 제대로 만들자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDBjgIqmDb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KoNLPy 설치\n",
        "import os\n",
        "\n",
        "# KoNLPy 라이브러리 설치\n",
        "!pip install konlpy\n",
        "\n",
        "# jdk, JPype1-py3 설치\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "# automake 설치\n",
        "os.chdir('/tmp')\n",
        "!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz\n",
        "!tar -zxvf automake-1.11.tar.gz\n",
        "os.chdir('/tmp/automake-1.11')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "\n",
        "# automake 설치 오류 시\n",
        "os.chdir('/tmp/') \n",
        "!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz\n",
        "!tar -zvxf m4-1.4.9.tar.gz\n",
        "os.chdir('/tmp/m4-1.4.9')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "os.chdir('/tmp')\n",
        "!curl -OL http://ftpmirror.gnu.org/autoconf/autoconf-2.69.tar.gz\n",
        "!tar xzf autoconf-2.69.tar.gz\n",
        "os.chdir('/tmp/autoconf-2.69')\n",
        "!./configure --prefix=/usr/local\n",
        "!make\n",
        "!make install\n",
        "!export PATH=/usr/local/bin\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRO1nXQgk5pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/dataset\"\n",
        "\n",
        "DATA_PATH = f'{data_path}/6-1.ChatBotData.csv'\n",
        "VOCABULARY_PATH = f'{data_path}/6-1.vocabulary.voc'\n",
        "TOKENIZE_AS_MORPH = False       # 형태소 분석 여부\n",
        "ENC_INPUT = 0                   # encoder 입력을 의미함\n",
        "DEC_INPUT = 1                   # decoder 입력을 의미함\n",
        "DEC_TARGET = 2                  # decoder 출력을 의미함\n",
        "MAX_SEQUENCE_LEN = 10           # 단어 시퀀스 길이\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PADDING>\"\n",
        "STD = \"<START>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNKNOWN>\"\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "# 판다스를 통해서 데이터를 불러와 학습 셋과 평가 셋으로 나누어 그 값을 리턴한다.\n",
        "def load_data():\n",
        "    data_df = pd.read_csv(DATA_PATH, header=0)\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    train_input, eval_input, train_label, eval_label = \\\n",
        "        train_test_split(question, answer, test_size=0.1, random_state=42)\n",
        "        \n",
        "    return train_input, train_label, eval_input, eval_label\n",
        "\n",
        "# 형태소 분석\n",
        "# 감성분석이나 문서 분류에는 형태소 분석이 필요하다. 하지만 답변 데이터에 형태소 분석을 \n",
        "# 적용하면 형태소로 답변하게 된다.\n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "# 인코더, 디코더의 입력과 출력 데이터를 생성한다.\n",
        "# 디코더 입력과 타켓에는 앞 뒤에 STD, END가 들어간다.\n",
        "#\n",
        "# 예시:\n",
        "# DEFINES.max_sequence_length = 10 인 경우\n",
        "# 인코더 입력 : \"가끔 궁금해\" -> [9310, 17707, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "# 디코더 입력 : \"그 사람도 그럴 거예요\" -> [STD, 20190, 4221, 13697, 14552, 0, ...]\n",
        "# 디코더 타켓 : [20190, 4221, 13697, 14552, END, 0, ...]\n",
        "def data_processing(value, dictionary, pType):\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if TOKENIZE_AS_MORPH:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    sequences_input_index = []\n",
        "    for sequence in value:\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        \n",
        "        if pType == DEC_INPUT:\n",
        "            # 디코더 입력은 <START>로 시작한다.\n",
        "            sequence_index = [dictionary[STD]]\n",
        "        else:\n",
        "            sequence_index = []\n",
        "        \n",
        "        for word in sequence.split():\n",
        "            # word가 딕셔너리에 없으면 UNK (out of vacabulary)를 넣는다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.append(dictionary[word])\n",
        "            else:\n",
        "                sequence_index.append(dictionary[UNK])\n",
        "        \n",
        "            # 문장의 단어수를  제한한다.\n",
        "            if len(sequence_index) >= MAX_SEQUENCE_LEN:\n",
        "                break\n",
        "        \n",
        "        # 디코더 출력은 <END>로 끝난다.\n",
        "        if pType == DEC_TARGET:\n",
        "            if len(sequence_index) < MAX_SEQUENCE_LEN:\n",
        "                sequence_index.append(dictionary[END])\n",
        "            else:\n",
        "                sequence_index[len(sequence_index)-1] = dictionary[END]\n",
        "                \n",
        "        # max_sequence_length보다 문장 길이가 작으면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE_LEN - len(sequence_index)) * [dictionary[PAD]]\n",
        "        sequences_input_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_input_index)\n",
        "\n",
        "# 토크나이징\n",
        "def data_tokenizer(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    return [word for word in words if word]\n",
        "\n",
        "# 사전 파일을 만든다\n",
        "def make_vocabulary():\n",
        "    data_df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "    \n",
        "    # 질문과 응답 문장의 단어를 형태소로 바꾼다\n",
        "    if TOKENIZE_AS_MORPH:  \n",
        "        question = prepro_like_morphlized(question)\n",
        "        answer = prepro_like_morphlized(answer)\n",
        "        \n",
        "    data = []\n",
        "    data.extend(question)\n",
        "    data.extend(answer)\n",
        "    words = data_tokenizer(data)\n",
        "    words = list(set(words))\n",
        "    words[:0] = MARKER\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(words)}\n",
        "    \n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# 질문과 응답 문장 전체의 단어 목록 dict를 만든다.\n",
        "word2idx, idx2word = make_vocabulary()\n",
        "\n",
        "# 질문과 응답 문장을 학습 데이터와 시험 데이터로 분리한다.\n",
        "train_input, train_label, eval_input, eval_label = load_data()\n",
        "\n",
        "# 학습 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "train_input_enc = data_processing(train_input, word2idx, ENC_INPUT)\n",
        "train_input_dec = data_processing(train_label, word2idx, DEC_INPUT)\n",
        "train_target_dec = data_processing(train_label, word2idx, DEC_TARGET)\n",
        "\t\n",
        "# 평가 데이터 : 인코딩, 디코딩 입력, 디코딩 출력을 만든다.\n",
        "eval_input_enc = data_processing(eval_input, word2idx, ENC_INPUT)\n",
        "eval_input_dec = data_processing(eval_label, word2idx, DEC_INPUT)\n",
        "eval_target_dec = data_processing(eval_label, word2idx, DEC_TARGET)\n",
        "\n",
        "# 결과를 저장한다.\n",
        "with open(f'{data_path}/6-1.vocabulary.pickle', 'wb') as f:\n",
        "    pickle.dump([word2idx, idx2word], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(f'{data_path}/6-1.train_data.pickle', 'wb') as f:\n",
        "    pickle.dump([train_input_enc, train_input_dec, train_target_dec], f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(f'{data_path}/6-1.eval_data.pickle', 'wb') as f:\n",
        "    pickle.dump([eval_input_enc, eval_input_dec, eval_target_dec], f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1WmZYDilIns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}