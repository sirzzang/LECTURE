{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Chatbot-Transformers-Predict-Practice.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIJLDtXsV0eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Activation, Add, LayerNormalization, Dropout\n",
        "from tensorflow.keras.layers import Embedding, TimeDistributed, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVJ32gzoWA3t",
        "colab_type": "text"
      },
      "source": [
        "#### ChatBotData\n",
        "- 11823개 문답 pair\n",
        "- label: 0-일상다반사, 1-이별(부정), 2-사랑(긍정)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeDxxlbcV1gi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "504f22ed-c494-4bde-8df0-604f2054604c"
      },
      "source": [
        "# chatBotData = pd.read_csv('/content/drive/My Drive/멀티캠퍼스/머신러닝/4. 자연어처리(딥러닝)/내폴더/08.11(화)/Seq2Seq/dataset/6-1.ChatBotData.csv')\n",
        "chatBotData = pd.read_csv(\"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/dataset/6-1.ChatBotData.csv\")\n",
        "chatBotData.head(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Q           A  label\n",
              "0       12시 땡!  하루가 또 가네요.      0\n",
              "1  1지망 학교 떨어졌어   위로해 드립니다.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkx6c1d7X4cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD = \"<PADDING>\"\n",
        "STD = \"<START>\"\n",
        "END = \"<END>\"\n",
        "OOV = \"<OOV>\"\n",
        "MARKER = [PAD, STD, END, OOV]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(pd.concat([chatBotData['Q'], chatBotData['A']], axis=0))\n",
        "word2idx =  list(tokenizer.word_index.keys())\n",
        "word2idx[:0] = MARKER\n",
        "word2idx = {word: idx for idx, word in enumerate(word2idx)}\n",
        "tokenizer.word_index = word2idx\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "chatBotData['Q'] = tokenizer.texts_to_sequences(chatBotData['Q'])\n",
        "chatBotData['A'] = tokenizer.texts_to_sequences(chatBotData['A'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmmgI13WUYS",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing\n",
        "- 문장의 최대 단어 개수를 제한\n",
        "- Source(Question), Target(<START> + Answer), Output(Answer + <END>)로 나눈다\n",
        "- Padding 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnyaHp9iV_w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LEN = 10\n",
        "\n",
        "def padding(x):\n",
        "    if (MAX_SEQUENCE_LEN - len(x)) > 0:\n",
        "        x += (MAX_SEQUENCE_LEN - len(x)) * [word2idx[PAD]]\n",
        "    return x\n",
        "\n",
        "data = pd.DataFrame()\n",
        "data['Source'] = chatBotData['Q'].apply(lambda x: padding(x[:MAX_SEQUENCE_LEN]))\n",
        "data['Target'] = chatBotData['A'].apply(lambda x: padding([word2idx[STD]] + x[:MAX_SEQUENCE_LEN]) if len(x) < MAX_SEQUENCE_LEN else padding([word2idx[STD]] + x[:MAX_SEQUENCE_LEN-1]))\n",
        "data['Output'] = chatBotData['A'].apply(lambda x: padding(x[:MAX_SEQUENCE_LEN] + [word2idx[END]]) if len(x) < MAX_SEQUENCE_LEN else padding(x[:MAX_SEQUENCE_LEN-1] + [word2idx[END]]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nU-XvWGU72X",
        "colab_type": "text"
      },
      "source": [
        "#### Train, Test Data 분리\n",
        "-  random_state: 세트를 섞을 때 해당 int 값을 보고 섞으며, 하이퍼 파라미터를 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlyMByAsUUFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainSource, testSource, trainTarget, testTarget, trainOutput, testOutput = \\\n",
        "        [np.vstack(d) for d in train_test_split(data['Source'].values, data['Target'].values, data['Output'].values, test_size=0.2, random_state=42)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87P-2XfZ46HH",
        "colab_type": "text"
      },
      "source": [
        "# Inputs + Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Jd2OSw49k-",
        "colab_type": "text"
      },
      "source": [
        "## Positional Encoding\n",
        "- trainSource.shape: (None, MAX_SEQUENCE_LEN)\n",
        "- Embeding Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- Positional Encoding Layer shape: (MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtJjUKaFxwaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    # pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52p0s8-5Ac-",
        "colab_type": "text"
      },
      "source": [
        "## Positional Encoding + Source Embedding Layer\n",
        "- [Input] Embeding Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Positional Encoding Layer shape: (MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Encoded Inputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsPl-Zuiuqh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7faa06fe-0ac4-409c-85dd-50f16d2cb0d8"
      },
      "source": [
        "VOCAB_SIZE = len(idx2word)\n",
        "EMB_SIZE = 128\n",
        "\n",
        "inputs = Input(batch_shape=(None, trainSource.shape[1]), name='inputs')\n",
        "inputEmbedding = Embedding(VOCAB_SIZE, EMB_SIZE, name='inputEmbedding')(inputs)\n",
        "encoded_inputs = tf.add(positional_encoding(MAX_SEQUENCE_LEN, EMB_SIZE), inputEmbedding, name='encoded_inputs')\n",
        "encoded_inputs.shape\n",
        "# # embeddings.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8GA88qV5FyY",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Multi Head Attention\n",
        "- [Input] Encoded Inputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] multi head attention shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmh-X9wxj_1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "04f6370b-71a4-4d54-a8d5-4c8907e9d394"
      },
      "source": [
        "num_heads = 4\n",
        "model_dim = EMB_SIZE\n",
        "key_dim = EMB_SIZE // num_heads\n",
        "value_dim = EMB_SIZE // num_heads\n",
        "\n",
        "def linear_projection(encPrevLayer, decPrevLayer):\n",
        "    q = Dense(key_dim, use_bias=False)(encPrevLayer)\n",
        "    k = Dense(key_dim, use_bias=False)(encPrevLayer)\n",
        "    v = Dense(key_dim, use_bias=False)(decPrevLayer)\n",
        "\n",
        "    return q, k, v\n",
        "\n",
        "def split_heads(q, k, v):\n",
        "    def split_last_dimension_then_transpose(tensor, num_heads):\n",
        "        tensor = K.expand_dims(tensor, axis=1)\n",
        "        return K.repeat_elements(tensor,rep=num_heads, axis=1)\n",
        "    \n",
        "    qs = split_last_dimension_then_transpose(q, num_heads)\n",
        "    ks = split_last_dimension_then_transpose(k, num_heads)\n",
        "    vs = split_last_dimension_then_transpose(v, num_heads)\n",
        "\n",
        "    return qs, ks, vs\n",
        "\n",
        "\n",
        "encQ, encK, encV = linear_projection(encoded_inputs, encoded_inputs)\n",
        "encQs, encKs, encVs = split_heads(encQ, encK, encV)\n",
        "encQs.shape, encKs.shape, encVs.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([None, 4, 10, 32]),\n",
              " TensorShape([None, 4, 10, 32]),\n",
              " TensorShape([None, 4, 10, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DZ851Uye0Fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbd604e3-ac2d-46af-e822-9318418feaa4"
      },
      "source": [
        "def _scaled_dot_product(qs, ks, vs, key_dim, masked):\n",
        "    '''\n",
        "    qs: (batch_size, num_heads, max_seq_len, dim)\n",
        "    ks: (batch_size, num_heads, max_seq_len, dim)\n",
        "    vs: (batch_size, num_heads, max_seq_len, dim)\n",
        "    '''\n",
        "    o1 = tf.matmul(qs, ks, transpose_b=True)    # Q * (K^T)\n",
        "    o2 = o1 / (key_dim ** 0.5)                  # Q * (K^T) / (dk^(1/2))\n",
        "\n",
        "    if masked:\n",
        "        diag_vals = tf.ones_like(o2[0, 0, :, :])\n",
        "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # 하삼각행렬: 자기 자신인 단어까지만.\n",
        "        masks = tf.tile(input=tf.reshape(tril, (1, 1, tril.shape[0], tril.shape[1])), \\\n",
        "                        multiples=[tf.shape(o2)[0], tf.shape(o2)[1], 1, 1]) # batch size, num_heads 만큼 행렬 사이즈 늘려서 마스크 만듦.\n",
        "        paddings = tf.ones_like(masks) * -1e9 # 주목하지 말아야 할 부분에 낮은 숫자.\n",
        "        o2 = tf.where(tf.equal(masks, 0), paddings, o2) # mask에 0인 위치(뒤의 단어)에 패딩을 넣어 준다.\n",
        "\n",
        "    o3 = Activation('softmax')(o2)              # softmax\n",
        "    o3 = tf.matmul(o3, vs)\n",
        "    \n",
        "    return o3\n",
        "\n",
        "def _concat_heads(outputs):\n",
        "    def transpose_then_concat_last_two_dimension(tensor):\n",
        "        return tf.reshape(tensor, shape=(-1, tensor.shape[2], tensor.shape[1]*tensor.shape[3]))\n",
        "    return transpose_then_concat_last_two_dimension(outputs)\n",
        "\n",
        "encAttValue = _scaled_dot_product(encQs, encKs, encVs, key_dim, False)\n",
        "encConAttValue = _concat_heads(encAttValue)\n",
        "encConAttValue.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPBvdZVi58Fz",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Add & Norm 1st\n",
        "- [Input] multi head attention shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Encoded Inputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Add And Norm Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGeDv6rxxkgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77de4fc4-48c4-4cde-9715-f60f0e4e9d67"
      },
      "source": [
        "addEnc1 = Add(name='addEnc1')([encConAttValue, encoded_inputs])\n",
        "addNormEnc1 = LayerNormalization(name='addNormEnc1')(addEnc1)\n",
        "addNormEnc1.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtNeV7B6-25n",
        "colab_type": "text"
      },
      "source": [
        "## Encoder FFN + Add & Norm 2nd\n",
        "- [Input] Add And Norm Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Encoder Output shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HY5eZHTxkaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1_dim=200\n",
        "w2_dim=128\n",
        "dropout=0.1\n",
        "\n",
        "encFFN = Dense(w1_dim, activation='relu', name='encFFN_1') (addNormEnc1)\n",
        "encFFN = Dense(w2_dim, name='encFFN_2') (encFFN)\n",
        "encFFNDropout = Dropout(1.0 - dropout, name='encFFNDropout')(encFFN)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0l7LaEZQ1ps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e3cdc77-3a61-412f-8d55-cdf3f2e6d17e"
      },
      "source": [
        "addEnc2 = Add(name='addEnc2')([addNormEnc1, encFFNDropout])\n",
        "addNormEnc2 = LayerNormalization(name='addNormEnc2')(addEnc2)\n",
        "addNormEnc2.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtll3pZTP7d",
        "colab_type": "text"
      },
      "source": [
        "# Outputs + Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0SGX7uYUmor",
        "colab_type": "text"
      },
      "source": [
        "## Positional Encoding + Target Embedding Layer\n",
        "- [Input] Embeding Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Positional Encoding Layer shape: (MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Encoded Outputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDq7LfISqAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9799596-ba14-472c-d128-2a1261888c32"
      },
      "source": [
        "outputs = Input(batch_shape=(None, trainTarget.shape[1]), name='outputs')\n",
        "outputEmbedding = Embedding(VOCAB_SIZE, EMB_SIZE, name='outputEmbedding')(outputs)\n",
        "encoded_outputs = tf.add(positional_encoding(MAX_SEQUENCE_LEN, EMB_SIZE), outputEmbedding, name='encoded_outputs')\n",
        "encoded_outputs.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOyKtEB0VERl"
      },
      "source": [
        "## Decoder Masked Multi Head Attention\n",
        "- [Input] Encoded Outputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] masked multi head attention shape: (None, head, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oOho7ve0mo9M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52952356-388e-4f30-822b-ec05807520bb"
      },
      "source": [
        "decMaskedQ, decMaskedK, decMaskedV = linear_projection(encoded_outputs, encoded_outputs)\n",
        "decMaskedQs, decMaskedKs, decMaskedVs = split_heads(decMaskedQ, decMaskedK, decMaskedV)\n",
        "\n",
        "decMaskedAttValue = _scaled_dot_product(decMaskedQs, decMaskedKs, decMaskedVs, key_dim, True)\n",
        "decMaskedConAttValue = _concat_heads(decMaskedAttValue)\n",
        "decMaskedConAttValue.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XzuWnTM9VERm"
      },
      "source": [
        "## Decoder Add & Norm 1st\n",
        "- [Input] masked multi head attention shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Encoded Outputs shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Decoder Add And Norm 1st Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "akvbCfyMVERm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "391b45a4-737b-4952-cbc7-24db78e62bdf"
      },
      "source": [
        "addDec1 = Add(name='addDec1')([decMaskedConAttValue, encoded_outputs])\n",
        "addNormDec1 = LayerNormalization(name='addNormDec1')(addDec1)\n",
        "addNormDec1.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ixUF2in2V04b"
      },
      "source": [
        "## Decoder Multi Head Attention\n",
        "- [Input] Encoder Output shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Decoder Add & Norm 1st Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Decoder multi head attention shape: (None, head, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7HKN35efKAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90066596-d8ce-420a-bd78-6b61b87eb47c"
      },
      "source": [
        "decQ, decK, decV = linear_projection(addNormEnc2, addNormDec1)\n",
        "decQs, decKs, decVs = split_heads(decQ, decK, decV)\n",
        "\n",
        "decAttValue = _scaled_dot_product(decQs, decKs, decVs, key_dim, False)\n",
        "decConAttValue = _concat_heads(decAttValue)\n",
        "decConAttValue.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W773bNnhVyca"
      },
      "source": [
        "## Decoder Add & Norm 2nd\n",
        "- [Input] Decoder Add And Norm 1st Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Input] Decoder multi head attention shape: (None, head, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Decoder Add And Norm 2nd Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGNKFg7vVycb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e072317-de3c-447e-dec8-43f53a023d14"
      },
      "source": [
        "addDec2 = Add(name='addDec2')([decConAttValue, addNormDec1])\n",
        "addNormDec2 = LayerNormalization(name='addNormDec2')(addDec2)\n",
        "addNormDec2.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co4zf5eIVERp"
      },
      "source": [
        "## Decoder FFN + Add & Norm 3rd\n",
        "- [Input] Decoder Add And Norm 2nd Layer shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)\n",
        "- [Output] Decoder Output shape: (None, MAX_SEQUENCE_LEN, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRWLe5i7VERq",
        "colab": {}
      },
      "source": [
        "w1_dim=200\n",
        "w2_dim=128\n",
        "dropout=0.1\n",
        "\n",
        "decFFN = Dense(w1_dim, activation='relu', name='decFFN_1') (addNormDec2)\n",
        "decFFN = Dense(w2_dim, name='decFFN_2') (decFFN)\n",
        "decFFNDropout = Dropout(1.0 - dropout, name='decFFNDropout')(decFFN)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3-eESccCVERs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dedaf7bd-83f7-449e-d58c-5a6b9871923b"
      },
      "source": [
        "addDec3 = Add(name='addDec3')([addNormDec2, decFFNDropout])\n",
        "addNormDec3 = LayerNormalization(name='addNormDec3')(addDec3)\n",
        "addNormDec3.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 10, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHklChC4XK6W",
        "colab_type": "text"
      },
      "source": [
        "# Linear + Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7zjH2kYjTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linearLayer = Dense(VOCAB_SIZE, name='linearLayer')(addNormDec3)\n",
        "OutputProb = Dense(VOCAB_SIZE, activation='softmax', name='OutputProb')(linearLayer)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2V3tazaDMJ",
        "colab_type": "text"
      },
      "source": [
        "# Model Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USY4f8LqaGWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62dc4095-f95e-4d85-e27c-15e7ad66bdc2"
      },
      "source": [
        "model = Model([inputs, outputs], OutputProb)\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "outputs (InputLayer)            [(None, 10)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "outputEmbedding (Embedding)     (None, 10, 128)      2648192     outputs[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "inputs (InputLayer)             [(None, 10)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_encoded_outputs (Te [(None, 10, 128)]    0           outputEmbedding[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "inputEmbedding (Embedding)      (None, 10, 128)      2648192     inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_outputs[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_outputs[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_encoded_inputs (Ten [(None, 10, 128)]    0           inputEmbedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 10, 32)]  0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_4 (Tenso [(None, 1, 10, 32)]  0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_3 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_4 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 1, 10, 32)]  0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 10, 32)]  0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_3[0][0]        \n",
            "                                                                 tf_op_layer_split_3[0][0]        \n",
            "                                                                 tf_op_layer_split_3[0][0]        \n",
            "                                                                 tf_op_layer_split_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_4 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_4[0][0]        \n",
            "                                                                 tf_op_layer_split_4[0][0]        \n",
            "                                                                 tf_op_layer_split_4[0][0]        \n",
            "                                                                 tf_op_layer_split_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split (TensorFlowOp [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_1 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_2 (Te [(None, 4, 10, 10)]  0           tf_op_layer_concat_3[0][0]       \n",
            "                                                                 tf_op_layer_concat_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 4, 10, 32)]  0           tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (TensorFl [(None, 4, 10, 10)]  0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2 (Tens [(None, 4, 10, 10)]  0           tf_op_layer_concat[0][0]         \n",
            "                                                                 tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 10, 32)]  0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(10, 10)]           0           tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorFlow [(None, 4, 10, 10)]  0           tf_op_layer_BatchMatMulV2[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_2 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 4, 10, 10)    0           tf_op_layer_RealDiv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill (TensorFlowOpL [(10, 10)]           0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_1 (TensorFlow [(4,)]               0           tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_2 (TensorFlow [(4,)]               0           tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_1 (Te [(None, 4, 10, 32)]  0           activation[0][0]                 \n",
            "                                                                 tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatrixBandPart (Ten [(10, 10)]           0           tf_op_layer_Fill[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(None, 10, 128)]    0           tf_op_layer_BatchMatMulV2_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_1 (TensorFl [(1, 1, 10, 10)]     0           tf_op_layer_MatrixBandPart[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile/multiples (Ten [(4,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "addEnc1 (Add)                   (None, 10, 128)      0           tf_op_layer_Reshape[0][0]        \n",
            "                                                                 tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile (TensorFlowOpL [(None, None, 10, 10 0           tf_op_layer_Reshape_1[0][0]      \n",
            "                                                                 tf_op_layer_Tile/multiples[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "addNormEnc1 (LayerNormalization (None, 10, 128)      256         addEnc1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_3 (TensorFlow [(4,)]               0           tf_op_layer_Tile[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "encFFN_1 (Dense)                (None, 10, 200)      25800       addNormEnc1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Fill_1 (TensorFlowO [(None, None, 10, 10 0           tf_op_layer_Shape_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_outputs[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "encFFN_2 (Dense)                (None, 10, 128)      25728       encFFN_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Equal (TensorFlowOp [(None, None, 10, 10 0           tf_op_layer_Tile[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul (TensorFlowOpLa [(None, None, 10, 10 0           tf_op_layer_Fill_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 10, 32)]  0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encFFNDropout (Dropout)         (None, 10, 128)      0           encFFN_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_SelectV2 (TensorFlo [(None, 4, 10, 10)]  0           tf_op_layer_Equal[0][0]          \n",
            "                                                                 tf_op_layer_Mul[0][0]            \n",
            "                                                                 tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_5 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "addEnc2 (Add)                   (None, 10, 128)      0           addNormEnc1[0][0]                \n",
            "                                                                 encFFNDropout[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 4, 10, 10)    0           tf_op_layer_SelectV2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_5 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_5[0][0]        \n",
            "                                                                 tf_op_layer_split_5[0][0]        \n",
            "                                                                 tf_op_layer_split_5[0][0]        \n",
            "                                                                 tf_op_layer_split_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "addNormEnc2 (LayerNormalization (None, 10, 128)      256         addEnc2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_3 (Te [(None, 4, 10, 32)]  0           activation_1[0][0]               \n",
            "                                                                 tf_op_layer_concat_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10, 32)       4096        addNormEnc2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10, 32)       4096        addNormEnc2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_2 (TensorFl [(None, 10, 128)]    0           tf_op_layer_BatchMatMulV2_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_6 (Tenso [(None, 1, 10, 32)]  0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_7 (Tenso [(None, 1, 10, 32)]  0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "addDec1 (Add)                   (None, 10, 128)      0           tf_op_layer_Reshape_2[0][0]      \n",
            "                                                                 tf_op_layer_encoded_outputs[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_6 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_7 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "addNormDec1 (LayerNormalization (None, 10, 128)      256         addDec1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_6 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_6[0][0]        \n",
            "                                                                 tf_op_layer_split_6[0][0]        \n",
            "                                                                 tf_op_layer_split_6[0][0]        \n",
            "                                                                 tf_op_layer_split_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_7 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_7[0][0]        \n",
            "                                                                 tf_op_layer_split_7[0][0]        \n",
            "                                                                 tf_op_layer_split_7[0][0]        \n",
            "                                                                 tf_op_layer_split_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 10, 32)       4096        addNormDec1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_4 (Te [(None, 4, 10, 10)]  0           tf_op_layer_concat_6[0][0]       \n",
            "                                                                 tf_op_layer_concat_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_8 (Tenso [(None, 1, 10, 32)]  0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (TensorFl [(None, 4, 10, 10)]  0           tf_op_layer_BatchMatMulV2_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_8 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 4, 10, 10)    0           tf_op_layer_RealDiv_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_8 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_8[0][0]        \n",
            "                                                                 tf_op_layer_split_8[0][0]        \n",
            "                                                                 tf_op_layer_split_8[0][0]        \n",
            "                                                                 tf_op_layer_split_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_5 (Te [(None, 4, 10, 32)]  0           activation_2[0][0]               \n",
            "                                                                 tf_op_layer_concat_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_3 (TensorFl [(None, 10, 128)]    0           tf_op_layer_BatchMatMulV2_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "addDec2 (Add)                   (None, 10, 128)      0           tf_op_layer_Reshape_3[0][0]      \n",
            "                                                                 addNormDec1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "addNormDec2 (LayerNormalization (None, 10, 128)      256         addDec2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decFFN_1 (Dense)                (None, 10, 200)      25800       addNormDec2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decFFN_2 (Dense)                (None, 10, 128)      25728       decFFN_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decFFNDropout (Dropout)         (None, 10, 128)      0           decFFN_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "addDec3 (Add)                   (None, 10, 128)      0           addNormDec2[0][0]                \n",
            "                                                                 decFFNDropout[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "addNormDec3 (LayerNormalization (None, 10, 128)      256         addDec3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "linearLayer (Dense)             (None, 10, 20689)    2668881     addNormDec3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "OutputProb (Dense)              (None, 10, 20689)    428055410   linearLayer[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 436,161,875\n",
            "Trainable params: 436,161,875\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZLLDaOaWuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87a00d62-45ad-4845-c4d0-92464dc5c9c0"
      },
      "source": [
        "hist = model.fit([trainSource, trainTarget], trainOutput,\n",
        "                 batch_size = 500, \n",
        "                 epochs=100, \n",
        "                 shuffle=True,\n",
        "                 validation_data = ([testSource, testTarget], testOutput))\n",
        "\n",
        "# Loss history\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 5.6372 - val_loss: 5.9289\n",
            "Epoch 2/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 3.8197 - val_loss: 5.2375\n",
            "Epoch 3/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 3.5292 - val_loss: 4.4817\n",
            "Epoch 4/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 3.2146 - val_loss: 3.5347\n",
            "Epoch 5/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 2.9421 - val_loss: 3.1448\n",
            "Epoch 6/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 2.7658 - val_loss: 3.0199\n",
            "Epoch 7/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 2.6061 - val_loss: 3.0222\n",
            "Epoch 8/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 2.4465 - val_loss: 2.8007\n",
            "Epoch 9/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 2.1795 - val_loss: 2.5651\n",
            "Epoch 10/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 1.8403 - val_loss: 2.3524\n",
            "Epoch 11/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 1.4379 - val_loss: 2.1147\n",
            "Epoch 12/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 1.0542 - val_loss: 1.9196\n",
            "Epoch 13/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.7156 - val_loss: 1.7869\n",
            "Epoch 14/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 0.4579 - val_loss: 1.6528\n",
            "Epoch 15/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.2870 - val_loss: 1.5788\n",
            "Epoch 16/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1823 - val_loss: 1.5363\n",
            "Epoch 17/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1208 - val_loss: 1.5097\n",
            "Epoch 18/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0845 - val_loss: 1.4932\n",
            "Epoch 19/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0619 - val_loss: 1.4831\n",
            "Epoch 20/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0472 - val_loss: 1.4919\n",
            "Epoch 21/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0371 - val_loss: 1.4890\n",
            "Epoch 22/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0302 - val_loss: 1.4853\n",
            "Epoch 23/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0264 - val_loss: 1.4792\n",
            "Epoch 24/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 0.0235 - val_loss: 1.4765\n",
            "Epoch 25/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0210 - val_loss: 1.4834\n",
            "Epoch 26/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0196 - val_loss: 1.4837\n",
            "Epoch 27/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0187 - val_loss: 1.4836\n",
            "Epoch 28/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0181 - val_loss: 1.4832\n",
            "Epoch 29/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0175 - val_loss: 1.4881\n",
            "Epoch 30/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0172 - val_loss: 1.4877\n",
            "Epoch 31/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0170 - val_loss: 1.4893\n",
            "Epoch 32/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0168 - val_loss: 1.4912\n",
            "Epoch 33/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0169 - val_loss: 1.4884\n",
            "Epoch 34/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0165 - val_loss: 1.4911\n",
            "Epoch 35/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0163 - val_loss: 1.4910\n",
            "Epoch 36/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0162 - val_loss: 1.4936\n",
            "Epoch 37/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0161 - val_loss: 1.4952\n",
            "Epoch 38/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0161 - val_loss: 1.4965\n",
            "Epoch 39/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0160 - val_loss: 1.4951\n",
            "Epoch 40/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0159 - val_loss: 1.4946\n",
            "Epoch 41/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0158 - val_loss: 1.4967\n",
            "Epoch 42/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0157 - val_loss: 1.4971\n",
            "Epoch 43/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0157 - val_loss: 1.5007\n",
            "Epoch 44/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0157 - val_loss: 1.5002\n",
            "Epoch 45/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0156 - val_loss: 1.5016\n",
            "Epoch 46/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 0.0156 - val_loss: 1.5018\n",
            "Epoch 47/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0155 - val_loss: 1.5012\n",
            "Epoch 48/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0155 - val_loss: 1.5006\n",
            "Epoch 49/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0155 - val_loss: 1.5024\n",
            "Epoch 50/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5060\n",
            "Epoch 51/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5050\n",
            "Epoch 52/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5038\n",
            "Epoch 53/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5062\n",
            "Epoch 54/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5052\n",
            "Epoch 55/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0154 - val_loss: 1.5071\n",
            "Epoch 56/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5079\n",
            "Epoch 57/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5103\n",
            "Epoch 58/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5085\n",
            "Epoch 59/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5080\n",
            "Epoch 60/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5119\n",
            "Epoch 61/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5089\n",
            "Epoch 62/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5125\n",
            "Epoch 63/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5082\n",
            "Epoch 64/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5133\n",
            "Epoch 65/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5120\n",
            "Epoch 66/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5133\n",
            "Epoch 67/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5124\n",
            "Epoch 68/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5149\n",
            "Epoch 69/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0151 - val_loss: 1.5159\n",
            "Epoch 70/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0151 - val_loss: 1.5143\n",
            "Epoch 71/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0151 - val_loss: 1.5149\n",
            "Epoch 72/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0153 - val_loss: 1.5157\n",
            "Epoch 73/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0152 - val_loss: 1.5170\n",
            "Epoch 74/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0159 - val_loss: 1.5267\n",
            "Epoch 75/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0166 - val_loss: 1.5265\n",
            "Epoch 76/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0183 - val_loss: 1.5279\n",
            "Epoch 77/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0234 - val_loss: 1.5507\n",
            "Epoch 78/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.0450 - val_loss: 1.5940\n",
            "Epoch 79/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 2.0859 - val_loss: 3.0139\n",
            "Epoch 80/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 1.6303 - val_loss: 2.2770\n",
            "Epoch 81/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.7257 - val_loss: 1.8520\n",
            "Epoch 82/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.4061 - val_loss: 1.7061\n",
            "Epoch 83/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.2900 - val_loss: 1.6180\n",
            "Epoch 84/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.2373 - val_loss: 1.5814\n",
            "Epoch 85/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.2118 - val_loss: 1.5741\n",
            "Epoch 86/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1966 - val_loss: 1.5548\n",
            "Epoch 87/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1894 - val_loss: 1.5447\n",
            "Epoch 88/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1863 - val_loss: 1.5411\n",
            "Epoch 89/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1843 - val_loss: 1.5416\n",
            "Epoch 90/100\n",
            "19/19 [==============================] - 36s 2s/step - loss: 0.1837 - val_loss: 1.5352\n",
            "Epoch 91/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1834 - val_loss: 1.5319\n",
            "Epoch 92/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1826 - val_loss: 1.5324\n",
            "Epoch 93/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1825 - val_loss: 1.5325\n",
            "Epoch 94/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1824 - val_loss: 1.5313\n",
            "Epoch 95/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1818 - val_loss: 1.5314\n",
            "Epoch 96/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1816 - val_loss: 1.5329\n",
            "Epoch 97/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1811 - val_loss: 1.5326\n",
            "Epoch 98/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1811 - val_loss: 1.5337\n",
            "Epoch 99/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1810 - val_loss: 1.5322\n",
            "Epoch 100/100\n",
            "19/19 [==============================] - 35s 2s/step - loss: 0.1810 - val_loss: 1.5325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxb9ZXw/8+RZMvxnsXZdwiBJCROYhKSEta2QICh+1D2KR2edjqFzrQUuhd+tFOeZx6glBkoLRTa8hSmQGhpaIFCAqFA0gSSkEAge+KszmLH8W7p/P74Xtmy492SZV+d9+ull6y76J5rSUdfnfu93yuqijHGGP8JpDoAY4wxyWEJ3hhjfMoSvDHG+JQleGOM8SlL8MYY41OW4I0xxqcswRvTiog8KiJ3djD/uIhM7suYjOkJS/Cm3xKRHSLy0VTH0Zqq5qrqto6WEZFzRaS0r2Iypi2W4I3ph0QklOoYzMBnCd4MOCISFpF7RWSvd7tXRMLevGEi8icRKReRIyKyQkQC3rxbRWSPiFSKyAcickEHmxksIku9ZVeKyElx21cROdn7e7GIvOctt0dEviEiOcCfgdFeOee4iIzuJO5zRaTUi3E/8CsR2SAil8VtN0NEDonI7MT/V40fWYI3A9F3gDOBYmAWMA/4rjfv60ApUASMAL4NqIhMBf4VOENV84ALgR0dbOMK4HZgMLAF+FE7yz0M/C/vOWcAr6hqFXAxsNcr5+Sq6t5O4gYYCQwBJgA3Ar8Gro6bvxjYp6rvdBC3MU0swZuB6CrgDlU9qKpluER8jTevARgFTFDVBlVdoW7ApQgQBqaJSIaq7lDVrR1sY4mqrlLVRuBxXFJuS4P3nPmqelRV3+5h3ABR4AeqWqeqNcBvgcUiku/Nvwb4TQfPb0wLluDNQDQa2Bn3eKc3DeD/4FrcL4rINhG5DUBVtwBfA34IHBSRJ0RkNO3bH/d3NZDbznKfxrWsd4rIqyKyoIdxA5Spam3sgdfq/xvwaREpxP0qeLyD5zemBUvwZiDaiytjxIz3pqGqlar6dVWdDPwD8O+xWruq/j9VPctbV4G7ehuIqv5dVS8HhgPPAv8Tm9WduDtY5zFcmeazwJuquqe3MZv0YQne9HcZIpIVdwsBvwO+KyJFIjIM+D6unIGIXCoiJ4uIABW40kxURKaKyPneQc1aoAZXEukxEckUkatEpEBVG4Bjcc95ABgqIgVxq7QbdweeBeYAN+Nq8sZ0mSV40989j0vGsdsPgTuB1cB64F3gbW8awBTgr8Bx4E3gv1V1Ga7+/hPgEK78Mhz4VgLiuwbYISLHgC/h6uyo6iZcQt/m9egZ3UncbfJq8U8Dk4BnEhCvSSNiF/wwpn8Tke8Dp6jq1Z0ubEwcO5nCmH5MRIYAN9Cyt40xXWIlGmP6KRH5Z2A38GdVfS3V8ZiBx0o0xhjjU9aCN8YYn+pXNfhhw4bpxIkTUx2GMcYMGGvWrDmkqkVtzetXCX7ixImsXr061WEYY8yAISI725tnJRpjjPEpS/DGGONTluCNMcanklqD90bA+yVunGwFvqCqbyZzm8aY/qehoYHS0lJqa2s7X9i0KSsri7Fjx5KRkdHldZJ9kPWnwF9U9TMikglkJ3l7xph+qLS0lLy8PCZOnIgbB850h6py+PBhSktLmTRpUpfXS1qJxhtF72zcFW9Q1XpVLU/W9owx/VdtbS1Dhw615N5DIsLQoUO7/QsomTX4SUAZ7tqS74jIL71rVbYgIjeKyGoRWV1WVpbEcIwxqWTJvXd68v9LZoIP4caxfkBVZwNVwG2tF1LVh1S1RFVLiora7KvfsWgEXvtP2PJyb+M1xhhfSWaCLwVKVXWl9/gpXMJPrEAQ3rgPPng+4U9tjPGHw4cPU1xcTHFxMSNHjmTMmDFNj+vr6ztcd/Xq1dx0003d2t7EiRM5dOhQb0JOiKQdZFXV/SKyW0SmquoHwAXAe0nZ2OCJcHRHUp7aGDPwDR06lLVr1wLwwx/+kNzcXL7xjW80zW9sbCQUajsdlpSUUFJS0idxJlqy+8F/FXhcRNbjrkr/46RsxRK8Maabrr/+er70pS8xf/58vvnNb7Jq1SoWLFjA7NmzWbhwIR988AEAy5cv59JLLwXcl8MXvvAFzj33XCZPnsx9993X6XbuvvtuZsyYwYwZM7j33nsBqKqq4pJLLmHWrFnMmDGDJ598EoDbbruNadOmMXPmzBZfQD2V1G6SqroWSP5X3+CJ8MGfXT0+EEz65owxPXf7cxt5b++xhD7ntNH5/OCy6d1er7S0lDfeeINgMMixY8dYsWIFoVCIv/71r3z729/m6aefPmGdTZs2sWzZMiorK5k6dSpf/vKX2+2bvmbNGn71q1+xcuVKVJX58+dzzjnnsG3bNkaPHs3SpUsBqKio4PDhwyxZsoRNmzYhIpSX977ToT/OZB08CSL1ULkv1ZEYYwaQz372swSDrlFYUVHBZz/7WWbMmMG//du/sXHjxjbXueSSSwiHwwwbNozhw4dz4MCBdp//9ddf55Of/CQ5OTnk5ubyqU99ihUrVnD66afz0ksvceutt7JixQoKCgooKCggKyuLG264gWeeeYbs7N6fNtSvRpPsscET3f3RHVAwNpWRGGM60ZOWdrLk5DT33P7e977Heeedx5IlS9ixYwfnnntum+uEw+Gmv4PBII2Njd3e7imnnMLbb7/N888/z3e/+10uuOACvv/977Nq1SpefvllnnrqKe6//35eeeWVbj93PJ+04Ce6e6vDG2N6qKKigjFjxgDw6KOPJuQ5Fy1axLPPPkt1dTVVVVUsWbKERYsWsXfvXrKzs7n66qu55ZZbePvttzl+/DgVFRUsXryYe+65h3Xr1vV6+/5owReMBQlagjfG9Ng3v/lNrrvuOu68804uueSShDznnDlzuP7665k3bx4AX/ziF5k9ezYvvPACt9xyC4FAgIyMDB544AEqKyu5/PLLqa2tRVW5++67e739fnVN1pKSEu3xBT/unQljz4DPPJzYoIwxvfb+++9z2mmnpTqMAa+t/6OIrFHVNjuz+KJEc83DK9kXHGkteGOMieOLBL9udzl7GWEJ3hhj4vgiweeGQ+wPjoTqQ1BXmepwjDGmX/BFgs8Jh9grI92Do+1ef9YYY9KKbxL8boa7B1amMcYYwCcJPjccYkfEG2rYErwxxgA+6QefnRlkZ2U2ZBVYgjfGnODw4cNccMEFAOzfv59gMEjs+hOrVq0iMzOzw/WXL19OZmYmCxcuPGHeo48+yurVq7n//vsTH3gv+SLB54ZDHK9rtFEljTFt6my44M4sX76c3NzcNhN8f+aLEk1OOER1vSV4Y0zXrVmzhnPOOYe5c+dy4YUXsm+fG6zwvvvuaxqy94orrmDHjh08+OCD3HPPPRQXF7NixYp2n3PHjh2cf/75zJw5kwsuuIBdu3YB8Pvf/54ZM2Ywa9Yszj77bAA2btzIvHnzKC4uZubMmWzevDnh++iLFnxOOERVXSRu2OAoBHzx3WWM//z5Ntj/bmKfc+TpcPFPury4qvLVr36VP/zhDxQVFfHkk0/yne98h0ceeYSf/OQnbN++nXA4THl5OYWFhXzpS1/qUqv/q1/9Ktdddx3XXXcdjzzyCDfddBPPPvssd9xxBy+88AJjxoxpGgb4wQcf5Oabb+aqq66ivr6eSCTSq39BW3yRBXPDQeojURrzJ9iwwcaYTtXV1bFhwwY+9rGPUVxczJ133klpaSkAM2fO5KqrruK3v/1tu1d5as+bb77JlVdeCcA111zD66+/DsBHPvIRrr/+en7xi180JfIFCxbw4x//mLvuuoudO3cyaNCgBO6h45sWPEBN7jjywBs2eEwqQzLGtKcbLe1kUVWmT5/Om2++ecK8pUuX8tprr/Hcc8/xox/9iHff7f2vjQcffJCVK1eydOlS5s6dy5o1a7jyyiuZP38+S5cuZfHixfz85z/n/PPP7/W24vmiBR9L8JWDvLHgj25PYTTGmP4uHA5TVlbWlOAbGhrYuHEj0WiU3bt3c95553HXXXdRUVHB8ePHycvLo7Ky87PkFy5cyBNPPAHA448/zqJFiwDYunUr8+fP54477qCoqIjdu3ezbds2Jk+ezE033cTll1/O+vXrE76fvkjwuV6CPxYeARKwA63GmA4FAgGeeuopbr31VmbNmkVxcTFvvPEGkUiEq6++mtNPP53Zs2dz0003UVhYyGWXXcaSJUs6Pcj6s5/9jF/96lfMnDmT3/zmN/z0pz8F4JZbbuH0009nxowZLFy4kFmzZvE///M/zJgxg+LiYjZs2MC1116b8P30xXDBr35YxnWPrOLpLy9g7pJzYNyZ8OlfJCFCY0xP2HDBiZGWwwXnht01FY/XRaBgPFTsTnFExhiTer5I8LEafFVdIxSOg/JdKY7IGGNSzx8JPtMl+ON1jVAwznWTjDSkOCpjTLz+VA4eiHry//NFgs9t3YLXKBzbk+KojDExWVlZHD582JJ8D6kqhw8fJisrq1vrJbUfvIjsACqBCNDY3oGA3mpRohk5zk0s3+3ObDXGpNzYsWMpLS2lrKws1aEMWFlZWYwdO7Zb6/TFiU7nqeqhZG4gMxQgMxhwB1kLx7uJdqDVmH4jIyODSZMmpTqMtOOLEg1ATjjoWvAF3jdcuSV4Y0x6S3aCV+BFEVkjIje2tYCI3Cgiq0VkdW9+vrkBxxohFIbckVBhPWmMMekt2Qn+LFWdA1wMfEVEzm69gKo+pKolqloSG4C/J5rGhAfrKmmMMSQ5wavqHu/+ILAEmJesbWVnBqmq9xJ8wTgr0Rhj0l7SEryI5IhIXuxv4OPAhmRtLycccgdZwbXgj+1x48IbY0yaSmYLfgTwuoisA1YBS1X1L8naWG6sBg+uBR+ph+MHkrU5Y4zp95LWTVJVtwGzkvX8reXEJ/j4rpL5o/oqBGOM6Vd8002yxUHWgtjJTnag1RiTvnyT4GP94FXV1eDBTnYyxqQ1HyX4EFGF2oYohPNg0GBrwRtj0ppvEnxswLEWZRrrKmmMSWO+SfCxIYNbHGi1Eo0xJo35J8G314K34UmNMWnKNwm+xZjw4A60NlRBzdEURmWMManjmwSf412XtbreO5vVukoaY9KcbxL8CQdZraukMSbN+SbB55xQopng7q0Fb4xJU75L8E0t+EGDISPHukoaY9KWfxJ8pqvBV8VGlBRxZRor0Rhj0pRvEnwoGCArI9A8JjxA3kio3J+6oIwxJoV8k+DBnezUVKIBd+k+GzLYGJOm/JXg44cMBsgb4RK8nexkjElD/k7wuSPdhT/sZCdjTBryVYLPDQdblWiGu3sr0xhj0pCvErxrwUeaJ+SNdPd2oNUYk4Z8mOBblWjAWvDGmLTkqwSf27oXTd4Id28teGNMGvJVgj+hBR/Oc2ezWgveGJOGfJXgc8NBquojRKNx3SLzRlgL3hiTlnyV4GPj0VQ3xB1otZOdjDFpypcJvs2TnYwxJs34KsGfMCY8QO4IqLQEb4xJP0lP8CISFJF3RORPyd5Wmy343BFQXwn1VcnevDHG9Ct90YK/GXi/D7bTdNm+ll0l7WQnY0x6SmqCF5GxwCXAL5O5nZjmC2/HH2T1+sJbHd4Yk2aS3YK/F/gmEG1vARG5UURWi8jqsrKyXm2s7YOs1oI3xqSnpCV4EbkUOKiqazpaTlUfUtUSVS0pKirq1TbbPshqwxUYY9JTMlvwHwH+QUR2AE8A54vIb5O4vbZb8NlDIJBhCd4Yk3aSluBV9VuqOlZVJwJXAK+o6tXJ2h5AdkbsuqxxCV7EukoaY9KSr/rBBwJCdmaQ4/EHWcGNC3/cavDGmPQS6ouNqOpyYHlfbCs3HKK8pr7lxLyRcHRnX2zeGGP6DV+14AGKxxXyxpbDLQccyx1hLXhjTNrxXYK/cPpI9h+rZf2eiuaJeSOh+jA01re/ojHG+IzvEvwFpw0nGBBe2BjXYo+d7FR1MDVBGWNMCvguwRdmZ3Lm5CEtE3ye9YU3xqQf3yV4cGWabWVVbDlY6SbEWvDWVdIYk0Z8meA/Ps212F/Y6CX0pvFo7ECrMSZ9+DLBjyzIYta4wuYyTe5wQKwFb4xJK75M8AAXTh/B+tIK9pbXQDADsodaC94Yk1Z8nOBdmebFWCs+b6S14I0xacW3Cf6kolxOHp7LS+97Sb1wPBzektqgjDGmD/k2wQNccOpwVm0/4oYPHnsGHN4MVYdSHZYxxvQJXyf4804dTkNEeX1zGYxf4Cbueiu1QRljTB/xdYKfO2EweVkhXtl0EMbMgWAYdr2Z6rCMMaZP+DrBZwQDnH1KEcs+KCMayHRJ3lrwxpg04esED3De1OGUVdbx3r5jrkyzby3UV6U6LGOMSTrfJ/hzpxYhgivTjF8A0UbY0+FlYo0xxhd8n+CH5YaZObbQJfhx8wCBnVaHN8b4n+8TPMD5U4ezrrScw5FBMGK6HWg1xqSF9Ejwpw5HFZZ/UAbjz4TSv0OksfMVjTFmAEuLBD99dD5FeWFe+cCrw9cfhwPvpjosY4xJqrRI8IGA8NHTRrB800FqR89zE627pDHG59IiwQNcOnMUVfURlu/LgILxsPONVIdkjDFJ1aUELyI3i0i+OA+LyNsi8vFkB5dI8ycNYWhOJs+t3wcnnw8f/gX2rk11WMYYkzRdbcF/QVWPAR8HBgPXAD9JWlRJEAoGuPj0kbzy/kGqF30HsofBU/8EdZWpDs0YY5KiqwlevPvFwG9UdWPctLZXEMkSkVUisk5ENorI7b0JNBEuOX00NQ0Rlu2KwGcehqM74E//DqqpDs0YYxIu1MXl1ojIi8Ak4FsikgdEO1mnDjhfVY+LSAbwuoj8WVVTdnRz3qQhFOWF+dP6vVxy9UI491uw7Ecw7BQYMQ0i9ZA/xjshyhhjBrauJvgbgGJgm6pWi8gQ4J86WkFVFTjuPczwbiltKgcDwuIZI3ni77upqmskZ9HXYccKWHZn80IShK+9CwVjUheoMemi6hDcNweufBImLEh1NL7T1RLNAuADVS0XkauB7wIVna0kIkERWQscBF5S1ZVtLHOjiKwWkdVlZWXdib1HLpk5mrrGKC9vOgiBIFz1NHzxZbjxVbjuOdAovP1Y0uMwxgBHd0JdhTv50CRcVxP8A0C1iMwCvg5sBX7d2UqqGlHVYmAsME9EZrSxzEOqWqKqJUVFRd0IvWdKJgxmRH6Y59btdRNCmTC2BEYXw6SzYcrHYM2j0Fif9FiMSXu1R919+a7UxuFTXU3wjV7J5XLgflX9LyCvqxtR1XJgGXBR90NMrEBA+NScsfz1/QNsPtBGD5oz/hmOH4BNf+r74IxJN7VeIcASfFJ0NcFXisi3cN0jl4pIAFdTb5eIFIlIoff3IOBjwKbeBJsoNy6aTE5miLtf+vDEmSdfAIUT4O8P931gxqSbmnJ3bwk+Kbqa4P8R1yvmC6q6H1dy+T+drDMKWCYi64G/42rw/aJZPDgnky8umsSfN+xnfWl5y5mBIJR8AXa+DgffT02AxqSL2rgEb92VE65LCd5L6o8DBSJyKVCrqh3W4FV1varOVtWZqjpDVe9IQLwJc8NZkxicncF/vthGK372Ne76rdaKNya5YiWahiqoOZraWHyoq0MVfA5YBXwW+BywUkQ+k8zAki0vK4Mvn3sSr31Yxspth1vOzBkKMz4F635nZ7oak0w1cb+gy3emLg6f6mqJ5jvAGap6napeC8wDvpe8sPrGtQsmMiI/zP9+4QO09c/Dki+4YYU3LklNcMakg9oKEC8NWR0+4bqa4AOqejDu8eFurNtvZWUEufmCU1iz8yhL393XcubYM6DoVHi7096gxpieqi2HoVPc35bgE66rSfovIvKCiFwvItcDS4HnkxdW3/nHM8Zx6sg8/uP5TdQ2RJpniMCca90JGAfeS12AxvhZbQUUjodwgSX4JOjqQdZbgIeAmd7tIVW9NZmB9ZVgQPjBZdPZU17DL17b1nLmzCsgkAHv/CY1wRnjdzXlMKjQJXlL8AnX5TKLqj6tqv/u3XxVmF5w0lAunjGS/16+lf0Vtc0zcobCaZe6g62NdakL0Bi/qi2HrAJL8EnSYYIXkUoROdbGrVJEjvVVkH3h24tPI6LKT/7cqu/7nGtd9y07s9WYxFJ1JZqsuBa89YVPqA4TvKrmqWp+G7c8Vc3vqyD7wrgh2fzzokk8u3Yvb++K64876Vx3iT872GpMYtVVusH9YiWa+uPWFz7BBnxPmET6l3NPZnhemNufe49o1GtJBAIw5xrYthyObOtwfWNMN8ROcoqVaMD6wieYJfg4OeEQt150Kut2l7PknT3NM2ZfA4EQrPpF6oIzxm9iwxTESjRgdfgEswTfyidnj2HWuELu+ssmquoa3cT8UTD9k/DOb+3MVmMSJXYWa4sWvCX4RLIE30ogIHz/0mkcrKzjv5dvaZ4x/0tQdwzW/i51wRnjJ7ESzaBCd7O+8AlnCb4NcycM5hPFo/nFiu0cqfIu/DG2BMaUwMoHIdrZ5WiNMZ2qjWvBg3WVTAJL8O3457MnU98Y5S8b9jdPPPPLcGQrbPlr6gIzxi+aDrIWuntL8AlnCb4d00blM7kohz+uizvYOu1yyBvlWvHGmN6pKQcEwl6Pa+sLn3CW4NshIlw2czQrtx/hwDHv7NZgBpxxA2x9GQ5tTm2Axgx0teWQle+6IoP1hU8CS/AduGzWaFRh6fq4kSbnXOe6TNqJT8b0Tuws1hjrC59wluA7cPLwXKaNyue59XubJ+YOh1MucuPTRBpSF5wxA11NefMBVrCukklgCb4Tl80azTu7ytl9pLp54uxroKoMPnwhdYEZM9DVVrjukTGDJ7r7Q21cRtP0iCX4Tlw6cxQAf4ov05z8UcgdacMIG9Mbta1a8Fn57uIfpWtSF5PPWILvxLgh2cweX8gf18WVaYIhKL4SNr8Ix/a2v7Ixpn2ta/DgrqS2Z7X1pEkQS/BdcNnM0by/7xjbD1U1T5x9tRsJb+3/S11gxgxksYt9xBs715U/rQ6fEJbgu+D8U4cDsGJzWfPEoSfBhLPc+DR2Zqsx3dNYB401LUs04M4WB3epTNNrluC7YMLQbMYNGcSKzYdazphzDRzdDttfTU1gxgxUrc9ijRkxHUKDYI/V4RMhaQleRMaJyDIReU9ENorIzcnaVrKJCGedXMRbWw/TEIlrrU/7BOQMh7/9NHXBGTMQeSNJHpdciu94kVXbj7jpwQwYXWwt+ARJZgu+Efi6qk4DzgS+IiLTkri9pFo0ZRiVdY2s213ePDEjCxb8C2xbBnvfSV1wxgw0Xgv+UGQQ5dUNvPrhweZ5Y0tg33porE9RcP6RtASvqvtU9W3v70rgfWBMsraXbAtPGkpAOLFMU3KDG+b09XtSE5gxA5E3kmSV5ALw7p64SzyPKYFIHRx4NxWR+Uqf1OBFZCIwG1jZF9tLhsLsTE4fW9jyQCu4vrvzvgjv/REObWl7ZWNMS7ESTSAHgHdLy9FY18ixsQOtq1MRma8kPcGLSC7wNPA1VT3WxvwbRWS1iKwuKys78Qn6kUUnD2NdaQUVNa2GKJj/ZQiF4W/3piYwYwYarwV/TF0L/mh1A6VHa9y8/DFu1FZL8L2W1AQvIhm45P64qj7T1jKq+pCqlqhqSVFRUTLD6bVFU4YRiSpvbj3cckZukRu+YN0TULGn7ZWNMc2aEnx206R393g9a0RgzFw70JoAyexFI8DDwPuqeneyttOXZo8fTHZmkNe3tPFLY+FXQQLw/C12Fp4xnamtgNAgjkeCTZOaEjy4M1qPboeqw22sbLoqmS34jwDXAOeLyFrvtjiJ20u6zFCABZOH8nrrA60AgyfAR38AHyx1Jz8ZY9rnjSRZVe8ubH/y8FzeLY1P8F4d3vrD90oye9G8rqqiqjNVtdi7PZ+s7fWVs6YMY8fh6pajS8bM/zJMXAR/uQ2O7ujz2IwZMGrdMAU19RFE4IyJg1kff6B19GwIZMCGp1Mb5wBnZ7J204KThgI0n5gRLxCATzzgSjVLvgTRSB9HZ8wA4Q00VlUXITsjyMyxhRyrbWRXrOGUmQML/xXWPwHbX0ttrAOYJfhuOmV4HnnhEGt2tXNZscJxcPH/hl1vwhv39W1wxgwUXommpqGR7HCI08e4MWla1OHPuRUGT4LnvgYNtSkKdGCzBN9NgYAwe8Jg1uzo4LqRs66A0y6DZT+GAxv7LjhjBgrvYh9VdRGyM4OcMiKPzGCgZR0+YxBceg8c2Qor/m/qYh3ALMH3QMmEwXx4sPLE/vAxInDpvW6kvGf+l51ybUxr3sU+qusjZGeGyAwFOG1UHuvjEzzASefBzH90Z4offD81sQ5gluB7YO6EwajCO+2VaQByhsFl97nTrV/9Sd8FZ0x/F41C7THIKqS6vpHsTNdVcsaYAjbsqSAabdXN+MIfuzPGH/8cHNmWgoAHLkvwPVA8rpCAwNs7O0jwAKcuhuKrXevDDhQZ49QdAxQGFXoteJfgZ44toLKukZ2te6jlDIOrn4H64/DIxVD2Qd/HPEBZgu+BnHCI00bls7qzBA9w0X/AkMnwm0/BmseSH5wx/Z13Fqsr0bRswQOsLy0/cZ3RxXD9UkDhV4tdg8lOKOyUJfgeKpkwmLW7y2mMdHI1p6x8uOElmHgWPHcTLP261eRNetvvjRKZ5VrwOZkhAE4qcuPSNI1J09qIafBPf3YHXx+7DP57Abz1ABzaDNVH7MpqbQilOoCBau7EITz25k427a9sanm0K3sIXPUUvHy76zq55a9uaIPiq9yb1Ri/i0Zg23I3IN/21yB7KIyZQ3X9RgZ5LfisjCAFgzI4cKyDLpFDT4J/eQs2PgNrHnUnFcZIAArHw+g5biybEdOhYKwbuCycm9Td668swffQ3AmDAViz82jnCR4gGIKP/3+uJf/qXa4lv+w/XJfKUy6E8Qvc1WyM6c9UXbKONrqa+LG9ULmvafhfRNwy6i1TfQR2vuHOC6k7Brkj4eN3wtzrIZxHdf06csLNaWhEfpj9FZ30eQ/nwpxr3W3/BtcVufowVB+Cw1vcIGUbW41tmJHdfAtleuUddV8KWYUwaN61yd0AABOvSURBVLC7AHjGIHfJwFDY2982fhWIgAQhEAQk9o9p/r9EI+4x4v0/om6aeic+BjIgEHLzYv/TcC6c9+0uvwxdZQm+h8YUDmJUQRardx7luoUTu77iKRfClI/Dzr/BGz+DlT+HN++HcD6Mm+9aHSOmw9CTXesje5g7Q7a+Go4fgLpKCGW5N2AoCzK9N20g2Pm2uyMahYYq94Zt/UaNNkKkASL17m9VL56we+NH6txFlaONzc+nUTctUu/dGrznamh+To26D1wg1PwBkIDbfrSxeT2NNH9AI/XQUAMN1d4HS05MMk0fOI8Emm+x5cGLqcHdN9S4W2Nt8we6KbaAe9wUk1dyi33oY/+f2LY14vYtElu+zi0f+6C3+LBHob7K25/G5tdagi6Whmr3PJnZ7mzPUJZ7T9Qdc+vFYpSg93o1upsEXGwS8F67OlcqjDY2x4e4RkYg5P5G3fTWt+4aOgVmfBomLYJTL21KnpGoUtsQZVBG83t3RH4WByrruv7cI2e4W2uVB+DQh+7L59geqDrk/ncNNe59GHtvRRtdn/zYl0Psf9xY5/4HEmh+bcB7X0W991arM9UDQe81jSV+dctLoPk9Ay3fF7H3X06RJfj+Zs6EwZ33pGmLiGvJTzzLfTi3vwabX4TSNe5nbDSuf30wE4JhqK/s+DlDWa7ffVahq/vHEkMg5L2Bj7iDW7GE3FZMsTdbQ61rneGDg1itE3nsi6GtZCUB9wENZnitPa8l1+JDHW1OioEM1xoMeL+8YolSgs2JMhBqjiGY4ZJy0P36cx/02Bdc1MUlQcgd7n1ph1zCiX1Z5o5wiT0Qckmo7riblzcKiqa65461sDXS/IUTCDbvbzTqfk0Gw17soeblYl8IEe/91/SeiPtyaPqSC7oY80ZB/mhXcokXCLrlMrJdibINNQ0uQeaEmxP88LwsthxsYzC/7sob4W5pzhJ8L5RMGMzS9fvYV1HDqIIe1tLDeXDqJe4G7sN16EM4st39/D22x33Ic4e7n7dZ+e5D3VgHjTXNLc26SpfIa8tdH+PGOqg56lqMWYUuAQwqdMko9sFtonGJT11iC+dBZq7XmvOmxz60sftgpkta6rWkG+tcYgl6vy4CwbjWjzS38oOZca3XYMtE2NTqbGgZU/z2mlpV3nPGflY3xer9EpCgazl1RGP7TufLmoSqrnO/8LIzm9PQyIIwByvriEaVQEDaW9V0kSX4XiiZ4Fomq3cc5bJZCTpYGsxoLtOY5JO4Eo3pU9X1rgUf6yYJrkQTiSqHq+opygunKjTfsCZLL5w2Ko+czCB/39HGyJLGmA7FxoKPb8EPz8sC6LgnjekyS/C9EAoGmDNhcNtDBxtjOtR2C9612g9WWoJPBEvwvTRv4hA+OFBJebWdvGRMd8QSfPxB1hH5sRZ8N3rSmHZZgu+lMyYNQdXV4Y0xXRc7yDooo7lEE6u7W4kmMSzB91LxuEIygwGrwxvTTW214DOCAYblZloLPkEswfdSVkaQmWMLWGl1eGO6pdo7yDoorgYP7kDrQWvBJ4Ql+AQ4Y9IQNuypaHrDGmM619SCz2zZW3tEfpgDdpA1ISzBJ8C8SUNojCprd7UxzKkxpk1VXoKPH6oAvOEKrESTEJbgE2DuhMGIYGUaY7qhpr6RQRnBE85YHZ6fxaHjdZ0PxW06ZQk+AfKzMjhtZL4daDWmG6rqIy0OsMaMyA+jCoeOW9fj3rIEnyDzJg3h7V1HqW+0VocxXVFTHznhACvACDubNWGSluBF5BEROSgiG5K1jf5k3qQh1DZE2bC3ovOFjTFU1TWecIAV4k92sgTfW8lswT8KXJTE5+9X5k1yA4+9ufVwiiMxZmCoaWinBe8NV9CtceFNm5KW4FX1NSBtitLDcsOcNiqfFZvLUh2KMQNCey34oblhAoL1hU+AlNfgReRGEVktIqvLygZ2clw0ZRhrdh6lqs76wxvTmep2avDBgFCUF7YSTQKkPMGr6kOqWqKqJUVFRakOp1cWTRlGQ0RtdEljuqC6PkJOGwkeXB1+v/WF77WUJ3g/OWPiEMKhAK9ZmcaYTrkWfNvXHLLhChLDEnwCZWUEmTdpCK9vTsA1JY3xuer6xg5a8FaiSYRkdpP8HfAmMFVESkXkhmRtqz9ZNGUYmw8eZ19FTapDMabfikaV6vpIi4t9xBuRn8XR6gbqGiN9HJm/JLMXzedVdZSqZqjqWFV9OFnb6k/OOtkdR7BWvDHtq/USd3a47RJN05WdrA7fK1aiSbBTR+YxLDfMCkvwxrSrqi42kmTbLfjh3slOdum+3rEEn2CBgHDWyUP525ZDRKOa6nCM6ZdqYiNJtnOQtXm4AmvB94Yl+CRYNKWIw1X1vLfvWKpDMaZfqvKundDRQVaw4Qp6yxJ8EiyaMgyAVzYdTHEkxvRP1U0t+LYT/ODsTAoGZbBxrzWSesMSfBIMz89i3sQh/GHtHlStTGNMa7Grn+W0c5A1EBAWTRnGqx+WWamzFyzBJ8nls0eztazKWiDGtKG6nas5xTtv6nDKKuus1NkLluCT5JLTR5ERFJ59Z0+qQzGm3+msBQ9wzlTX5XiZlTp7zBJ8khRmZ3Lu1OH8cd1eIvYT05gWYi349k50AjdC68yxBSz/0Ib+6ClL8En0ydljOFhZZ2PEG9NKdV3nCR7g3KnDeWfXUcqr7fJ9PWEJPonOP3U4eeEQS6xMY0wLzS349ks0AOdNLSKq8JqdONgjluCTKCsjyMWnj+SFjfubTuwwxrgafDgUIBiQDpebObaQwdkZLLc6fI9Ygk+yTxSP4XhdIy+9fyDVoRjTb3Q00Fi8YEA455Qi6y7ZQ5bgk2z+5KGMH5LNA8u32hvUGE9VfWOn5ZmY804dzuGqet7dYxe07y5L8EkWDAhf//gpvL/vGH9ctzfV4RjTL9TUR8gJd96CBzf0hwj8Ya19frrLEnwfuGzmaKaPzuc/X/zAxrc2Bqjq4GpOrQ3JyeRzc8fxyN+287KVOrvFEnwfCASE2y4+ldKjNTz+1q5Uh2NMylXXtX81p7bcfvl0ZozJ52tPrmX7oaokRuYvluD7yKIpRXzk5KHcv2wLlbUNqQ7HmJTq6kHWmKyMIA9ePZdQQLjx16upqmtMYnT+YQm+D9160akcqarn/774YapDMSalqrtxkDVm7OBsfvb5OWwtO84VD73Fmp1HkxSdf1iC70MzxxZy/cKJPPrGDh57Y0eqwzEmZbrbgo85a8ow7r9yDgeO1fLpB97g5ifeYWvZcRu1tR3d+wo1vfa9S6dRerSGHz63kZEFWVw4fWSqQzKmz7kE37P0s/j0UZxzShEPLN/KQyu28Ye1exk3ZBBnTyli3qQhnDIij8lFOYRD3f8C8RtL8H0sGBB+9vnZXPGLt7jpd+/wmxvmM2/SkFSHZUyfUVWvRNPzBJwTDvGNC6dy1Znjeem9A7z2YRlL3tnD4ytdJ4aAwKiCQQzJyWRITib5gzLICgXIygiSGQqQEQyQGRSCgQABcR0hAERAEETccwRECIh7HBN/7q1480SkaXrr5xBana3bxsm7WRlB/mHW6B7/P9pjCT4FBmUGefi6Ej7zwBt8/hdv8S/nnsRXz59CZsgqZsb/6hqjRBWyu9gPviOjCgZx7YKJXLtgIvWNUbYdOs7mA8fZfKCS0qM1HK2u50hVPbuOVFPbEKGuMUptQ4TGiFIfiSZgbxJjWG7YEryfDMsN84evnMXtf9rIz17ZwkvvHeCOy2dwxsTBiHQ8PocxA1nTQGMdXOyjJzJDAU4dmc+pI/O7tLyqEokqCqhCNK6OH1VFFSKqLc5Ab13qV2/ZpnWVpudTlNYnr7d3rCCQpM+8JfgUKsjO4O7PFbN4xii+veRdPvfzN5k6Io8r5o3j8uIxDMnJTHWIxiRcrItjdgcX++gLIkIo6O/GVFL/wyJyEfBTIAj8UlV/ksztDVQfnTaCM08aynPr9vLEql3c/tx73P7ce0wdkcf8yUOYM34wJw/P5aSi3HYvUmzMQFHT4FrwOT08yGq6Lmn/YREJAv8FfAwoBf4uIn9U1feStc2BLDcc4vPzxvP5eePZuLeCZZsOsnL7EZ5aU8qv39wJuIM3o/KzGFU4iJEFWYzMz2JwdgaF2ZkUZmeQnRkkKyPIoAx3Hw4FCGcEyQwGyAwFyAwGCAWFUECsDGT6TCSqHK9rpKyyjrd3HW26BF9vDrKarknmV+g8YIuqbgMQkSeAywFL8J2YPrqA6aML+FegIRJlW1kVWw4eZ8vB4+w8XMW+ilre23uMZZsONtUzu6uph0BACIo0PW7qERB7DF4PguaeBPG9BWJa9xRwPQlo8UXS3ndKR8/T9jLtPU/7X1rd/jrrxfdfsr46k/2l3N6zu5py27XjWO06qnF166jSGFUao1HqG6MnvEcLszO4aPpIiscVJnYHzAmSmeDHALvjHpcC81svJCI3AjcCjB8/PonhDEwZwQBTR+YxdWRem/PrGiOUVzdQUdNAdX2EmvoINQ2N1DdGqW2IUtcYob4xSl1jlPpIlGjswxdxB4ZiB5HcB7X5QJN68zTuoJETm98cw4kHnk5cT2nnRBRt88+Wi8RtoP1l2pnRwTrtLt+Lk2aSdrpNks/jaff18Qhy4jeAuu6FAW+W+9s9DgUDhAJCRjBAbjhEXlaIwuxMiscVMHlYblO3RJNcKS+CqepDwEMAJSUldjpaN4VDQUbkBxmRn5XqUIwx/UwyO17vAcbFPR7rTTPGGNMHkpng/w5MEZFJIpIJXAH8MYnbM8YYEydpJRpVbRSRfwVewHWTfERVNyZre8YYY1pKag1eVZ8Hnk/mNowxxrTNBj8xxhifsgRvjDE+ZQneGGN8yhK8Mcb4lPSnS12JSBmws4erDwMOJTCcgSAd9xnSc7/TcZ8hPfe7u/s8QVWL2prRrxJ8b4jIalUtSXUcfSkd9xnSc7/TcZ8hPfc7kftsJRpjjPEpS/DGGONTfkrwD6U6gBRIx32G9NzvdNxnSM/9Ttg++6YGb4wxpiU/teCNMcbEsQRvjDE+NeATvIhcJCIfiMgWEbkt1fEki4iME5FlIvKeiGwUkZu96UNE5CUR2ezdD051rIkmIkEReUdE/uQ9niQiK73X/ElvOGpfEZFCEXlKRDaJyPsissDvr7WI/Jv33t4gIr8TkSw/vtYi8oiIHBSRDXHT2nxtxbnP2//1IjKnO9sa0Ak+7sLeFwPTgM+LyLTURpU0jcDXVXUacCbwFW9fbwNeVtUpwMveY7+5GXg/7vFdwD2qejJwFLghJVEl10+Bv6jqqcAs3P779rUWkTHATUCJqs7ADTF+Bf58rR8FLmo1rb3X9mJgine7EXigOxsa0AmeuAt7q2o9ELuwt++o6j5Vfdv7uxL3gR+D29/HvMUeAz6RmgiTQ0TGApcAv/QeC3A+8JS3iB/3uQA4G3gYQFXrVbUcn7/WuOHLB4lICMgG9uHD11pVXwOOtJrc3mt7OfBrdd4CCkVkVFe3NdATfFsX9h6Tolj6jIhMBGYDK4ERqrrPm7UfGJGisJLlXuCbQNR7PBQoV9VG77EfX/NJQBnwK6809UsRycHHr7Wq7gH+E9iFS+wVwBr8/1rHtPfa9irHDfQEn3ZEJBd4Gviaqh6Ln6euz6tv+r2KyKXAQVVdk+pY+lgImAM8oKqzgSpalWN8+FoPxrVWJwGjgRxOLGOkhUS+tgM9wafVhb1FJAOX3B9X1We8yQdiP9m8+4Opii8JPgL8g4jswJXfzsfVpgu9n/Hgz9e8FChV1ZXe46dwCd/Pr/VHge2qWqaqDcAzuNff7691THuvba9y3EBP8GlzYW+v9vww8L6q3h0364/Add7f1wF/6OvYkkVVv6WqY1V1Iu61fUVVrwKWAZ/xFvPVPgOo6n5gt4hM9SZdALyHj19rXGnmTBHJ9t7rsX329Wsdp73X9o/AtV5vmjOBirhSTudUdUDfgMXAh8BW4DupjieJ+3kW7mfbemCtd1uMq0m/DGwG/goMSXWsSdr/c4E/eX9PBlYBW4DfA+FUx5eE/S0GVnuv97PAYL+/1sDtwCZgA/AbIOzH1xr4He44QwPu19oN7b22gOB6Cm4F3sX1MurytmyoAmOM8amBXqIxxhjTDkvwxhjjU5bgjTHGpyzBG2OMT1mCN8YYn7IEb0wCiMi5sdEujekvLMEbY4xPWYI3aUVErhaRVSKyVkR+7o01f1xE7vHGIn9ZRIq8ZYtF5C1vHO4lcWN0nywifxWRdSLytoic5D19btwY7o97Z2QakzKW4E3aEJHTgH8EPqKqxUAEuAo3sNVqVZ0OvAr8wFvl18CtqjoTdxZhbPrjwH+p6ixgIe6sRHAjfH4Nd22CybixVIxJmVDnixjjGxcAc4G/e43rQbhBnaLAk94yvwWe8cZkL1TVV73pjwG/F5E8YIyqLgFQ1VoA7/lWqWqp93gtMBF4Pfm7ZUzbLMGbdCLAY6r6rRYTRb7Xarmejt9RF/d3BPt8mRSzEo1JJy8DnxGR4dB0HcwJuM9BbMTCK4HXVbUCOCoii7zp1wCvqruaVqmIfMJ7jrCIZPfpXhjTRdbCMGlDVd8Tke8CL4pIADea31dwF9SY5807iKvTgxu29UEvgW8D/smbfg3wcxG5w3uOz/bhbhjTZTaapEl7InJcVXNTHYcxiWYlGmOM8SlrwRtjjE9ZC94YY3zKErwxxviUJXhjjPEpS/DGGONTluCNMcan/n89INZMZ2HtHwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDxbiZVgzhmN",
        "colab_type": "text"
      },
      "source": [
        "# Model Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSlNVbzD0hCs",
        "colab_type": "text"
      },
      "source": [
        "## 인코더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoP4YKg3CeXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49a60871-ef9b-470f-f214-2573683df134"
      },
      "source": [
        "# 인코더 모델\n",
        "model_enc = Model(inputs, addNormEnc2)\n",
        "model_enc.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, 10)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inputEmbedding (Embedding)      (None, 10, 128)      2648192     inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_encoded_inputs (Ten [(None, 10, 128)]    0           inputEmbedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 1, 10, 32)]  0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 10, 32)]  0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split (TensorFlowOp [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_1 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 4, 10, 32)]  0           tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "                                                                 tf_op_layer_split[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "                                                                 tf_op_layer_split_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 32)       4096        tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2 (Tens [(None, 4, 10, 10)]  0           tf_op_layer_concat[0][0]         \n",
            "                                                                 tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 10, 32)]  0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorFlow [(None, 4, 10, 10)]  0           tf_op_layer_BatchMatMulV2[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_2 (TensorFlow [(None, 1, 10, 32)]  0           tf_op_layer_ExpandDims_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 4, 10, 10)    0           tf_op_layer_RealDiv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 4, 10, 32)]  0           tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "                                                                 tf_op_layer_split_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_1 (Te [(None, 4, 10, 32)]  0           activation[0][0]                 \n",
            "                                                                 tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(None, 10, 128)]    0           tf_op_layer_BatchMatMulV2_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "addEnc1 (Add)                   (None, 10, 128)      0           tf_op_layer_Reshape[0][0]        \n",
            "                                                                 tf_op_layer_encoded_inputs[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "addNormEnc1 (LayerNormalization (None, 10, 128)      256         addEnc1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encFFN_1 (Dense)                (None, 10, 200)      25800       addNormEnc1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "encFFN_2 (Dense)                (None, 10, 128)      25728       encFFN_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "encFFNDropout (Dropout)         (None, 10, 128)      0           encFFN_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "addEnc2 (Add)                   (None, 10, 128)      0           addNormEnc1[0][0]                \n",
            "                                                                 encFFNDropout[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "addNormEnc2 (LayerNormalization (None, 10, 128)      256         addEnc2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,712,520\n",
            "Trainable params: 2,712,520\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb9RBeHt3BBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question을 입력받아 Answer 생성\n",
        "def genAnswer(question):\n",
        "    question = question.reshape(-1, MAX_SEQUENCE_LEN)\n",
        "    print(f\"question: {question}\")    \n",
        "\n",
        "    # 시작 단어 <START> 초기화\n",
        "    answer = np.zeros(10).reshape(-1, MAX_SEQUENCE_LEN)\n",
        "    answer[0][0] = word2idx['<START>']\n",
        "    print(f\"answer: {answer}\")\n",
        "\n",
        "    for i in range(MAX_SEQUENCE_LEN):\n",
        "        pred = model.predict([question, answer])\n",
        "        # print(pred[0][i])\n",
        "        # print(pred[0][i+1])\n",
        "        # print(pred[0][i+1].shape)\n",
        "        # break\n",
        "        nextWord = np.argmax(pred[0][i+1])\n",
        "        print(nextWord)\n",
        "        \n",
        "        if nextWord == word2idx['<END>'] or nextWord == word2idx['<PADDING>']:\n",
        "            break\n",
        "        \n",
        "        # 다음 예상 단어에 answer 추가\n",
        "        answer[0][i+1] = nextWord \n",
        "    \n",
        "    return \" \".join([idx2word[an] for an in answer[0]])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULmN37ve-DTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "3f0706cb-fa81-442e-a911-431358142c75"
      },
      "source": [
        "# Chatting\n",
        "def chatting(n=100):\n",
        "    for i in range(n):\n",
        "        question = input('Q : ')\n",
        "        \n",
        "        if  question == 'quit':\n",
        "            break\n",
        "        \n",
        "        q_idx = []\n",
        "        for x in question.split(' '):\n",
        "            if x in word2idx:\n",
        "                q_idx.append(word2idx[x])\n",
        "            else:\n",
        "                q_idx.append(word2idx['<OOV>'])   # out-of-vocabulary (OOV)\n",
        "        \n",
        "        # <PADDING>을 삽입한다.\n",
        "        if len(q_idx) < MAX_SEQUENCE_LEN:\n",
        "            q_idx.extend([word2idx['<PADDING>']] * (MAX_SEQUENCE_LEN - len(q_idx)))\n",
        "        else:\n",
        "            q_idx = q_idx[0:MAX_SEQUENCE_LEN]\n",
        "        \n",
        "        answer = genAnswer(np.array(q_idx))\n",
        "        print('A :', answer)\n",
        "\n",
        "chatting(100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q : 가끔 궁금해\n",
            "question: [[390 585   0   0   0   0   0   0   0   0]]\n",
            "answer: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "0\n",
            "A : <START> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING>\n",
            "Q : <PADDING>\n",
            "question: [[0 0 0 0 0 0 0 0 0 0]]\n",
            "answer: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "0\n",
            "A : <START> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING>\n",
            "Q : 뭐가 문제인가\n",
            "question: [[291   3   0   0   0   0   0   0   0   0]]\n",
            "answer: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "0\n",
            "A : <START> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING>\n",
            "Q : 양지인씨 안갔네?\n",
            "question: [[3 3 0 0 0 0 0 0 0 0]]\n",
            "answer: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "0\n",
            "A : <START> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING>\n",
            "Q : 아 스터디룸 갔구나 이사람들\n",
            "question: [[511   3   3   3   0   0   0   0   0   0]]\n",
            "answer: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "0\n",
            "A : <START> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-f5908b5269dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mchatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-f5908b5269dd>\u001b[0m in \u001b[0;36mchatting\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchatting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Q : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m  \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q5hUNXRRC-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}