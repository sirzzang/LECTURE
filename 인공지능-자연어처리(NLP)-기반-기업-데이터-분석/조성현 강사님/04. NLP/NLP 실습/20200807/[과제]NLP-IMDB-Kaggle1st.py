# -*- coding: utf-8 -*-
"""[과제]NLP-Popcorn-Kaggle1st-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ORUL6PUbW_tu-fqEammoLgA0HsNb7EUm
"""

# 모듈 불러 오기
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import re

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import word_tokenize

from collections import Counter
import math

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K
import matplotlib.pyplot as plt

from sklearn.metrics import roc_auc_score

# 경로 설정
root_path = "/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님"
data_path = f"{root_path}/dataset"

# 데이터 로드 및 원본 보존
df_raw = pd.read_csv(f"{data_path}/4-1.labeledTrainData.tsv",
                     header=0,
                     delimiter='\t',
                     quoting=3)
display(df_raw)

# 데이터 전처리
def clean_text(x):
    x = BeautifulSoup(x, 'lxml').get_text() # HTML 태그 제거
    x = re.sub("[^a-zA-Z]", " ", x) # 영어 제외 바꾸기
    x = x.lower() # 소문자
    x = [w for w in x.split() if not w in stopwords.words('english')] # 불용어 제거
    x = [PorterStemmer().stem(word) for word in x] # 포터 스테밍으로 어간 추출
    return " ".join(x)

df_raw['text'] = df_raw['review'].apply(lambda x: clean_text(x))
df = df_raw[['text', 'sentiment']]

# MI Score 기반 어휘집 생성
def calc_MI(pos_df, neg_df):

    pos_vocab = build_vocab(pos_df)
    neg_vocab = build_vocab(neg_df)
    
    merge_vocab = {}
    for k, v in pos_vocab:
        merge_vocab[k] = [v, 0]
    for k, v in neg_vocab:
        if k in merge_vocab:
            merge_vocab[k][1] = v
        else:
            merge_vocab[k] = [0, v]
    
    # MI 계산
    clip = 0.00001
    for k, v in merge_vocab.items():
        merge_vocab[k] = (v[0] / (len(pos_df)+len(neg_df))) * math.log(2*(v[0]+clip) / (v[0] + v[1])) + \
                        (v[1] / (len(pos_df)+len(neg_df))) * math.log(2*(v[1]+clip) / (v[0] + v[1]))
    
    return {k:v for k, v in sorted(merge_vocab.items(), key=lambda x:x[1], reverse=True)}

# 긍정 sentiment와 부정 sentiment로 나누기
df_pos = df[df['sentiment'] == 1]
df_neg = df[df['sentiment'] == 0]

# 어휘집 생성
vocabulary = calc_MI(df_pos, df_neg)

# 상위 50퍼센트만 남기기
num = int(len(vocabulary) * 0.5)
word2idx = {k:i+1 for i, (k, v) in enumerate(vocabulary.items()) if i <= num}
idx2word = {v:k for k, v in word2idx.items()}

# 토크나이징
tokens = []

for idx, text in enumerate(df['text']):
    temp = []
    delete_indices = []

    for word in text.split():
        try:
            temp.append(word2idx[word])
        except KeyError: # OOV
            continue 
    
    if len(temp) > 0:
        tokens.append(temp)
    else:
        delete_indices.append(idx)

# 문장 패딩
MAX_LENGTH = 200
X_train = pad_sequences(tokens, maxlen=MAX_LENGTH, padding='post', truncating='post')
X_train = np.array(X_train)
print(f"패딩 후 train data: {X_train.shape}")

# 문장 라벨
label = df['sentiment']
y_train = np.array(y_train)

# TF-IDF 벡터 생성 위해 원래 문장으로 바꾸기
sequences = []
for vector in train_features:
    temp = []
    for x in vector:
        try:
            temp.append(idx2word[x])
        except KeyError:
            pass

    sequences.append(temp)

# TF-IDF 벡터 생성
corpus = [" ".join(sequence) for sequence in sequences ]
vectorizer = TfidfVectorizer().fit(corpus)
tfidf_vec = vectorizer.transform(corpus).toarray()

# Doc2Vec 벡터 생성
doc2vec_features = 400
model_path = f"{data_path}/IMDB_{doc2vec_features}features.doc2vec" # 사용할/로드할 모델 경로 설정

try:
    doc_model = Doc2Vec.load(model_path)
except: # 저장된 모델 없는 경우
    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sequences)]
    doc_model = Doc2Vec(vector_size=doc2vec_features,
                        alpha=0.005,
                        min_alpha=0.0001,
                        min_count=1,
                        workers=4,
                        dm=1)
    doc_model.build_vocab(documents)
    doc_model.train(documents, total_examples=doc_model.corpus_count, epochs=10)
    doc_model.save(model_path)

doc2vec_vec = [doc_model.docvecs[i] for i in range(len(sequences))]
doc2vec_vec = np.array(doc2vec_vec)

# 데이터 분리
X_train_tf, X_test_tf, X_train_doc, X_test_doc, y_train, y_test = train_test_split(tfidf_vec, doc2vec_vec, train_labels,
                                                                                   test_size=0.2,
                                                                                   random_state=42)

# FFN 네트워크 설정
X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력
X_dense_1 = Dense(64, activation='linear')(X_input_1) # 선형 projection
X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력
X_dense_2 = Dense(64, activation='linear')(X_input_2)
X_concat = Concatenate()([X_dense_1, X_dense_2])
y_output = Dense(1, activation='sigmoid')(X_concat)

# 모델 구성
K.clear_session()
model = Model([X_input_1, X_input_2], y_output)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))
print("====== 전체 모델 구조 확인 ======")
print(model.summary())

# 모델 학습
es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
hist = model.fit([X_train_tf, X_train_doc], y_train,
                 epochs=300,
                 batch_size=300,
                 callbacks=[es],
                 validation_data=([X_test_tf, X_test_doc], y_test))

# loss 시각화
plt.plot(hist.history['loss'], label='Train loss')
plt.plot(hist.history['val_loss'], label = 'Test loss')
plt.legend()
plt.title("Loss history")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

# 예측 및 결과 확인
y_pred = model.predict([X_test_tf, X_test_doc])
y_pred = np.where(y_pred > 0.5, 1, 0)
accuracy = (y_test.reshape(-1, 1) == y_pred).mean()
rocauc = roc_auc_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy}")
print(f"Roc-Auc score: {rocauc}")