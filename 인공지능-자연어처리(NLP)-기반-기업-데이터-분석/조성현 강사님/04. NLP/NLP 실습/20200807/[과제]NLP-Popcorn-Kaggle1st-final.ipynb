{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[과제]NLP-Popcorn-Kaggle1st-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ-YISh48xqg",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle 1등 수상자 코드 아이디어\n",
        "\n",
        "- 정확도가 50% 나오는데, 뭔가 이상하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFlqgi08tKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "242841f2-e025-4ce8-d9b1-08b0d87af149"
      },
      "source": [
        "# 모듈 불러 오기\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9jJaK1u9CqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 경로 설정\n",
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/dataset\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nat_F0a69XBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "baa62023-f68b-4445-ed93-30bbc69d176a"
      },
      "source": [
        "# 데이터 로드 및 원본 보존\n",
        "df_raw = pd.read_csv(f\"{data_path}/4-1.labeledTrainData.tsv\",\n",
        "                     header=0,\n",
        "                     delimiter='\\t',\n",
        "                     quoting=3)\n",
        "display(df_raw)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>\"3453_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It seems like more consideration has gone int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>\"5064_1\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"I don't believe they made this film. Complete...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>\"10905_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"Guy is a loser. Can't get girls, needs to bui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>\"10194_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"This 30 minute documentary Buñuel made in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>\"8478_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"I saw this movie as a child and it broke my h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  sentiment                                             review\n",
              "0       \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1       \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2       \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3       \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4       \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
              "...          ...        ...                                                ...\n",
              "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
              "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
              "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
              "24998  \"10194_3\"          0  \"This 30 minute documentary Buñuel made in the...\n",
              "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h...\n",
              "\n",
              "[25000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CD2LR7pEOoO",
        "colab_type": "text"
      },
      "source": [
        "## _1_. 전처리\n",
        "\n",
        "- 텍스트 기본 전처리\n",
        "    - HTML 태그 제거\n",
        "    - 영어 제외 변환\n",
        "    - 소문자화\n",
        "    - 불용어 제거\n",
        "    - 포터 스테밍 어간 추출\n",
        "- MI Score 상위 50%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFmvj6fYEQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(x):\n",
        "    x = BeautifulSoup(x, 'lxml').get_text() # HTML 태그 제거\n",
        "    x = re.sub(\"[^a-zA-Z]\", \" \", x) # 영어 제외 바꾸기\n",
        "    x = x.lower() # 소문자\n",
        "    x = [w for w in x.split() if not w in stopwords.words('english')] # 불용어 제거\n",
        "    x = [PorterStemmer().stem(word) for word in x] # 포터 스테밍으로 어간 추출\n",
        "    return \" \".join(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuEbaOZPFUDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "022db941-8ee1-43f9-9eaf-8c7762da9064"
      },
      "source": [
        "%%time\n",
        "df_raw['text'] = df_raw['review'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 36s, sys: 53.2 s, total: 9min 29s\n",
            "Wall time: 9min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKV2a50hJvLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "532b8cc0-2611-4184-dfb0-e9714e944520"
      },
      "source": [
        "df = df_raw[['text', 'sentiment']]\n",
        "display(df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stuff go moment mj start listen music watch od...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>classic war world timothi hine entertain film ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film start manag nichola bell give welcom inve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>must assum prais film greatest film opera ever...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>superbl trashi wondrous unpretenti exploit hoo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>seem like consider gone imdb review film went ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>believ made film complet unnecessari first fil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>guy loser get girl need build pick stronger su...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>minut documentari bu uel made earli one spain ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>saw movi child broke heart stori unfinish end ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  sentiment\n",
              "0      stuff go moment mj start listen music watch od...          1\n",
              "1      classic war world timothi hine entertain film ...          1\n",
              "2      film start manag nichola bell give welcom inve...          0\n",
              "3      must assum prais film greatest film opera ever...          0\n",
              "4      superbl trashi wondrous unpretenti exploit hoo...          1\n",
              "...                                                  ...        ...\n",
              "24995  seem like consider gone imdb review film went ...          0\n",
              "24996  believ made film complet unnecessari first fil...          0\n",
              "24997  guy loser get girl need build pick stronger su...          0\n",
              "24998  minut documentari bu uel made earli one spain ...          0\n",
              "24999  saw movi child broke heart stori unfinish end ...          1\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIJMyRbsOSnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 긍정 sentiment와 부정 sentiment로 나누기\n",
        "df_pos = df[df['sentiment'] == 1]\n",
        "df_neg = df[df['sentiment'] == 0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqh9QndvTicz",
        "colab_type": "text"
      },
      "source": [
        "### 어휘집 생성 : MI Score 계산\n",
        "\n",
        "- `counter` 방식 맞는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HRgLgG7bHOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(df):\n",
        "    vocabulary = Counter()\n",
        "\n",
        "    for text in df['text']:\n",
        "        for word in list(set(word_tokenize(text))): # set으로 중복 단어 제거\n",
        "            vocabulary[word] += 1\n",
        "    \n",
        "    return vocabulary.most_common()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u180Yt0CYjbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_MI(pos_df, neg_df):\n",
        "\n",
        "    pos_vocab = build_vocab(pos_df)\n",
        "    neg_vocab = build_vocab(neg_df)\n",
        "    \n",
        "    merge_vocab = {}\n",
        "    for k, v in pos_vocab:\n",
        "        merge_vocab[k] = [v, 0]\n",
        "    for k, v in neg_vocab:\n",
        "        if k in merge_vocab:\n",
        "            merge_vocab[k][1] = v\n",
        "        else:\n",
        "            merge_vocab[k] = [0, v]\n",
        "    \n",
        "    # MI 계산\n",
        "    clip = 0.00001\n",
        "    for k, v in merge_vocab.items():\n",
        "        merge_vocab[k] = (v[0] / (len(pos_df)+len(neg_df))) * math.log(2*(v[0]+clip) / (v[0] + v[1])) + \\\n",
        "                        (v[1] / (len(pos_df)+len(neg_df))) * math.log(2*(v[1]+clip) / (v[0] + v[1]))\n",
        "    \n",
        "    return {k:v for k, v in sorted(merge_vocab.items(), key=lambda x:x[1], reverse=True)}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yXqpc0Kgujc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ebdcd2b2-1421-499c-e211-ee0bf37675ee"
      },
      "source": [
        "# 어휘집 생성\n",
        "%%time\n",
        "vocabulary = calc_MI(df_pos, df_neg)\n",
        "print(f\"총 어휘집 크기: {len(vocabulary)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "총 어휘집 크기: 50593\n",
            "CPU times: user 13.2 s, sys: 20.2 ms, total: 13.2 s\n",
            "Wall time: 13.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijLMTAJCkdpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c726fe3-b643-4056-aec1-53105ed1fa2f"
      },
      "source": [
        "# 가장 흔한 단어 20개 확인\n",
        "print(\"상위 20개 단어 확인\")\n",
        "print(list(vocabulary.keys())[:20])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "상위 20개 단어 확인\n",
            "['worst', 'bad', 'wast', 'aw', 'excel', 'great', 'stupid', 'bore', 'terribl', 'wors', 'horribl', 'beauti', 'love', 'perfect', 'poor', 'crap', 'noth', 'poorli', 'suppos', 'lame']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2nmtm7okDBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 상위 50퍼센트만 남기기 + OOV 버리기\n",
        "num = int(len(vocabulary) * 0.5)\n",
        "word2idx = {k:i+1 for i, (k, v) in enumerate(vocabulary.items()) if i <= num}\n",
        "idx2word = {v:k for k, v in word2idx.items()}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg5c6z9htqd7",
        "colab_type": "text"
      },
      "source": [
        "### 시퀀스 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lpYHhcFnONM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f22f6324-109b-4146-beb3-387be44b1b2d"
      },
      "source": [
        "# 토크나이징\n",
        "print(f\"기존 문장 개수: {len(df['text'])}\")\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for idx, text in enumerate(df['text']):\n",
        "    temp = []\n",
        "    delete_indices = []\n",
        "\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            temp.append(word2idx[word])\n",
        "        except KeyError: # OOV\n",
        "            continue \n",
        "    \n",
        "    if len(temp) > 0:\n",
        "        tokens.append(temp)\n",
        "    else:\n",
        "        delete_indices.append(idx)\n",
        "\n",
        "print(f\"OOV 없는 문장으로만 남긴 개수: {len(tokens)}\")\n",
        "print(f\"삭제해야 할 문장 인덱스: {delete_indices}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "기존 문장 개수: 25000\n",
            "OOV 없는 문장으로만 남긴 개수: 25000\n",
            "삭제해야 할 문장 인덱스: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gfIokvstuaJ",
        "colab_type": "text"
      },
      "source": [
        "### 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t6_lxz-sUHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "6658233a-d613-43e1-e317-3656a2e4b179"
      },
      "source": [
        "# 문장 길이 체크\n",
        "def check_len(threshold, sentences):\n",
        "    cnt = 0\n",
        "    for sent in sentences:\n",
        "        if len(sent) <= threshold:\n",
        "            cnt += 1\n",
        "    \n",
        "    return f'전체 문장 중 길이가 {threshold} 이하인 샘플의 비율: {(cnt/len(sentences))*100}'\n",
        "\n",
        "for i in range(10, 300, 10):\n",
        "    print(check_len(i, tokens))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 문장 중 길이가 10 이하인 샘플의 비율: 0.132\n",
            "전체 문장 중 길이가 20 이하인 샘플의 비율: 2.084\n",
            "전체 문장 중 길이가 30 이하인 샘플의 비율: 6.672000000000001\n",
            "전체 문장 중 길이가 40 이하인 샘플의 비율: 11.856\n",
            "전체 문장 중 길이가 50 이하인 샘플의 비율: 22.567999999999998\n",
            "전체 문장 중 길이가 60 이하인 샘플의 비율: 37.303999999999995\n",
            "전체 문장 중 길이가 70 이하인 샘플의 비율: 48.46\n",
            "전체 문장 중 길이가 80 이하인 샘플의 비율: 56.912\n",
            "전체 문장 중 길이가 90 이하인 샘플의 비율: 63.188\n",
            "전체 문장 중 길이가 100 이하인 샘플의 비율: 68.328\n",
            "전체 문장 중 길이가 110 이하인 샘플의 비율: 72.432\n",
            "전체 문장 중 길이가 120 이하인 샘플의 비율: 75.976\n",
            "전체 문장 중 길이가 130 이하인 샘플의 비율: 78.996\n",
            "전체 문장 중 길이가 140 이하인 샘플의 비율: 81.636\n",
            "전체 문장 중 길이가 150 이하인 샘플의 비율: 83.804\n",
            "전체 문장 중 길이가 160 이하인 샘플의 비율: 85.804\n",
            "전체 문장 중 길이가 170 이하인 샘플의 비율: 87.508\n",
            "전체 문장 중 길이가 180 이하인 샘플의 비율: 88.872\n",
            "전체 문장 중 길이가 190 이하인 샘플의 비율: 90.13600000000001\n",
            "전체 문장 중 길이가 200 이하인 샘플의 비율: 91.316\n",
            "전체 문장 중 길이가 210 이하인 샘플의 비율: 92.32000000000001\n",
            "전체 문장 중 길이가 220 이하인 샘플의 비율: 93.244\n",
            "전체 문장 중 길이가 230 이하인 샘플의 비율: 93.988\n",
            "전체 문장 중 길이가 240 이하인 샘플의 비율: 94.692\n",
            "전체 문장 중 길이가 250 이하인 샘플의 비율: 95.204\n",
            "전체 문장 중 길이가 260 이하인 샘플의 비율: 95.776\n",
            "전체 문장 중 길이가 270 이하인 샘플의 비율: 96.22\n",
            "전체 문장 중 길이가 280 이하인 샘플의 비율: 96.616\n",
            "전체 문장 중 길이가 290 이하인 샘플의 비율: 96.956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJhAiYGYsqAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58cd8377-4512-482c-abce-5afd85d97fc8"
      },
      "source": [
        "# 문장 패딩\n",
        "# MAX_LENGTH = int(input('문장 최대 길이 설정: '))\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "X_train = pad_sequences(tokens, \n",
        "                        maxlen=MAX_LENGTH,\n",
        "                        padding='post',\n",
        "                        truncating='post')\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "print(f\"패딩 후 train data: {X_train.shape}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "패딩 후 train data: (25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHsa3iO3t9Br",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "0689f423-6746-460d-9fd3-8cc72ae00ed7"
      },
      "source": [
        "# 문장 확인\n",
        "print(X_train[0])\n",
        "print(f\"샘플 문장 길이: {len(X_train[0])}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1848   405  1232  1862   600   358   730  9191   623   730   730  1941\n",
            "   291   277   743  1230  3309   102 15403   715   291   501 15947   335\n",
            "  1941  8890  8587  8890  4602  9003   439   405  1528   636  1541   556\n",
            "   132  1994  1862  1349  6389    60   332  1994  6289     2 15371   573\n",
            "   531  1045  1908  8808    50   157   472  1862  1178   405   677   527\n",
            "     8   415   389  1862 13527   501   202  1862  1991   213   233   610\n",
            "  1991    99   715   676   228  4602  9003   526  1986   600    24   748\n",
            "  2378 15220  1916 15296   160  6289  3683   277  1862   750     2  1837\n",
            "  1862  4905  1916 15296  5067   277  3846  8726  6289  9160  3275   291\n",
            "   677  1862   358  5823   197   472  1862  6183  2693  8898   363 15251\n",
            " 13643 15220    60  1784  3230 14260  8592  4911  9003     2 15220 14021\n",
            "  1784   677   622  2304   387   363   103    27   164  1402  1616   327\n",
            "   681   202  3846   472  1862  3806  1422  3846  2799  3971    71  8034\n",
            "  9186  1994  5825  1862  9908   202  8424  1908  8808   522  8744  3846\n",
            "  1018    72  2365  8743  4832    72  8726  3846   126  9134  4489  8726\n",
            "  5861   162 15804   676     7   102  8305  2320 13729     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "샘플 문장 길이: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9cJAv94uReP",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il8u1opZuPmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 라벨 가져오기\n",
        "label = df['sentiment']\n",
        "label = np.array(y_train)\n",
        "print(f\"train label: {label.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU-CCt7zug5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전처리 완료된 데이터 저장\n",
        "input_out_path = f\"{data_path}/popcorn_input_mydata.npy\"\n",
        "label_out_path = f\"{data_path}/popcorn_label_mydata.npy\"\n",
        "\n",
        "np.save(open(input_out_path, 'wb'), X_train)\n",
        "np.save(open(label_out_path, 'wb'), y_train)\n",
        "df.to_csv(f\"{data_path}/popcorn_clean_mydata.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izxGDESagEhS",
        "colab_type": "text"
      },
      "source": [
        "## _2_. 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giwrbJs3gDe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 로드\n",
        "train_features = np.load(open(f\"{data_path}/popcorn_input_mydata.npy\", 'rb'))\n",
        "train_labels = np.load(open(f\"{data_path}/popcorn_label_mydata.npy\", 'rb'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcU1qh6c92u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 원래 문장으로 바꾸기\n",
        "sequences = []\n",
        "for vector in train_features:\n",
        "    temp = []\n",
        "    for x in vector:\n",
        "        try:\n",
        "            temp.append(idx2word[x])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    sequences.append(temp)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqjsjYgTw8lk",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBQs7GlO20bV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6c3b7cf5-7cfa-4783-fb1c-815a92cf9414"
      },
      "source": [
        "corpus = [\" \".join(sequence) for sequence in sequences ]\n",
        "print(corpus[:3])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['stuff go moment mj start music watch odd documentari watch watch moonwalk mayb want get certain insight guy thought realli mayb make mind innoc moonwalk part biographi part featur film rememb go see cinema origin releas subtl messag mj feel toward also obviou messag drug bad kay visual impress cours michael jackson unless remot like mj anyway go hate find bore may call mj consent make movi mj fan would say made fan true realli nice actual featur film bit final start minut smooth crimin sequenc joe pesci power drug lord want mj dead bad beyond mj nah joe pesci rant want peopl know drug etc dunno mayb hate mj music lot thing like mj turn car robot whole speed demon sequenc also director must patienc saint came film bad sequenc usual director hate work kid let whole bunch perform complex danc scene bottom line movi peopl like mj anoth think peopl stay away tri give wholesom messag iron mj bestest movi girl michael jackson truli talent peopl ever well attent gave hmmm well know peopl differ close door know fact either extrem nice stupid guy sickest hope latter', 'classic war world timothi entertain film obvious great effort length faith recreat h well classic book mr succe watch film appreci fact standard predict hollywood year spielberg version tom cruis slightest resembl book obvious everyon look differ thing movi envis amateur critic look critic other rate movi import base like entertain peopl critic enjoy effort mr put faith h well classic novel entertain made easi overlook critic perceiv', 'film start manag give welcom investor robert carradin park secret project mutat anim fossil dna like haiduk park scientist resurrect natur sabretooth tiger smilodon ambit turn creatur escap begin stalk human meanwhil enter restrict area secur center attack pack larg pre histor anim addit secur wimmer brian jurassik hardli smilodon sabretooth cours real star star astound though giant anim stalk group run afoul natur furthermor sabretooth danger slow stalk victim movi deliv lot blood gore behead hair rais chill full scare sabretooth appear mediocr special effect stori provid excit stir entertain result quit bore giant anim major made comput gener seem total lousi middl perform though player appropri becom actor give vigor physic perform beast run bound dangl pack ridicul final scene small kid realist gori violent attack scene film sabretooth smilodon follow sabretooth jame vanessa angel david john davi much better bc cliff curti motion pictur bloodi moment badli georg miller origin take mani element film miller australian director usual work televis wave journey center earth mani other occasion cinema man snowi river roxann robinson rate averag bottom barrel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiGzuZwz8ftL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d86cf720-c07d-449f-f33b-1dc83298b953"
      },
      "source": [
        "vectorizer = TfidfVectorizer().fit(corpus)\n",
        "tfidf_vec = vectorizer.transform(corpus).toarray()\n",
        "print(f\"TF-IDF 행렬: {tfidf_vec.shape}\")\n",
        "print(\"========== 어휘집 내 단어 샘플 20개 확인 ==========\")\n",
        "print(list(vectorizer.vocabulary_)[:20])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF 행렬: (25000, 24530)\n",
            "========== 어휘집 내 단어 샘플 20개 확인 ==========\n",
            "['stuff', 'go', 'moment', 'mj', 'start', 'music', 'watch', 'odd', 'documentari', 'moonwalk', 'mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'make', 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAC4NQ4R9izz",
        "colab_type": "text"
      },
      "source": [
        "### Doc2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFrj5WA8hZcZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fc75c997-bf67-4072-bc2c-950e0e894f96"
      },
      "source": [
        "# Doc2Vec 파라미터 설정\n",
        "# doc2vec_features = int(input('Doc2Vec 임베딩 차원 설정: '))\n",
        "doc2vec_features = 400\n",
        "\n",
        "# 사용할/로드할 모델 경로 설정\n",
        "model_path = f\"{data_path}/IMDB_{doc2vec_features}features.doc2vec\"\n",
        "\n",
        "try:\n",
        "    doc_model = Doc2Vec.load(model_path)\n",
        "except: # 저장된 모델 없는 경우\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sequences)]\n",
        "    doc_model = Doc2Vec(vector_size=doc2vec_features,\n",
        "                        alpha=0.005,\n",
        "                        min_alpha=0.0001,\n",
        "                        min_count=1,\n",
        "                        workers=4,\n",
        "                        dm=1)\n",
        "    doc_model.build_vocab(documents)\n",
        "    doc_model.train(documents, total_examples=doc_model.corpus_count, epochs=10)\n",
        "    doc_model.save(model_path)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM10kL8O95J0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "83aa4dfa-d887-4ddd-dc1d-728de432539c"
      },
      "source": [
        "# 모델 확인\n",
        "keys = list(doc_model.wv.vocab.keys())\n",
        "print(f\"단어 개수: {len(keys)}\")\n",
        "print(\"========= 샘플 확인 =========\")\n",
        "print(keys[:20])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 개수: 24539\n",
            "========= 샘플 확인 =========\n",
            "['stuff', 'go', 'moment', 'mj', 'start', 'music', 'watch', 'odd', 'documentari', 'moonwalk', 'mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'make', 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbqyT-Bz-JI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5e7c0c4-8401-4c20-93ea-08c148865ccc"
      },
      "source": [
        "# Doc2Vec 벡터 생성\n",
        "doc2vec_vec = [doc_model.docvecs[i] for i in range(len(sequences))]\n",
        "doc2vec_vec = np.array(doc2vec_vec)\n",
        "print(doc2vec_vec.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loclU_X1C8ny",
        "colab_type": "text"
      },
      "source": [
        "### 결합 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-XcMOctlPXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a7e51109-eaf5-4fae-9723-0d3918bac677"
      },
      "source": [
        "X_train_tf, X_test_tf, X_train_doc, X_test_doc, y_train, y_test = train_test_split(tfidf_vec, doc2vec_vec, train_labels,\n",
        "                                                                                   test_size=0.2,\n",
        "                                                                                   random_state=42)\n",
        "print(f\"TFIDF: train {X_train_tf.shape}, test {X_test_tf.shape}\")\n",
        "print(f\"Doc2Vec: train {X_train_doc.shape}, test {X_test_doc.shape}\")\n",
        "print(f\"label: train {y_train.shape}, test {y_test.shape}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFIDF: train (20000, 24530), test (5000, 24530)\n",
            "Doc2Vec: train (20000, 400), test (5000, 400)\n",
            "label: train (20000,), test (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIWiJ8q0-UmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "12c92902-e143-4fc5-9159-19f0a81d6167"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(64, activation='linear')(X_input_1) # 선형 projection\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(64, activation='linear')(X_input_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           1569984     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           25664       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 128)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,595,777\n",
            "Trainable params: 1,595,777\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw1rHn_UkHTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "3c09e3a7-b22d-4d53-c33c-925d01d147d3"
      },
      "source": [
        "# 학습\n",
        "es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 callbacks=[es],\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 20ms/step - loss: 0.5610 - val_loss: 0.4392\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3555 - val_loss: 0.3190\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2552 - val_loss: 0.2678\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2012 - val_loss: 0.2473\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1667 - val_loss: 0.2394\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1412 - val_loss: 0.2375\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1218 - val_loss: 0.2400\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1058 - val_loss: 0.2457\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0927 - val_loss: 0.2521\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0815 - val_loss: 0.2605\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0723 - val_loss: 0.2701\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0640 - val_loss: 0.2798\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0571 - val_loss: 0.2900\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0510 - val_loss: 0.3025\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0456 - val_loss: 0.3134\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0406 - val_loss: 0.3247\n",
            "Epoch 00016: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNyRtFwpmpuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c4ae03cb-61b6-40f4-ab67-1aa79c4f1798"
      },
      "source": [
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAoUhAmZnF0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1a1e5703-9750-49ff-895a-c84d6ad76fc5"
      },
      "source": [
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.882\n",
            "Roc-Auc score: 0.8819901437507032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xexZaoWHw8S",
        "colab_type": "text"
      },
      "source": [
        "### 여러 모델링 시도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHjOWN8DWztd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eec4c899-aa0b-44c6-958c-6c99fdc38d58"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(64, activation='linear')(X_input_1) # 선형 projection\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(64, activation='linear')(X_input_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           1569984     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           25664       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 128)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,595,777\n",
            "Trainable params: 1,595,777\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 18ms/step - loss: 0.5638 - val_loss: 0.4438\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3593 - val_loss: 0.3202\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2569 - val_loss: 0.2686\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.2021 - val_loss: 0.2470\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1674 - val_loss: 0.2389\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.1419 - val_loss: 0.2377\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1222 - val_loss: 0.2392\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.1066 - val_loss: 0.2455\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0932 - val_loss: 0.2516\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0822 - val_loss: 0.2598\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0726 - val_loss: 0.2693\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0645 - val_loss: 0.2792\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0574 - val_loss: 0.2902\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0515 - val_loss: 0.3013\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0459 - val_loss: 0.3131\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0410 - val_loss: 0.3247\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0367 - val_loss: 0.3375\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0330 - val_loss: 0.3490\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0297 - val_loss: 0.3608\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0267 - val_loss: 0.3731\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0241 - val_loss: 0.3868\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0217 - val_loss: 0.3999\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0197 - val_loss: 0.4102\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0177 - val_loss: 0.4232\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0161 - val_loss: 0.4341\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0145 - val_loss: 0.4483\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0132 - val_loss: 0.4579\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0119 - val_loss: 0.4701\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0108 - val_loss: 0.4814\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0100 - val_loss: 0.4925\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0090 - val_loss: 0.5048\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0082 - val_loss: 0.5144\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0075 - val_loss: 0.5268\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0069 - val_loss: 0.5376\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0064 - val_loss: 0.5481\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0059 - val_loss: 0.5571\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0054 - val_loss: 0.5675\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0050 - val_loss: 0.5794\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0046 - val_loss: 0.5883\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0043 - val_loss: 0.5971\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0039 - val_loss: 0.6064\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0037 - val_loss: 0.6166\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0034 - val_loss: 0.6248\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0032 - val_loss: 0.6338\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0030 - val_loss: 0.6432\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0028 - val_loss: 0.6539\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6597\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0024 - val_loss: 0.6696\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0023 - val_loss: 0.6773\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0021 - val_loss: 0.6858\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0020 - val_loss: 0.6937\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.7019\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.7093\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.7195\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.7253\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.7346\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.7410\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7501\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7563\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7632\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7716\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7791\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7857\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.7110e-04 - val_loss: 0.7922\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1958e-04 - val_loss: 0.8005\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.7390e-04 - val_loss: 0.8073\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.3024e-04 - val_loss: 0.8147\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8809e-04 - val_loss: 0.8217\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5093e-04 - val_loss: 0.8278\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1299e-04 - val_loss: 0.8351\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7842e-04 - val_loss: 0.8420\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4805e-04 - val_loss: 0.8491\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1546e-04 - val_loss: 0.8560\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8867e-04 - val_loss: 0.8622\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5967e-04 - val_loss: 0.8689\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3312e-04 - val_loss: 0.8756\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0891e-04 - val_loss: 0.8822\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8637e-04 - val_loss: 0.8892\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6307e-04 - val_loss: 0.8951\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4234e-04 - val_loss: 0.9026\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2267e-04 - val_loss: 0.9083\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0357e-04 - val_loss: 0.9156\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8614e-04 - val_loss: 0.9222\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6922e-04 - val_loss: 0.9279\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5185e-04 - val_loss: 0.9345\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3715e-04 - val_loss: 0.9410\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2174e-04 - val_loss: 0.9471\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0860e-04 - val_loss: 0.9528\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9500e-04 - val_loss: 0.9592\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8280e-04 - val_loss: 0.9657\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7025e-04 - val_loss: 0.9720\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5847e-04 - val_loss: 0.9776\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4775e-04 - val_loss: 0.9848\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3724e-04 - val_loss: 0.9904\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2739e-04 - val_loss: 0.9969\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1790e-04 - val_loss: 1.0022\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0894e-04 - val_loss: 1.0086\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0032e-04 - val_loss: 1.0153\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9206e-04 - val_loss: 1.0217\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8407e-04 - val_loss: 1.0271\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7670e-04 - val_loss: 1.0334\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6970e-04 - val_loss: 1.0381\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6243e-04 - val_loss: 1.0445\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5571e-04 - val_loss: 1.0525\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4976e-04 - val_loss: 1.0564\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4384e-04 - val_loss: 1.0629\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3779e-04 - val_loss: 1.0682\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3214e-04 - val_loss: 1.0731\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2735e-04 - val_loss: 1.0812\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2220e-04 - val_loss: 1.0867\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1712e-04 - val_loss: 1.0929\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1258e-04 - val_loss: 1.0971\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0818e-04 - val_loss: 1.1036\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0394e-04 - val_loss: 1.1095\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9851e-05 - val_loss: 1.1151\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.6021e-05 - val_loss: 1.1224\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.2383e-05 - val_loss: 1.1257\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.8855e-05 - val_loss: 1.1324\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5345e-05 - val_loss: 1.1382\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.2149e-05 - val_loss: 1.1448\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8893e-05 - val_loss: 1.1503\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5862e-05 - val_loss: 1.1556\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3072e-05 - val_loss: 1.1620\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.0020e-05 - val_loss: 1.1670\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7503e-05 - val_loss: 1.1729\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4804e-05 - val_loss: 1.1783\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2427e-05 - val_loss: 1.1843\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.0171e-05 - val_loss: 1.1890\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7900e-05 - val_loss: 1.1957\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5681e-05 - val_loss: 1.2020\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3665e-05 - val_loss: 1.2064\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1603e-05 - val_loss: 1.2114\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9647e-05 - val_loss: 1.2181\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7849e-05 - val_loss: 1.2234\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6016e-05 - val_loss: 1.2284\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4361e-05 - val_loss: 1.2344\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2741e-05 - val_loss: 1.2397\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1286e-05 - val_loss: 1.2449\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9644e-05 - val_loss: 1.2503\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8160e-05 - val_loss: 1.2562\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6790e-05 - val_loss: 1.2612\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5548e-05 - val_loss: 1.2682\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4202e-05 - val_loss: 1.2734\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2959e-05 - val_loss: 1.2786\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1830e-05 - val_loss: 1.2828\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0616e-05 - val_loss: 1.2891\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9535e-05 - val_loss: 1.2949\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8431e-05 - val_loss: 1.2996\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7430e-05 - val_loss: 1.3061\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6497e-05 - val_loss: 1.3113\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5489e-05 - val_loss: 1.3164\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4625e-05 - val_loss: 1.3206\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3738e-05 - val_loss: 1.3278\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2896e-05 - val_loss: 1.3326\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2119e-05 - val_loss: 1.3369\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1308e-05 - val_loss: 1.3424\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0575e-05 - val_loss: 1.3488\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9838e-05 - val_loss: 1.3538\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9141e-05 - val_loss: 1.3592\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8469e-05 - val_loss: 1.3640\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7833e-05 - val_loss: 1.3689\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7223e-05 - val_loss: 1.3747\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6607e-05 - val_loss: 1.3800\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6061e-05 - val_loss: 1.3842\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5462e-05 - val_loss: 1.3912\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4945e-05 - val_loss: 1.3960\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4453e-05 - val_loss: 1.4009\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3940e-05 - val_loss: 1.4060\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3472e-05 - val_loss: 1.4114\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3013e-05 - val_loss: 1.4157\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2583e-05 - val_loss: 1.4224\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2191e-05 - val_loss: 1.4277\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1734e-05 - val_loss: 1.4317\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1327e-05 - val_loss: 1.4381\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0959e-05 - val_loss: 1.4425\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0618e-05 - val_loss: 1.4467\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0235e-05 - val_loss: 1.4519\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9096e-06 - val_loss: 1.4570\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.5822e-06 - val_loss: 1.4637\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.2345e-06 - val_loss: 1.4685\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.9407e-06 - val_loss: 1.4732\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6677e-06 - val_loss: 1.4768\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.3639e-06 - val_loss: 1.4840\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.0809e-06 - val_loss: 1.4881\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8215e-06 - val_loss: 1.4930\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5598e-06 - val_loss: 1.4978\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3100e-06 - val_loss: 1.5034\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.0768e-06 - val_loss: 1.5085\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.8628e-06 - val_loss: 1.5125\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.6208e-06 - val_loss: 1.5189\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4040e-06 - val_loss: 1.5225\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2201e-06 - val_loss: 1.5288\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.0067e-06 - val_loss: 1.5335\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8102e-06 - val_loss: 1.5369\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.6269e-06 - val_loss: 1.5429\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4537e-06 - val_loss: 1.5480\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.2566e-06 - val_loss: 1.5535\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0866e-06 - val_loss: 1.5592\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9530e-06 - val_loss: 1.5632\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7724e-06 - val_loss: 1.5672\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6361e-06 - val_loss: 1.5731\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4772e-06 - val_loss: 1.5782\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3438e-06 - val_loss: 1.5826\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1936e-06 - val_loss: 1.5868\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0666e-06 - val_loss: 1.5915\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9325e-06 - val_loss: 1.5973\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8178e-06 - val_loss: 1.6021\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6952e-06 - val_loss: 1.6070\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5843e-06 - val_loss: 1.6110\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4652e-06 - val_loss: 1.6160\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3650e-06 - val_loss: 1.6203\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2579e-06 - val_loss: 1.6254\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1527e-06 - val_loss: 1.6309\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0615e-06 - val_loss: 1.6352\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 2.9713e-06 - val_loss: 1.6399\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8740e-06 - val_loss: 1.6454\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7811e-06 - val_loss: 1.6490\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6992e-06 - val_loss: 1.6539\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6165e-06 - val_loss: 1.6589\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5321e-06 - val_loss: 1.6647\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4561e-06 - val_loss: 1.6672\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3797e-06 - val_loss: 1.6739\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 2.3094e-06 - val_loss: 1.6772\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2409e-06 - val_loss: 1.6825\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1676e-06 - val_loss: 1.6875\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1029e-06 - val_loss: 1.6919\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0416e-06 - val_loss: 1.6960\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9770e-06 - val_loss: 1.7023\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9225e-06 - val_loss: 1.7058\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8611e-06 - val_loss: 1.7107\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8048e-06 - val_loss: 1.7153\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7456e-06 - val_loss: 1.7211\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6995e-06 - val_loss: 1.7241\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6475e-06 - val_loss: 1.7281\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5949e-06 - val_loss: 1.7343\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5490e-06 - val_loss: 1.7390\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5006e-06 - val_loss: 1.7436\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4571e-06 - val_loss: 1.7471\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4141e-06 - val_loss: 1.7533\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3722e-06 - val_loss: 1.7562\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3308e-06 - val_loss: 1.7599\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2924e-06 - val_loss: 1.7655\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2542e-06 - val_loss: 1.7706\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2169e-06 - val_loss: 1.7754\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1797e-06 - val_loss: 1.7796\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1455e-06 - val_loss: 1.7841\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1080e-06 - val_loss: 1.7867\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0785e-06 - val_loss: 1.7926\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0476e-06 - val_loss: 1.7978\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0159e-06 - val_loss: 1.8020\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.8464e-07 - val_loss: 1.8055\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.5559e-07 - val_loss: 1.8116\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.2806e-07 - val_loss: 1.8155\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.0092e-07 - val_loss: 1.8217\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.7783e-07 - val_loss: 1.8258\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.4942e-07 - val_loss: 1.8301\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 8.2458e-07 - val_loss: 1.8341\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.0062e-07 - val_loss: 1.8381\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7806e-07 - val_loss: 1.8419\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5464e-07 - val_loss: 1.8477\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3288e-07 - val_loss: 1.8513\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1269e-07 - val_loss: 1.8555\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9159e-07 - val_loss: 1.8607\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7089e-07 - val_loss: 1.8641\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.5182e-07 - val_loss: 1.8693\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3359e-07 - val_loss: 1.8733\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1457e-07 - val_loss: 1.8774\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9771e-07 - val_loss: 1.8818\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8013e-07 - val_loss: 1.8873\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.6385e-07 - val_loss: 1.8906\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4703e-07 - val_loss: 1.8948\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3155e-07 - val_loss: 1.9009\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1738e-07 - val_loss: 1.9037\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0249e-07 - val_loss: 1.9088\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8816e-07 - val_loss: 1.9115\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7470e-07 - val_loss: 1.9165\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6052e-07 - val_loss: 1.9213\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4857e-07 - val_loss: 1.9248\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3706e-07 - val_loss: 1.9298\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2334e-07 - val_loss: 1.9347\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1191e-07 - val_loss: 1.9389\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0063e-07 - val_loss: 1.9427\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8853e-07 - val_loss: 1.9482\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7840e-07 - val_loss: 1.9517\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6771e-07 - val_loss: 1.9546\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5775e-07 - val_loss: 1.9591\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4761e-07 - val_loss: 1.9649\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3862e-07 - val_loss: 1.9684\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2890e-07 - val_loss: 1.9730\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1924e-07 - val_loss: 1.9782\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 3.1158e-07 - val_loss: 1.9800\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0262e-07 - val_loss: 1.9856\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9494e-07 - val_loss: 1.9892\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8690e-07 - val_loss: 1.9932\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7911e-07 - val_loss: 1.9979\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7156e-07 - val_loss: 2.0012\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6428e-07 - val_loss: 2.0061\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5718e-07 - val_loss: 2.0104\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5076e-07 - val_loss: 2.0122\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4445e-07 - val_loss: 2.0170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8554\n",
            "Roc-Auc score: 0.8554934933041732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCgKfsvmX66R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2687bc23-c5ed-46fe-c38c-156ba534295f"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(64, activation='relu')(X_input_1) # 선형 projection\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(64, activation='relu')(X_input_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           1569984     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           25664       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 128)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,595,777\n",
            "Trainable params: 1,595,777\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 0.5811 - val_loss: 0.4575\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3681 - val_loss: 0.3260\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2602 - val_loss: 0.2710\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2040 - val_loss: 0.2485\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1689 - val_loss: 0.2395\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1435 - val_loss: 0.2371\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1241 - val_loss: 0.2394\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1084 - val_loss: 0.2441\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0952 - val_loss: 0.2496\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0840 - val_loss: 0.2574\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0747 - val_loss: 0.2654\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0665 - val_loss: 0.2744\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0593 - val_loss: 0.2843\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0531 - val_loss: 0.2946\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0475 - val_loss: 0.3046\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0425 - val_loss: 0.3151\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0382 - val_loss: 0.3261\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0343 - val_loss: 0.3370\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0310 - val_loss: 0.3474\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.3597\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0254 - val_loss: 0.3712\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0228 - val_loss: 0.3806\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0206 - val_loss: 0.3913\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0186 - val_loss: 0.4016\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0169 - val_loss: 0.4122\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0153 - val_loss: 0.4224\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0139 - val_loss: 0.4330\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0127 - val_loss: 0.4428\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0116 - val_loss: 0.4528\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0105 - val_loss: 0.4623\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0096 - val_loss: 0.4722\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0088 - val_loss: 0.4816\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0081 - val_loss: 0.4916\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0074 - val_loss: 0.5009\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0069 - val_loss: 0.5100\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0063 - val_loss: 0.5186\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0058 - val_loss: 0.5269\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0054 - val_loss: 0.5358\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0050 - val_loss: 0.5453\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0047 - val_loss: 0.5530\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0043 - val_loss: 0.5616\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0040 - val_loss: 0.5693\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0038 - val_loss: 0.5771\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0035 - val_loss: 0.5854\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0033 - val_loss: 0.5926\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0031 - val_loss: 0.6002\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0029 - val_loss: 0.6082\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0028 - val_loss: 0.6162\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6229\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0024 - val_loss: 0.6306\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0023 - val_loss: 0.6377\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6444\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0020 - val_loss: 0.6514\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.6582\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.6650\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.6717\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6788\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6853\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6913\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.6985\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7046\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7108\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7175\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7238\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7295\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7358\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.8863e-04 - val_loss: 0.7421\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.4113e-04 - val_loss: 0.7480\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.9795e-04 - val_loss: 0.7538\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5831e-04 - val_loss: 0.7597\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.1637e-04 - val_loss: 0.7654\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8179e-04 - val_loss: 0.7714\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.4364e-04 - val_loss: 0.7774\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1170e-04 - val_loss: 0.7830\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7804e-04 - val_loss: 0.7889\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4834e-04 - val_loss: 0.7943\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2022e-04 - val_loss: 0.8003\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9263e-04 - val_loss: 0.8058\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.6768e-04 - val_loss: 0.8117\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4135e-04 - val_loss: 0.8172\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1861e-04 - val_loss: 0.8226\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9604e-04 - val_loss: 0.8282\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7460e-04 - val_loss: 0.8340\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.5396e-04 - val_loss: 0.8396\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3537e-04 - val_loss: 0.8449\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1569e-04 - val_loss: 0.8500\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9870e-04 - val_loss: 0.8558\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8143e-04 - val_loss: 0.8618\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6601e-04 - val_loss: 0.8662\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4994e-04 - val_loss: 0.8718\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3631e-04 - val_loss: 0.8772\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2164e-04 - val_loss: 0.8824\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0778e-04 - val_loss: 0.8883\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9499e-04 - val_loss: 0.8937\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8226e-04 - val_loss: 0.8988\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7108e-04 - val_loss: 0.9039\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5944e-04 - val_loss: 0.9095\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4860e-04 - val_loss: 0.9147\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3840e-04 - val_loss: 0.9196\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2834e-04 - val_loss: 0.9250\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1853e-04 - val_loss: 0.9306\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1004e-04 - val_loss: 0.9355\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0124e-04 - val_loss: 0.9411\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9288e-04 - val_loss: 0.9464\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8489e-04 - val_loss: 0.9514\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7726e-04 - val_loss: 0.9567\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7024e-04 - val_loss: 0.9618\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6356e-04 - val_loss: 0.9671\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5703e-04 - val_loss: 0.9723\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5075e-04 - val_loss: 0.9773\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4430e-04 - val_loss: 0.9821\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3842e-04 - val_loss: 0.9874\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3286e-04 - val_loss: 0.9929\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2747e-04 - val_loss: 0.9980\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2244e-04 - val_loss: 1.0031\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1755e-04 - val_loss: 1.0085\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.1273e-04 - val_loss: 1.0133\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0852e-04 - val_loss: 1.0185\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0404e-04 - val_loss: 1.0235\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9897e-05 - val_loss: 1.0285\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.6033e-05 - val_loss: 1.0337\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.2279e-05 - val_loss: 1.0383\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.8724e-05 - val_loss: 1.0439\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5266e-05 - val_loss: 1.0487\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.1853e-05 - val_loss: 1.0541\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8841e-05 - val_loss: 1.0586\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5663e-05 - val_loss: 1.0639\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.2718e-05 - val_loss: 1.0687\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9951e-05 - val_loss: 1.0739\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7304e-05 - val_loss: 1.0787\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4677e-05 - val_loss: 1.0841\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2318e-05 - val_loss: 1.0888\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9828e-05 - val_loss: 1.0937\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7608e-05 - val_loss: 1.0987\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5359e-05 - val_loss: 1.1034\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3456e-05 - val_loss: 1.1085\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1415e-05 - val_loss: 1.1132\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9451e-05 - val_loss: 1.1183\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7609e-05 - val_loss: 1.1231\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.5790e-05 - val_loss: 1.1280\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4068e-05 - val_loss: 1.1330\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2423e-05 - val_loss: 1.1381\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0959e-05 - val_loss: 1.1425\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9412e-05 - val_loss: 1.1475\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7896e-05 - val_loss: 1.1523\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6497e-05 - val_loss: 1.1574\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5260e-05 - val_loss: 1.1619\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3853e-05 - val_loss: 1.1670\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2682e-05 - val_loss: 1.1718\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1479e-05 - val_loss: 1.1766\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0316e-05 - val_loss: 1.1813\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9219e-05 - val_loss: 1.1860\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8196e-05 - val_loss: 1.1909\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7183e-05 - val_loss: 1.1956\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6189e-05 - val_loss: 1.2006\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5276e-05 - val_loss: 1.2053\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4349e-05 - val_loss: 1.2098\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3497e-05 - val_loss: 1.2145\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2650e-05 - val_loss: 1.2198\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1826e-05 - val_loss: 1.2242\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1060e-05 - val_loss: 1.2288\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0307e-05 - val_loss: 1.2337\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9640e-05 - val_loss: 1.2381\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8935e-05 - val_loss: 1.2428\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8270e-05 - val_loss: 1.2471\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7601e-05 - val_loss: 1.2527\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6985e-05 - val_loss: 1.2570\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6422e-05 - val_loss: 1.2613\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5864e-05 - val_loss: 1.2660\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5296e-05 - val_loss: 1.2707\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4775e-05 - val_loss: 1.2754\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4291e-05 - val_loss: 1.2797\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3753e-05 - val_loss: 1.2843\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3342e-05 - val_loss: 1.2889\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2855e-05 - val_loss: 1.2935\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2413e-05 - val_loss: 1.2977\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1993e-05 - val_loss: 1.3031\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1574e-05 - val_loss: 1.3071\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1192e-05 - val_loss: 1.3120\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0796e-05 - val_loss: 1.3159\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0443e-05 - val_loss: 1.3207\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0091e-05 - val_loss: 1.3253\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.7462e-06 - val_loss: 1.3296\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.4268e-06 - val_loss: 1.3344\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1021e-06 - val_loss: 1.3392\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.7950e-06 - val_loss: 1.3433\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5303e-06 - val_loss: 1.3480\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.2283e-06 - val_loss: 1.3525\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9558e-06 - val_loss: 1.3569\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.6991e-06 - val_loss: 1.3613\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.4359e-06 - val_loss: 1.3655\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1935e-06 - val_loss: 1.3702\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9743e-06 - val_loss: 1.3743\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7330e-06 - val_loss: 1.3789\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.5254e-06 - val_loss: 1.3833\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3007e-06 - val_loss: 1.3875\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.0912e-06 - val_loss: 1.3921\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8918e-06 - val_loss: 1.3964\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7035e-06 - val_loss: 1.4006\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5394e-06 - val_loss: 1.4051\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3481e-06 - val_loss: 1.4096\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1820e-06 - val_loss: 1.4140\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0090e-06 - val_loss: 1.4183\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8458e-06 - val_loss: 1.4222\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6922e-06 - val_loss: 1.4268\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.5435e-06 - val_loss: 1.4309\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4031e-06 - val_loss: 1.4354\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2585e-06 - val_loss: 1.4397\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1242e-06 - val_loss: 1.4443\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9846e-06 - val_loss: 1.4482\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8619e-06 - val_loss: 1.4525\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7425e-06 - val_loss: 1.4568\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6245e-06 - val_loss: 1.4608\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5095e-06 - val_loss: 1.4650\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4012e-06 - val_loss: 1.4696\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2907e-06 - val_loss: 1.4736\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1890e-06 - val_loss: 1.4779\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0899e-06 - val_loss: 1.4821\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9950e-06 - val_loss: 1.4863\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8995e-06 - val_loss: 1.4905\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8105e-06 - val_loss: 1.4952\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7242e-06 - val_loss: 1.4991\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6454e-06 - val_loss: 1.5035\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5555e-06 - val_loss: 1.5073\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4778e-06 - val_loss: 1.5117\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4039e-06 - val_loss: 1.5156\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3269e-06 - val_loss: 1.5201\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2587e-06 - val_loss: 1.5240\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1873e-06 - val_loss: 1.5285\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1208e-06 - val_loss: 1.5327\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0547e-06 - val_loss: 1.5368\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9963e-06 - val_loss: 1.5405\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9313e-06 - val_loss: 1.5451\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8712e-06 - val_loss: 1.5489\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8190e-06 - val_loss: 1.5526\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7599e-06 - val_loss: 1.5575\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7061e-06 - val_loss: 1.5613\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6557e-06 - val_loss: 1.5657\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6076e-06 - val_loss: 1.5693\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5602e-06 - val_loss: 1.5738\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5104e-06 - val_loss: 1.5775\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4660e-06 - val_loss: 1.5821\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4244e-06 - val_loss: 1.5858\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3803e-06 - val_loss: 1.5901\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3372e-06 - val_loss: 1.5941\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2968e-06 - val_loss: 1.5981\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2572e-06 - val_loss: 1.6023\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2214e-06 - val_loss: 1.6064\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1843e-06 - val_loss: 1.6107\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1493e-06 - val_loss: 1.6144\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1125e-06 - val_loss: 1.6185\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0815e-06 - val_loss: 1.6227\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0488e-06 - val_loss: 1.6264\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0187e-06 - val_loss: 1.6305\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9002e-07 - val_loss: 1.6346\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.5854e-07 - val_loss: 1.6384\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.3060e-07 - val_loss: 1.6422\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.0353e-07 - val_loss: 1.6465\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.7684e-07 - val_loss: 1.6510\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5077e-07 - val_loss: 1.6546\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.2616e-07 - val_loss: 1.6584\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.0089e-07 - val_loss: 1.6624\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7902e-07 - val_loss: 1.6664\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5451e-07 - val_loss: 1.6705\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3358e-07 - val_loss: 1.6747\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1034e-07 - val_loss: 1.6788\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9092e-07 - val_loss: 1.6827\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7158e-07 - val_loss: 1.6864\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.5104e-07 - val_loss: 1.6904\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3277e-07 - val_loss: 1.6945\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1452e-07 - val_loss: 1.6981\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9664e-07 - val_loss: 1.7021\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8019e-07 - val_loss: 1.7062\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.6353e-07 - val_loss: 1.7100\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4627e-07 - val_loss: 1.7138\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3084e-07 - val_loss: 1.7178\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1538e-07 - val_loss: 1.7217\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0224e-07 - val_loss: 1.7257\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8622e-07 - val_loss: 1.7296\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7328e-07 - val_loss: 1.7334\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.5930e-07 - val_loss: 1.7373\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4596e-07 - val_loss: 1.7415\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3393e-07 - val_loss: 1.7448\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2158e-07 - val_loss: 1.7485\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0915e-07 - val_loss: 1.7529\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9838e-07 - val_loss: 1.7566\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8676e-07 - val_loss: 1.7602\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7667e-07 - val_loss: 1.7642\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6616e-07 - val_loss: 1.7677\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5530e-07 - val_loss: 1.7718\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4591e-07 - val_loss: 1.7755\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3652e-07 - val_loss: 1.7793\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2702e-07 - val_loss: 1.7830\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1844e-07 - val_loss: 1.7866\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0933e-07 - val_loss: 1.7905\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0101e-07 - val_loss: 1.7942\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9297e-07 - val_loss: 1.7980\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8482e-07 - val_loss: 1.8013\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7760e-07 - val_loss: 1.8055\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6951e-07 - val_loss: 1.8088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5dn/8c+VhbDLDgKyKbghoEQQXAARRUBxrShaaFUerRZ9arVaW7VU+9PHp+4LUkXUWlFRFAUfd1QE2RRFEGQRJSwCYQuELcn1+2MGPOIJBMhksnzfL/LKObNekwP5MnPP3Le5OyIiIrtLibsAEREpnRQQIiKSlAJCRESSUkCIiEhSCggREUlKASEiIkkpIESKmZmNMrM79zB/k5m1KsmaRPaHAkLKLTNbYmanxV3H7ty9ursv3tMyZtbdzLJKqiaRZBQQIuWQmaXFXYOUfQoIqXDMLMPMHjCz5eHXA2aWEc6rZ2Zvmtl6M1trZp+YWUo4709mtszMcsxsvpn13MNuapvZ+HDZqWZ2aML+3cwOC1/3MbO54XLLzOyPZlYNeAtoHF6O2mRmjfdSd3czywprXAk8bWZfm9lZCftNN7M1ZnZs8f9UpTxSQEhFdCtwAtABaA90Av4SzrsByALqAw2BPwNuZocD1wLHu3sN4AxgyR72MQD4G1AbWAjcVchyTwH/FW6zLfCBu28GzgSWh5ejqrv78r3UDdAIqAM0B4YAzwKXJszvA6xw9y/2ULfILgoIqYgGAsPcfZW7ryb4RX5ZOG8HcDDQ3N13uPsnHnRYlg9kAEeZWbq7L3H3RXvYx1h3n+buecDzBL/Uk9kRbrOmu69z98/3s26AAuB2d9/m7luAfwN9zKxmOP8y4Lk9bF/kZxQQUhE1Br5PeP99OA3gXoL/8b9jZovN7GYAd18IXA/cAawys9Fm1pjCrUx4nQtUL2S58wn+Z/+9mX1kZl32s26A1e6+deeb8KzjU+B8M6tFcFby/B62L/IzCgipiJYTXIbZqVk4DXfPcfcb3L0VcDbwh51tDe7+H3c/KVzXgXsOtBB3n+7u/YEGwGvASztn7Uvde1jnGYLLTBcCU9x92YHWLBWHAkLKu3Qzq5zwlQa8APzFzOqbWT3gNoLLMZhZPzM7zMwM2EBwaanAzA43s1PDRuGtwBaCSzr7zcwqmdlAMzvI3XcAGxO2+SNQ18wOSlil0Lr34DXgOOA6gjYJkSJTQEh5N4Hgl/nOrzuAO4EZwFfAbODzcBpAa+A9YBMwBXjM3T8kaH+4G1hDcPmoAXBLMdR3GbDEzDYCVxG0M+Du8wgCYXF4R1XjvdSdVNgW8QrQEni1GOqVCsQ0YJBI+WZmtwFt3P3SvS4skkAP04iUY2ZWB7icn9/tJFIkusQkUk6Z2ZXAUuAtd/847nqk7NElJhERSUpnECIiklS5aoOoV6+et2jRIu4yRETKjJkzZ65x9/rJ5pWrgGjRogUzZsyIuwwRkTLDzL4vbJ4uMYmISFIKCBERSUoBISIiSZWrNohkduzYQVZWFlu3bt37wpJU5cqVadq0Kenp6XGXIiIlqNwHRFZWFjVq1KBFixYE/a/JvnB3srOzycrKomXLlnGXIyIlqNxfYtq6dSt169ZVOOwnM6Nu3bo6AxOpgMp9QAAKhwOkn59IxVQhAkJEpFzKz4N542HS/ZFsXgERoezsbDp06ECHDh1o1KgRTZo02fV++/bte1x3xowZDB06dJ/216JFC9asWXMgJYtIWbBuCbz/d7j/aBh9CUwfCXnbin035b6ROk5169Zl1qxZANxxxx1Ur16dP/7xj7vm5+XlkZaW/CPIzMwkMzOzROoUkTIgbzvMnwAzR8HiiWAGh/WCjvdB6zMgtfh/nesMooQNHjyYq666is6dO3PTTTcxbdo0unTpwrHHHkvXrl2ZP38+ABMnTqRfv35AEC6//e1v6d69O61ateKhhx7a637uu+8+2rZtS9u2bXnggQcA2Lx5M3379qV9+/a0bduWF198EYCbb76Zo446inbt2v0swESkFMheBO/eBvcdCS8PgjULoPstcP1sGPgSHNE3knCACnYG8bc35jB3+cZi3eZRjWty+1lH79M6WVlZTJ48mdTUVDZu3Mgnn3xCWloa7733Hn/+85955ZVXfrHOvHnz+PDDD8nJyeHwww/n6quvLvS5hJkzZ/L0008zdepU3J3OnTvTrVs3Fi9eTOPGjRk/fjwAGzZsIDs7m7FjxzJv3jzMjPXr1+/7D0FEild+Hnz7Fkx/ChZ/CJYKh58JHQfDoadCSmqJlBFZQJjZSKAfsMrd2yaZfyPh+LthHUcC9d19rZktAXIIBozPc/dyda3lwgsvJDU1+IA3bNjAoEGDWLBgAWbGjh07kq7Tt29fMjIyyMjIoEGDBvz44480bdo06bKTJk3i3HPPpVq1agCcd955fPLJJ/Tu3ZsbbriBP/3pT/Tr14+TTz6ZvLw8KleuzOWXX06/fv12nbWISAw2roDPn4GZz0DOcqjZFHr8BY67DGo0KvFyojyDGAU8AjybbKa73wvcC2BmZwH/7e5rExbp4e7F2uK6r//Tj8rOX9wAf/3rX+nRowdjx45lyZIldO/ePek6GRkZu16npqaSl5e3z/tt06YNn3/+ORMmTOAvf/kLPXv25LbbbmPatGm8//77jBkzhkceeYQPPvhgn7ctIvvJHb77KDhbmDcePB8O7Ql9/wmtT4/s8lFRRLZnd//YzFoUcfGLgReiqqU027BhA02aNAFg1KhRxbLNk08+mcGDB3PzzTfj7owdO5bnnnuO5cuXU6dOHS699FJq1arFk08+yaZNm8jNzaVPnz6ceOKJtGrVqlhqEJG9yF0LX74AM0ZC9kKoUge6XAOZv4E6pePfYextEGZWFegNXJsw2YF3zMyBJ9x9xB7WHwIMAWjWrFmUpUbipptuYtCgQdx555307du3WLZ53HHHMXjwYDp16gTAFVdcwbHHHsvbb7/NjTfeSEpKCunp6Tz++OPk5OTQv39/tm7dirtz3333FUsNIpKEOyybCdOfhK9fhfxt0PR4OPcJOOocSK8cd4U/E+mY1OEZxJvJ2iASlrkIuNTdz0qY1sTdl5lZA+Bd4PdFGXQ9MzPTdx8w6JtvvuHII4/czyOQnfRzFDkAO7bA16/AtH/BillQqTq0uyg4W2h0TKylmdnMwtp5Yz+DAAaw2+Uld18Wfl9lZmOBTsBeA0JEpFRZ+x3MeAq++DdsWQf1j4A+/wvtB0BGjbir26tYA8LMDgK6AZcmTKsGpLh7Tvj6dGBYTCWKiOybggJY+B5M/xcseBcsBY7sB8dfCS1OCh5wKyOivM31BaA7UM/MsoDbgXQAdx8eLnYu8I67b05YtSEwNuwgLg34j7v/X1R1iogUi9y1wZnCjKeCrjCqN4RuNwXPLtRsHHd1+yXKu5guLsIyowhuh02cthhoH01VIiLFbOVsmPoEzH4Z8rZCs67Q8zY44ixIqxR3dQekNLRBiIiULfk7YN6bMHUE/DAZ0qoEjc6dhkCjQu/JKXMUECIiRbV5TdBZ3vSngiedazWH0++EYy+FKrXjrq7YKSAilJ2dTc+ePQFYuXIlqamp1K9fH4Bp06ZRqdKeTz8nTpxIpUqV6Nq16y/mjRo1ihkzZvDII48Uf+Ei8nMrvvrpMlL+NmjVHfrdFzzpXEL9IsVBARGhvXX3vTcTJ06kevXqSQNCRCKWnwfzxwfB8P2nkF41OFPoNAQaHBF3dSVC3X2XsJkzZ9KtWzc6duzIGWecwYoVKwB46KGHdnW5PWDAAJYsWcLw4cO5//776dChA5988kmh21yyZAmnnnoq7dq1o2fPnvzwww8AvPzyy7Rt25b27dtzyimnADBnzhw6depEhw4daNeuHQsWLIj+oEXKkty1MOkBeKgDvPRr2LAUTr8L/jA3OGuoIOEAFe0M4q2bgzsOilOjY+DMu4u0qLvz+9//ntdff5369evz4osvcuuttzJy5EjuvvtuvvvuOzIyMli/fj21atXiqquuKtJZx+9//3sGDRrEoEGDGDlyJEOHDuW1115j2LBhvP322zRp0mRXN97Dhw/nuuuuY+DAgWzfvp38/PwD/hGIlAs/zoVpT8CXL0LeFmhxMpx5D7TpXa4vI+1JxQqImG3bto2vv/6aXr16AZCfn8/BBx8MQLt27Rg4cCDnnHMO55xzzj5td8qUKbz66qsAXHbZZdx0000AnHjiiQwePJhf/epXnHfeeQB06dKFu+66i6ysLM477zxat25dXIcnUvYU5MO3b8PU4UGPqmmVod2voPNV0LB09P4cp4oVEEX8n35U3J2jjz6aKVOm/GLe+PHj+fjjj3njjTe46667mD37wM90hg8fztSpUxk/fjwdO3Zk5syZXHLJJXTu3Jnx48fTp08fnnjiCU499dQD3pdImbJ1A3zxfHDGsG4J1GwCPW8PHmqrWifu6koNtUGUoIyMDFavXr0rIHbs2MGcOXMoKChg6dKl9OjRg3vuuYcNGzawadMmatSoQU5Ozl6327VrV0aPHg3A888/z8knnwzAokWL6Ny5M8OGDaN+/fosXbqUxYsX06pVK4YOHUr//v356quvojtgkdJm7WJ4609w31Hw9i1Q42C4cBRc9xWc/AeFw24q1hlEzFJSUhgzZgxDhw5lw4YN5OXlcf3119OmTRsuvfRSNmzYgLszdOhQatWqxVlnncUFF1zA66+/zsMPP7zrF//uHn74YX7zm99w7733Ur9+fZ5++mkAbrzxRhYsWIC707NnT9q3b88999zDc889R3p6Oo0aNeLPf/5zSf4IREqeOyyZBJ89DvMnQEoatD0fTrgKGh8bd3WlWqTdfZc0dfcdHf0cpczJ2xZ0sf3ZY8HNKVXrQuZv4fgrYhm+s7Qq7d19i4gUn40rglHaZj4Nm1dD/SPh7IfhmAshvUrc1ZUpCggRKR+Wz4Ipj8KcV4O7k9qcAZ3/C1r1KFNdbJcmFSIg3B3TX5D9Vp4uQ0o5U1AAC96BKY/Akk+CkdqOvxI6XQl1D427ujKv3AdE5cqVyc7Opm7dugqJ/eDuZGdnU7ly6RorVyq4HVuD9oXJD8Pqb4LbVHv9HY77NVSpFXd15Ua5D4imTZuSlZXF6tWr4y6lzKpcuTJNmzaNuwwR2JAV9KT6+TOQmw0NjoJznwjuSkpNj7u6cqfcB0R6ejotW7aMuwwRORBZM2HyQ/DNG4BDmzOh8xBo2U3tCxEq9wEhImXUzvaFyQ/D95Mg4yDock1wm2rt5nFXVyFEOSb1SKAfsMrdfzHEkpl1B14Hvgsnveruw8J5vYEHgVTgSXePt48MESk52zbBly8ED7atXQQ1m8IZ/wjaFzJqxF1dhRLlGcQo4BHg2T0s84m790ucYGapwKNALyALmG5m49x9blSFikgpkLMyGHthxlNBX0lNOsL5T8FR/dW+EJPIAsLdPzazFvuxaidgobsvBjCz0UB/QAEhUh6tmhdcRpr9UjDW85FnQZdr4ZBOal+IWdxtEF3M7EtgOfBHd58DNAGWJiyTBXQubANmNgQYAtCsWbMISxWRYrOzf6TJDwXtDGlVgktIJ/xOzy+UInEGxOdAc3ffZGZ9gNeAfR6cwN1HACMg6IupeEsUkWKVnwdzXwvOGFbMgqr1oMetkHk5VKsbd3Wym9gCwt03JryeYGaPmVk9YBlwSMKiTcNpIlJWbcuBz58LGp43/AB1D4N+D0D7AeofqRSLLSDMrBHwo7u7mXUiGJsiG1gPtDazlgTBMAC4JK46ReQAbFwejNY2YxRs2wDNugQDd7U5E1I0HE1pF+Vtri8A3YF6ZpYF3A6kA7j7cOAC4GozywO2AAM86PQnz8yuBd4muM11ZNg2ISJlxer58OmD8NVL4Plw5NnQ9ffQNGmv0lJKlfvxIESkBP0wFT59IBiYJ60KHHdZ8HBb7RZxVyaF0HgQIhKdnU88f/oA/DAFqtSGbjdDpyFqeC7jFBAisn/yd8DsMcGlpNXfwEGHQO97grOGStXirk6KgQJCRPbNtk3w+bPB4Dwbs6DB0XDuCGh7np54LmcUECJSNJvXBHckTfsXbF0PzU+EfvdD61564rmcUkCIyJ6t/S4Yse2Lf0PeNjiiL5x4PRxyfNyVScQUECKS3IqvgobnOWPBUqH9RdD1OqjfJu7KpIQoIETkJ+7B2M6T7odFH0ClGsFtqif8Dmo2jrs6KWEKCBEJblWdPwEm3QfLZkK1BtDztqCPJI3xXGEpIEQqsvw8+HpMcMaweh7Uag5974MOAyG9ctzVScwUECIVUd62YNS2SffDuiXQ4Cg470k4+lxI1a8FCehvgkhFsmNL8AzDpw/CxmXQ+Fg44/9Bm97qPE9+QQEhUhFsy4EZI2HyI7B5VdCr6tkPw6Gn6hkGKZQCQqQ827Iepo2Azx6DLeugVQ84ZRS0ODHuyqQMUECIlEc5K4OuMGY8DdtzgvEXTvmjutuWfaKAEClPclbCpAdg5tOQvz1odD7xeji4XdyVSRmkgBApD3JWBg3PM0YGvay2vxhO/gPUPTTuyqQMU0CIlGU5PwbdYSQGwyk3QJ1WcVcm5YACQqQsyvkxPGN4KgyGAXDyDTpjkGIV5ZjUI4F+wCp3b5tk/kDgT4ABOcDV7v5lOG9JOC0fyCtsODyRCifnR5j8EEx/KmhjUDBIhKI8gxgFPAI8W8j874Bu7r7OzM4ERgCdE+b3cPc1EdYnUnZsWhWcMUx/CvK3QbsBwV1JCgaJUGQB4e4fm1mLPcyfnPD2M6BpVLWIlFm/CIaL4JQbFQxSIkpLG8TlwFsJ7x14x8wceMLdRxS2opkNAYYANGvWLNIiRUrMptUw+UGY9qSCQWITe0CYWQ+CgDgpYfJJ7r7MzBoA75rZPHf/ONn6YXiMAMjMzPTICxaJ0s5gmP4U5G2FY34VBEO9w+KuTCqgWAPCzNoBTwJnunv2zunuviz8vsrMxgKdgKQBIVIubFodNj4/GQbDhWEwtI67MqnAYgsIM2sGvApc5u7fJkyvBqS4e074+nRgWExlikRr85ogGKb9S8EgpU6Ut7m+AHQH6plZFnA7kA7g7sOB24C6wGMW9Ca583bWhsDYcFoa8B93/7+o6hSJxe7B0PaCIBg03rOUIlHexXTxXuZfAVyRZPpioH1UdYnEavMamPxwEAw7cuGYC+CUmxQMUirF3kgtUiFszv7pjGFXMNwI9Q+PuzKRQikgRKKUuzYIhqkjgmBoez50u0nBIGWCAkIkCttyYMpjMOWR4HXb86DbnxQMUqYoIESK044twa2qk+6H3Gw4oh/0uBUaHhV3ZSL7TAEhUhzyd8AXz8FH90LO8mCs51P/Ak06xl2ZyH5TQIgciIJ8mD0GJv4D1i2BQzrD+f+CFiftdVWR0k4BIbI/3GHeePjgTlj9DTQ6Bi55GVr3guAZHpEyTwEhsi/cYfGH8P7fYfnnULc1XDgKjuwPKSlxVydSrBQQIkX1w1T44O+w5BM46BDo/2gwLkOq/hlJ+aS/2SJ78+NceH8YfPsWVKsPZ/4PdBwMaRlxVyYSKQWESGHWL4WJ/w9m/QcyasKpf4UTroZK1eKuTKREKCBEdpe7Fj75Z9AtBkDXa+GkP0DVOvHWJVLCFBAiO23PhamPw6QHYPsmaH8xdL8Fah0Sd2UisVBAiOTnBQ+5TbwbNq2ENmdCz9v09LNUeAoIqbjc4ZtxQQN09sLgIbcLR0HzLnFXJlIqKCCkYvruY3jvDlg2E+ofAQNegMPP1ENuIgkUEFKxrJwdBMPC96Bmk+BZhvYXQ0pq3JWJlDoKCKkY1i2BD+6C2S9D5YOg19+h05WQXiXuykRKrUj7BjCzkWa2ysy+LmS+mdlDZrbQzL4ys+MS5g0yswXh16Ao65RybPMaeOtP8HAmfPMGnHQ9XPclnDhU4SCyF1GfQYwCHgGeLWT+mUDr8Ksz8DjQ2czqALcDmYADM81snLuvi7heKS92bIHPHoNP7ocdm+HYy6D7zVCzcdyViZQZkQaEu39sZi32sEh/4Fl3d+AzM6tlZgcD3YF33X0tgJm9C/QGXoiyXikHCgrg6zHBnUkblsLhfeC0OzSSm8h+iLsNogmwNOF9VjitsOkihVvyKbxzKyz/Ag5uD+c8Bi1PibsqkTIr7oA4YGY2BBgC0KxZs5irkVhkL4J3b4N5b0KNxnDOcGh3kbrfFjlAcQfEMiCxH4Om4bRlBJeZEqdPTLYBdx8BjADIzMz0KIqUUmrTavjoHpj5NKRVDob4POEaqFQ17spEyoW4A2IccK2ZjSZopN7g7ivM7G3gH2ZWO1zudOCWuIqUUmZ7Lnz2KEx6EHbkQsdB0O1mqNEw7spEypVIA8LMXiA4E6hnZlkEdyalA7j7cGAC0AdYCOQCvwnnrTWzvwPTw00N29lgLRVYQT7Meh4+/AfkrIAj+kHP26F+m7grEymXihQQZnYd8DSQAzwJHAvc7O7v7Gk9d794L/MduKaQeSOBkUWpT8o5d1jwDrx7ezD+c9Pj4YKn1WeSSMSKegbxW3d/0MzOAGoDlwHPAXsMCJEDtuob+L9bgnGg67SCXz0LR56tPpNESkBRA2Lnv8Y+wHPuPsdM/0IlQrlrg9Hcpj8FGdWh992QeTmkVYq7MpEKo6gBMdPM3gFaAreYWQ2gILqypMLK2w4zngrGZti2ETJ/C93/DNXqxl2ZSIVT1IC4HOgALHb33LArjN9EV5ZUOO4wbzy8+1dYuxhadYcz/gENj467MpEKq6gB0QWY5e6bzexS4DjgwejKkgpl+Sx4+1b4fhLUOxwGjoHDTlM7g0jMihoQjwPtzaw9cAPBnUzPAt2iKkwqgE2r4P2/wRfPQ9W60PefcNxgSI378RwRgaIHRJ67u5n1Bx5x96fM7PIoC5NyLG87TBsRPAW9Ywt0vRZOuTEYp0FESo2iBkSOmd1CcHvryWaWQvjAm8g+WfhecNvqmm/hsF7B3Un1Dou7KhFJoqgBcRFwCcHzECvNrBlwb3RlSbnz41z44O8wf0LwPMMlL0GbM+KuSkT2oEgBEYbC88DxZtYPmObuhQ0CJPKT3LXBnUlf/BsqVYfT/gYnXA1pGXFXJiJ7UdSuNn5FcMYwkeChuYfN7EZ3HxNhbVKWFeQHvax+cCds3Qhdh8JJ/w1V68RdmYgUUVEvMd0KHO/uqwDMrD7wHqCAkF/KmgFv/jes/ApanBy0MzRqG3dVIrKPihoQKTvDIZQNaDQW+bnctcFtqzOfgRqNgg71jj5XzzOIlFFFDYj/C8do2Dkm9EUEXXWLBONAf/mfYFS3LeuhyzXQ/WbIqBF3ZSJyAIraSH2jmZ0PnBhOGuHuY6MrS8qMH+fA+BvghylwSGfoe58uJ4mUE0V+ZNXdXwFeibAWKUu25QQd6n32ePCA29mPQIeBGgdapBzZY0CYWQ6QbJxnIxjvp2YkVUnp5Q5zXw8edstZDscNgtPu0N1JIuXQHgPC3XURWX6SvQjeuil4GrrRMcHgPYccH3dVIhIR9Yome7djK3z6AHxyH6RWgt73wPFXqFM9kXIu0n/hZtaboFvwVOBJd797t/n3Az3Ct1WBBu5eK5yXD8wO5/3g7mdHWasUYuF7MP6PsO47aHs+nH4X1Dw47qpEpAREFhBmlgo8CvQCsoDpZjbO3efuXMbd/zth+d8DxyZsYou7d4iqPtmLTauDy0lzXoW6h8Flr8GhPfa+noiUG1GeQXQCFrr7YgAzGw30B+YWsvzFwO0R1iNF4Q5fvwITboTtm6DHrXDideo7SaQCijIgmgBLE95nAZ2TLWhmzQnGu/4gYXJlM5sB5AF3u/trhaw7BBgC0KxZs2IouwLbuCJ4pmH+eGjSEfo/Cg2OjLsqEYlJaWllHACMcff8hGnN3X2ZmbUCPjCz2e6+aPcV3X0EMAIgMzMz2S25sjfuMOs/8PYtkLcNTr8TTvgdpKTGXZmIxCjKgFgGHJLwvmk4LZkBwDWJE9x9Wfh9sZlNJGif+EVAyAFavxTeuA4WvQ/NukL/R6DuoXFXJSKlQJQBMR1obWYtCYJhAMGgQz9jZkcAtYEpCdNqA7nuvs3M6hF08fE/EdZa8RQUBN1xv3tbcAZx5r3Brat6ElpEQpEFhLvnmdm1wNsEt7mOdPc5ZjYMmOHu48JFBwCj3T3x8tCRwBNmVkDQa+zdiXc/yQFavxRe/x189zG07AZnPwS1W8RdlYiUMvbz38tlW2Zmps+YMSPuMkovd/jqJZjwR/CCoK2h42B1xy1SgZnZTHfPTDavtDRSS9S2rIM3roe5r8EhJ8C5w6FOy7irEpFSTAFREXw/BV65AjathJ63B8816A4lEdkLBUR5VpAPH/8vfHQ31GoOl78TPN8gIlIECojyasMyeHUIfD8JjvkV9P0nVFbv7CJSdAqI8mjehOAupbztcM5w6HBx3BWJSBmkgChP8rbDu3+FqcOhUTu44Gmod1jcVYlIGaWAKC82rYKXfh2MDd35auj1N3WwJyIHRAFRHiz7HEYPDG5lvWBkMG6DiMgBUkCUdV+OhnFDoXrD4C6lg9vFXZGIlBMKiLIqPy/oR+mzR6HFyXDhKKhWL+6qRKQcUUCURblr4eXB8N1H0PmqoMuM1PS4qxKRckYBAUyYvYJmdarStslBcZeyd6u/hRcugg1ZwYA+x14ad0UiUk6pb2fghpe+5PVZhQ1VUYos+hCePA22boRBbyocRCRSCgigaqVUcrfn733BOM0cBf8+Hw5qAld+AM2Sjt4qIlJsdIkJqJqRypbSGhAF+fDe7TD5YTjstODhN3WZISIlQAEBVE1PK51nENtz4dUrYd6bcPyV0PtuSNVHJiIlQ79tgCqVUsndUcoCYscWGH0JLJ4Ive+BE66KuyIRqWAUEIRtENvy4i7jJ1s3wAuXwPefhncqDYy7IhGpgCJtpDaz3mY238wWmtnNSeYPNrPVZjYr/LoiYd4gM1sQfg2Kss5S1Ui9eQ2M6gdLP4Pzn1Q4iEhsIjuDMLNU4FGgF5AFTDezce4+d7dFX3T3a3dbtw5wO5AJODAzXHddFLVWqZTGltJwiWnTanj2bFj7HT4Kb8gAABFxSURBVFw8Glr3irsiEanAojyD6AQsdPfF7r4dGA30L+K6ZwDvuvvaMBTeBXpHVCfVKqWSuz3mS0ybVsMzZwXhcMmLCgcRiV2UAdEEWJrwPiuctrvzzewrMxtjZofs47qY2RAzm2FmM1avXr1fhVaJ+xLTznBYtyQIh1bd4qtFRCQU94NybwAt3L0dwVnCM/u6AXcf4e6Z7p5Zv379/SpiZxuEu+/X+gckMRwGvqRwEJFSI8qAWAYckvC+aThtF3fPdvdt4dsngY5FXbc4Va2URn6Bsz2/IKpdJLdpNTzT76dwaHlKye5fRGQPogyI6UBrM2tpZpWAAcC4xAXM7OCEt2cD34Sv3wZON7PaZlYbOD2cFokq6akAJfs09Zb18Ny5sO57GPiywkFESp3I7mJy9zwzu5bgF3sqMNLd55jZMGCGu48DhprZ2UAesBYYHK671sz+ThAyAMPcfW1EhXLOtItZnnocudtPpVbVSPbyc9tz4T8Xwep5cMloaHlyCexURGTfRPqgnLtPACbsNu22hNe3ALcUsu5IYGSU9QFgRrWtK2luP5ZMQ3XednjpMsiaFgwPethp0e9TRGQ/xN1IXSrkZdSilm2K/lbXgnwYOwQWvgf9HoCjz412fyIiB0ABARRk1OIgNkd7BuEO4/8Ac8ZCr2HQMdKHw0VEDpgCAiioHJxBRNpI/d4dwZgOJ/0BTrwuuv2IiBQTBQRAldrUYlN0ZxCTH4ZPH4DM30LP2/a+vIhIKaCAAFKq1o6uDWLOa/DOX+Co/tDnf8Gs+PchIhIBdfcNpFarQ1XbwpatW4t3w0unwdj/gkM6w7lPQEpq8W5fRCRCOoMA0qrVBSA/d0PxbXTtYnhhANQ4GAa8AOlVim/bIiIlQAEBpNcIAoItxfQsXu5aeP5C8AIYOAbCABIRKUt0iQmwKnUA8C3FMNxE3jYYPRDW/wC/Hgf1DjvwbYqIxEABAVClNgD5mw7wDMIdXr8GfpgM5z8FzbsUQ3EiIvHQJSaAKrUAKMg9wID48C6Y/XJwK+sxFxRDYSIi8VFAwK4zCNt6AJeYPn8OPr4Xjr0seBhORKSMU0AAVD6IAozUrev3b/1FH8Kb10OrHtDvfj3rICLlggICICWVLem1qJm3et9HlVs5G168DOq1gV89A6np0dQoIlLCFBChjdVacShZbNy6D09Tr/se/n0BVK4Z3M5a+aDoChQRKWEKiNCW2ofT2paRnVPEp6lz18K/z4e8LXDpK3BQk2gLFBEpYQqIUEG9w6lhW9j445K9L7xjS/CU9Pof4OLR0ODIyOsTESlpCohQWqOjAdixcu6eF8zbDmN+G/SzdN4IaN61BKoTESl5kQaEmfU2s/lmttDMbk4y/w9mNtfMvjKz982secK8fDObFX6Ni7JOgGpN2wKQsmZe4Qvl74BXfgvzJ0Df/4Wjz4m6LBGR2ET2JLWZpQKPAr2ALGC6mY1z98T/on8BZLp7rpldDfwPcFE4b4u7d4iqvt3VrteQHwrqU+/HT5MvsD03GC70mzeg991w/BUlVZqISCyiPIPoBCx098Xuvh0YDfRPXMDdP3T33PDtZ0DTCOvZo7TUFMannkrz9VMhe9HPZ+ashFF94Zs3ofc9cMLV8RQpIlKCogyIJsDShPdZ4bTCXA68lfC+spnNMLPPzKxEruV8Xu9s8kiFd2+D/Lyg473PHofHusDq+TDgP3DCVSVRiohI7EpFZ31mdimQCXRLmNzc3ZeZWSvgAzOb7e6Lkqw7BBgC0KxZswOqo97BzXlwzUBumPcsPNQBtm8OugBv1T24rKS7lUSkAokyIJYBhyS8bxpO+xkzOw24Fejm7tt2Tnf3ZeH3xWY2ETgW+EVAuPsIYARAZmbmPj4G/XOtG1Rn2JbeXHHBKRy0aBxUqg7tLoRDTz2QzYqIlElRBsR0oLWZtSQIhgHAJYkLmNmxwBNAb3dflTC9NpDr7tvMrB5wIkEDdqRaN6wOwJzaPeh60YVR705EpFSLrA3C3fOAa4G3gW+Al9x9jpkNM7Ozw8XuBaoDL+92O+uRwAwz+xL4ELh7t7ufItGmYQ0AFvy4KepdiYiUepG2Qbj7BGDCbtNuS3h9WiHrTQaOibK2ZBrUyKBm5TTmrdxY0rsWESl19CR1AjPj+BZ1mLIoO+5SRERip4DYzYmH1WNJdi5L1+bufWERkXJMAbGbk1rXA2DyojUxVyIiEi8FxG5aN6hOw5oZvP/Nqr0vLCJSjikgdmNm9GvXmA/nr2Ld5u1xlyMiEhsFRBLnHdeEHfnOm18tj7sUEZHYKCCSOLrxQRzduCbPTPmegoIDejhbRKTMUkAU4r+6HcrCVZt4Z+7KuEsREYmFAqIQfY85mJb1qnHv2/PZnlcQdzkiIiVOAVGI1BTjr/2OZNHqzTz96XdxlyMiUuIUEHtw6hEN6XVUQ/757rfMX5kTdzkiIiVKAbEX/++8Y6hZOY2rn5/Jhi074i5HRKTEKCD2ol71DB655Dh+yM7lymdnkLNVISEiFYMCoghOaFWX+y/qwOffr+PSJ6fqAToRqRAUEEV0VvvGPHFZR75ZmcM5j33KrKXr4y5JRCRSCoh90PPIhrxwZWfy8p0LHp/MA+99y9Yd+XGXJSISCQXEPurYvA4TrjuZPscczAPvLaDnPz9izMwsPSshIuWOuZefriQyMzN9xowZJba/KYuyuXP8XOYs30iDGhlc0rkZ/Ts0oWW9aiVWg4jIgTCzme6emXSeAuLAFBQ4Hy9YzVOTvuOTBcEYEkc0qsEJrepyQqu6dG5Zh9rVKpVoTSIiRRVbQJhZb+BBIBV40t3v3m1+BvAs0BHIBi5y9yXhvFuAy4F8YKi7v723/cUREIlWbNjCm1+uYOK3q5j5/Tq27gguOx1avxqtG9TgsAbVObRBNVrWC8acqFc9g/RUXeUTkfjEEhBmlgp8C/QCsoDpwMXuPjdhmd8B7dz9KjMbAJzr7heZ2VHAC0AnoDHwHtDG3ffYIhx3QCTanlfAV1nr+WxxNl9mbWDRqk18vzaX/ITeYc2gbrVK1K9RmXrVK1GzSjo1K6dRo3I6NTLSqF45jcrpqVRKTSEjPYWMtFQqpaWQkZay63tGWgqpKSmkmmEWdBGSmmKkWPA91YyUFJJMsxh/OiJSWuwpINIi3G8nYKG7Lw6LGA30B+YmLNMfuCN8PQZ4xMwsnD7a3bcB35nZwnB7UyKst1hVSkshs0UdMlvU2TVte14B32dvZkl2LqtytrJq4zZW5Wxj1catrNm0jWXrtpCzLY+crTt2nX1EaWdYWJgVZmAkvCcYQGnnayz8Hk633d+H29i59s/nB9v+6fVP2941bdd+LWE7B6a4YtCKqaBii2X9fCRB7aqVeOmqLsW+3SgDogmwNOF9FtC5sGXcPc/MNgB1w+mf7bZuk2Q7MbMhwBCAZs2aFUvhUamUlkLrhjVo3bDGXpfdnlfA5m15bMsrYFtePtvzCsLXwftteQW7puUXFJBfELSHFLiT705BgZNf4OR7MD3fg/c7X/80DRwn/MPOM0oP3//02kk82XT3Qud7OG3nFtxJmOdJt83P1i2es9riOjcurpPs4qundP18imtDXnwVVTg1K6dHst0oA6JEuPsIYAQEl5hiLqfYVEpLoVKaGrdFJD5RtpAuAw5JeN80nJZ0GTNLAw4iaKwuyroiIhKhKANiOtDazFqaWSVgADBut2XGAYPC1xcAH3hw/jwOGGBmGWbWEmgNTIuwVhER2U1kl5jCNoVrgbcJbnMd6e5zzGwYMMPdxwFPAc+FjdBrCUKEcLmXCBq084Br9nYHk4iIFC89KCciUoHt6TZXPaUlIiJJKSBERCQpBYSIiCSlgBARkaTKVSO1ma0Gvt/P1esBa4qxnDjpWEqf8nIcoGMprfb3WJq7e/1kM8pVQBwIM5tRWEt+WaNjKX3Ky3GAjqW0iuJYdIlJRESSUkCIiEhSCoifjIi7gGKkYyl9ystxgI6ltCr2Y1EbhIiIJKUzCBERSUoBISIiSVX4gDCz3mY238wWmtnNcdezr8xsiZnNNrNZZjYjnFbHzN41swXh99px15mMmY00s1Vm9nXCtKS1W+Ch8HP6ysyOi6/yXyrkWO4ws2XhZzPLzPokzLslPJb5ZnZGPFUnZ2aHmNmHZjbXzOaY2XXh9DL32ezhWMrcZ2Nmlc1smpl9GR7L38LpLc1saljzi+HwCoTDJbwYTp9qZi32eafuXmG/CLohXwS0AioBXwJHxV3XPh7DEqDebtP+B7g5fH0zcE/cdRZS+ynAccDXe6sd6AO8RTBs8QnA1LjrL8Kx3AH8McmyR4V/1zKAluHfwdS4jyGhvoOB48LXNYBvw5rL3Gezh2Mpc59N+POtHr5OB6aGP++XgAHh9OHA1eHr3wHDw9cDgBf3dZ8V/QyiE7DQ3Re7+3ZgNNA/5pqKQ3/gmfD1M8A5MdZSKHf/mGAckESF1d4feNYDnwG1zOzgkql07wo5lsL0B0a7+zZ3/w5YSPB3sVRw9xXu/nn4Ogf4hmBM+DL32ezhWApTaj+b8Oe7KXybHn45cCowJpy+++ey8/MaA/Q0M9uXfVb0gGgCLE14n8We//KURg68Y2YzzWxIOK2hu68IX68EGsZT2n4prPay+lldG152GZlwqa/MHEt4WeJYgv+tlunPZrdjgTL42ZhZqpnNAlYB7xKc4ax397xwkcR6dx1LOH8DUHdf9lfRA6I8OMndjwPOBK4xs1MSZ3pwflkm72Uuy7WHHgcOBToAK4B/xlvOvjGz6sArwPXuvjFxXln7bJIcS5n8bNw93907AE0JzmyOiHJ/FT0glgGHJLxvGk4rM9x9Wfh9FTCW4C/NjztP8cPvq+KrcJ8VVnuZ+6zc/cfwH3QB8C9+ulRR6o/FzNIJfqE+7+6vhpPL5GeT7FjK8mcD4O7rgQ+BLgSX9HYOH51Y765jCecfBGTvy34qekBMB1qHdwFUImjIGRdzTUVmZtXMrMbO18DpwNcExzAoXGwQ8Ho8Fe6XwmofB/w6vGPmBGBDwuWOUmm36/DnEnw2EBzLgPAuk5ZAa2BaSddXmPA69VPAN+5+X8KsMvfZFHYsZfGzMbP6ZlYrfF0F6EXQpvIhcEG42O6fy87P6wLgg/DMr+jibpmP+4vgDoxvCa7l3Rp3PftYeyuCOy6+BObsrJ/gOuP7wALgPaBO3LUWUv8LBKf3OwiunV5eWO0Ed3A8Gn5Os4HMuOsvwrE8F9b6VfiP9eCE5W8Nj2U+cGbc9e92LCcRXD76CpgVfvUpi5/NHo6lzH02QDvgi7Dmr4HbwumtCEJsIfAykBFOrxy+XxjOb7Wv+1RXGyIiklRFv8QkIiKFUECIiEhSCggREUlKASEiIkkpIEREJCkFhEgpYGbdzezNuOsQSaSAEBGRpBQQIvvAzC4N++SfZWZPhJ2nbTKz+8M++t83s/rhsh3M7LOwQ7ixCeMnHGZm74X9+n9uZoeGm69uZmPMbJ6ZPb+vPW+KFDcFhEgRmdmRwEXAiR50mJYPDASqATPc/WjgI+D2cJVngT+5ezuCp3Z3Tn8eeNTd2wNdCZ7AhqCn0esJxiRoBZwY+UGJ7EHa3hcRkVBPoCMwPfzPfRWCDusKgBfDZf4NvGpmBwG13P2jcPozwMth31lN3H0sgLtvBQi3N83ds8L3s4AWwKToD0skOQWESNEZ8Iy73/KziWZ/3W25/e2/ZlvC63z071NipktMIkX3PnCBmTWAXWM0Nyf4d7SzN81LgEnuvgFYZ2Ynh9MvAz7yYFSzLDM7J9xGhplVLdGjECki/Q9FpIjcfa6Z/YVgBL8Ugp5brwE2A53CeasI2ikg6Gp5eBgAi4HfhNMvA54ws2HhNi4swcMQKTL15ipygMxsk7tXj7sOkeKmS0wiIpKUziBERCQpnUGIiEhSCggREUlKASEiIkkpIEREJCkFhIiIJPX/ASerKANqytExAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.855\n",
            "Roc-Auc score: 0.8551056309012409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XASCB0KdBY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "daab7ec4-e07f-4c58-83dd-6ca835c84b1a"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(64, activation='relu')(X_input_1) # 선형 projection\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(64, activation='relu')(X_input_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           1569984     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           25664       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 128)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,595,777\n",
            "Trainable params: 1,595,777\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 18ms/step - loss: 0.5885 - val_loss: 0.4677\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3764 - val_loss: 0.3298\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2643 - val_loss: 0.2730\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2070 - val_loss: 0.2493\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1714 - val_loss: 0.2399\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1456 - val_loss: 0.2373\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1260 - val_loss: 0.2388\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1101 - val_loss: 0.2430\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0968 - val_loss: 0.2493\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0858 - val_loss: 0.2581\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0762 - val_loss: 0.2644\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0680 - val_loss: 0.2733\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0608 - val_loss: 0.2819\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0545 - val_loss: 0.2919\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0490 - val_loss: 0.3024\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0438 - val_loss: 0.3128\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0393 - val_loss: 0.3236\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0355 - val_loss: 0.3342\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0321 - val_loss: 0.3458\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0290 - val_loss: 0.3561\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0262 - val_loss: 0.3673\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0236 - val_loss: 0.3778\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0214 - val_loss: 0.3881\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0194 - val_loss: 0.3992\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0177 - val_loss: 0.4089\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0159 - val_loss: 0.4196\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0145 - val_loss: 0.4298\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0132 - val_loss: 0.4399\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0121 - val_loss: 0.4502\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0110 - val_loss: 0.4594\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0101 - val_loss: 0.4698\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0092 - val_loss: 0.4791\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0084 - val_loss: 0.4887\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0078 - val_loss: 0.4986\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0072 - val_loss: 0.5070\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0066 - val_loss: 0.5163\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0061 - val_loss: 0.5256\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0056 - val_loss: 0.5347\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0052 - val_loss: 0.5434\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0049 - val_loss: 0.5508\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0046 - val_loss: 0.5596\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0042 - val_loss: 0.5680\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0040 - val_loss: 0.5766\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0037 - val_loss: 0.5841\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0035 - val_loss: 0.5923\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0033 - val_loss: 0.6000\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0031 - val_loss: 0.6076\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0029 - val_loss: 0.6154\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0027 - val_loss: 0.6227\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6304\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0024 - val_loss: 0.6372\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0023 - val_loss: 0.6445\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6518\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0020 - val_loss: 0.6589\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.6653\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.6732\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.6798\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6862\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6932\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6993\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.7057\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7126\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7190\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7259\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7319\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7382\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7439\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7503\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.7707e-04 - val_loss: 0.7568\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.3429e-04 - val_loss: 0.7632\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.9471e-04 - val_loss: 0.7692\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.5828e-04 - val_loss: 0.7752\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.2362e-04 - val_loss: 0.7814\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9146e-04 - val_loss: 0.7861\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5776e-04 - val_loss: 0.7922\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.2675e-04 - val_loss: 0.7986\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9808e-04 - val_loss: 0.8045\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.6970e-04 - val_loss: 0.8102\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4516e-04 - val_loss: 0.8157\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2225e-04 - val_loss: 0.8215\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9641e-04 - val_loss: 0.8274\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7321e-04 - val_loss: 0.8341\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5378e-04 - val_loss: 0.8387\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3169e-04 - val_loss: 0.8441\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1315e-04 - val_loss: 0.8492\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9387e-04 - val_loss: 0.8547\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7682e-04 - val_loss: 0.8609\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6025e-04 - val_loss: 0.8662\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4380e-04 - val_loss: 0.8714\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2894e-04 - val_loss: 0.8773\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1252e-04 - val_loss: 0.8828\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9811e-04 - val_loss: 0.8886\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8424e-04 - val_loss: 0.8939\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7086e-04 - val_loss: 0.8992\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5857e-04 - val_loss: 0.9043\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4538e-04 - val_loss: 0.9098\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3369e-04 - val_loss: 0.9147\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2204e-04 - val_loss: 0.9206\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1243e-04 - val_loss: 0.9251\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0249e-04 - val_loss: 0.9303\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9174e-04 - val_loss: 0.9362\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8209e-04 - val_loss: 0.9414\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7222e-04 - val_loss: 0.9468\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6418e-04 - val_loss: 0.9520\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5462e-04 - val_loss: 0.9576\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4745e-04 - val_loss: 0.9627\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3807e-04 - val_loss: 0.9679\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3159e-04 - val_loss: 0.9728\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2395e-04 - val_loss: 0.9779\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1691e-04 - val_loss: 0.9840\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1093e-04 - val_loss: 0.9883\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0347e-04 - val_loss: 0.9925\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9794e-04 - val_loss: 0.9980\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9156e-04 - val_loss: 1.0029\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8562e-04 - val_loss: 1.0076\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7998e-04 - val_loss: 1.0130\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7536e-04 - val_loss: 1.0182\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6839e-04 - val_loss: 1.0236\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6417e-04 - val_loss: 1.0289\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5865e-04 - val_loss: 1.0341\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5289e-04 - val_loss: 1.0393\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4991e-04 - val_loss: 1.0446\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4424e-04 - val_loss: 1.0485\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4023e-04 - val_loss: 1.0537\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3530e-04 - val_loss: 1.0591\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3221e-04 - val_loss: 1.0637\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2790e-04 - val_loss: 1.0693\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2497e-04 - val_loss: 1.0740\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2096e-04 - val_loss: 1.0788\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1588e-04 - val_loss: 1.0857\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1298e-04 - val_loss: 1.0913\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0953e-04 - val_loss: 1.0958\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0699e-04 - val_loss: 1.1011\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0260e-04 - val_loss: 1.1046\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9713e-05 - val_loss: 1.1096\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.7324e-05 - val_loss: 1.1147\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.4124e-05 - val_loss: 1.1198\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1752e-05 - val_loss: 1.1247\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.8293e-05 - val_loss: 1.1293\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6091e-05 - val_loss: 1.1348\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.2956e-05 - val_loss: 1.1399\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9618e-05 - val_loss: 1.1456\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7732e-05 - val_loss: 1.1499\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.4895e-05 - val_loss: 1.1556\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3646e-05 - val_loss: 1.1605\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1272e-05 - val_loss: 1.1647\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9176e-05 - val_loss: 1.1705\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7362e-05 - val_loss: 1.1740\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4101e-05 - val_loss: 1.1804\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2815e-05 - val_loss: 1.1844\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1177e-05 - val_loss: 1.1906\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.9220e-05 - val_loss: 1.1954\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 5.7780e-05 - val_loss: 1.1999\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5568e-05 - val_loss: 1.2048\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4381e-05 - val_loss: 1.2101\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1897e-05 - val_loss: 1.2153\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0080e-05 - val_loss: 1.2216\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8891e-05 - val_loss: 1.2252\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7246e-05 - val_loss: 1.2292\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4857e-05 - val_loss: 1.2343\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4023e-05 - val_loss: 1.2386\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3045e-05 - val_loss: 1.2445\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1504e-05 - val_loss: 1.2490\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0283e-05 - val_loss: 1.2534\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8735e-05 - val_loss: 1.2586\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7715e-05 - val_loss: 1.2643\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6087e-05 - val_loss: 1.2683\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5509e-05 - val_loss: 1.2728\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3362e-05 - val_loss: 1.2781\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2918e-05 - val_loss: 1.2835\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1779e-05 - val_loss: 1.2887\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1139e-05 - val_loss: 1.2933\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9708e-05 - val_loss: 1.2982\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8216e-05 - val_loss: 1.3030\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8093e-05 - val_loss: 1.3079\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7118e-05 - val_loss: 1.3116\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5703e-05 - val_loss: 1.3184\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4749e-05 - val_loss: 1.3225\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4622e-05 - val_loss: 1.3277\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3721e-05 - val_loss: 1.3316\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3046e-05 - val_loss: 1.3366\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1730e-05 - val_loss: 1.3422\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1490e-05 - val_loss: 1.3458\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0796e-05 - val_loss: 1.3527\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0119e-05 - val_loss: 1.3565\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9396e-05 - val_loss: 1.3621\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8503e-05 - val_loss: 1.3654\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8504e-05 - val_loss: 1.3718\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7861e-05 - val_loss: 1.3749\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7001e-05 - val_loss: 1.3800\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6487e-05 - val_loss: 1.3846\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5798e-05 - val_loss: 1.3892\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5117e-05 - val_loss: 1.3944\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4688e-05 - val_loss: 1.3987\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4206e-05 - val_loss: 1.4035\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3903e-05 - val_loss: 1.4074\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3355e-05 - val_loss: 1.4126\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3095e-05 - val_loss: 1.4176\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2432e-05 - val_loss: 1.4224\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2285e-05 - val_loss: 1.4266\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1715e-05 - val_loss: 1.4307\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1506e-05 - val_loss: 1.4357\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0929e-05 - val_loss: 1.4401\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0739e-05 - val_loss: 1.4442\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0207e-05 - val_loss: 1.4486\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0142e-05 - val_loss: 1.4540\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.6492e-06 - val_loss: 1.4575\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.5330e-06 - val_loss: 1.4620\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1843e-06 - val_loss: 1.4661\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.7783e-06 - val_loss: 1.4700\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6632e-06 - val_loss: 1.4760\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.3754e-06 - val_loss: 1.4798\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9118e-06 - val_loss: 1.4846\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7554e-06 - val_loss: 1.4896\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5725e-06 - val_loss: 1.4947\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.3900e-06 - val_loss: 1.4989\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1631e-06 - val_loss: 1.5024\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9310e-06 - val_loss: 1.5075\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.6886e-06 - val_loss: 1.5120\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4522e-06 - val_loss: 1.5176\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3297e-06 - val_loss: 1.5218\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1514e-06 - val_loss: 1.5252\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.0342e-06 - val_loss: 1.5302\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7187e-06 - val_loss: 1.5338\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5710e-06 - val_loss: 1.5382\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4162e-06 - val_loss: 1.5413\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.4116e-06 - val_loss: 1.5480\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0923e-06 - val_loss: 1.5530\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.9315e-06 - val_loss: 1.5571\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7499e-06 - val_loss: 1.5603\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6671e-06 - val_loss: 1.5653\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4535e-06 - val_loss: 1.5703\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3851e-06 - val_loss: 1.5765\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.2033e-06 - val_loss: 1.5801\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0239e-06 - val_loss: 1.5838\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9648e-06 - val_loss: 1.5883\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8918e-06 - val_loss: 1.5933\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7936e-06 - val_loss: 1.5978\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6266e-06 - val_loss: 1.6018\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5726e-06 - val_loss: 1.6060\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4503e-06 - val_loss: 1.6107\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3754e-06 - val_loss: 1.6145\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2065e-06 - val_loss: 1.6182\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1916e-06 - val_loss: 1.6220\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1033e-06 - val_loss: 1.6279\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9918e-06 - val_loss: 1.6325\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8615e-06 - val_loss: 1.6358\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8191e-06 - val_loss: 1.6420\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7541e-06 - val_loss: 1.6464\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.6161e-06 - val_loss: 1.6495\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.5987e-06 - val_loss: 1.6538\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4882e-06 - val_loss: 1.6580\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.4587e-06 - val_loss: 1.6629\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3848e-06 - val_loss: 1.6677\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3364e-06 - val_loss: 1.6721\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.2469e-06 - val_loss: 1.6768\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1403e-06 - val_loss: 1.6814\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1375e-06 - val_loss: 1.6852\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0568e-06 - val_loss: 1.6894\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0020e-06 - val_loss: 1.6936\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9579e-06 - val_loss: 1.6987\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8911e-06 - val_loss: 1.7014\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8373e-06 - val_loss: 1.7061\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8032e-06 - val_loss: 1.7115\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.7307e-06 - val_loss: 1.7155\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6982e-06 - val_loss: 1.7198\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6105e-06 - val_loss: 1.7243\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6260e-06 - val_loss: 1.7278\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5687e-06 - val_loss: 1.7326\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4923e-06 - val_loss: 1.7372\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4923e-06 - val_loss: 1.7418\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4239e-06 - val_loss: 1.7458\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.4040e-06 - val_loss: 1.7495\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3448e-06 - val_loss: 1.7537\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3464e-06 - val_loss: 1.7579\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3191e-06 - val_loss: 1.7617\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2752e-06 - val_loss: 1.7667\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2387e-06 - val_loss: 1.7711\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1765e-06 - val_loss: 1.7752\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1788e-06 - val_loss: 1.7787\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1454e-06 - val_loss: 1.7839\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1111e-06 - val_loss: 1.7883\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0540e-06 - val_loss: 1.7915\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0629e-06 - val_loss: 1.7969\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0131e-06 - val_loss: 1.8008\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0007e-06 - val_loss: 1.8052\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.7663e-07 - val_loss: 1.8089\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.4489e-07 - val_loss: 1.8137\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.3431e-07 - val_loss: 1.8175\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.8456e-07 - val_loss: 1.8216\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6615e-07 - val_loss: 1.8256\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.4512e-07 - val_loss: 1.8298\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.3369e-07 - val_loss: 1.8341\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9216e-07 - val_loss: 1.8383\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7844e-07 - val_loss: 1.8431\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7917e-07 - val_loss: 1.8463\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.6186e-07 - val_loss: 1.8505\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7369e-07 - val_loss: 1.8544\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.2347e-07 - val_loss: 1.8596\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.1235e-07 - val_loss: 1.8621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8532\n",
            "Roc-Auc score: 0.8532979264882339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rGh6g0gojG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fea7a45d-97e3-4b4d-c2e5-1abfc43ce13e"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(200, activation='relu')(X_input_1) # 선형 projection\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(200, activation='relu')(X_input_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200)          4906200     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200)          80200       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 400)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            401         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 4,986,801\n",
            "Trainable params: 4,986,801\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 0.6770 - val_loss: 0.6535\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.6259 - val_loss: 0.5984\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.5693 - val_loss: 0.5465\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.5166 - val_loss: 0.5003\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.4697 - val_loss: 0.4601\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.4290 - val_loss: 0.4265\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3941 - val_loss: 0.3981\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3642 - val_loss: 0.3743\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3386 - val_loss: 0.3543\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3164 - val_loss: 0.3371\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2969 - val_loss: 0.3225\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2798 - val_loss: 0.3101\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2645 - val_loss: 0.2995\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2511 - val_loss: 0.2899\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2390 - val_loss: 0.2821\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2281 - val_loss: 0.2751\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2181 - val_loss: 0.2691\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2090 - val_loss: 0.2637\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2007 - val_loss: 0.2594\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1930 - val_loss: 0.2555\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1859 - val_loss: 0.2521\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1793 - val_loss: 0.2491\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1731 - val_loss: 0.2465\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1673 - val_loss: 0.2444\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1618 - val_loss: 0.2426\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1567 - val_loss: 0.2412\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1517 - val_loss: 0.2398\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1472 - val_loss: 0.2388\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1427 - val_loss: 0.2383\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1386 - val_loss: 0.2374\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1345 - val_loss: 0.2369\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1307 - val_loss: 0.2368\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1270 - val_loss: 0.2365\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1234 - val_loss: 0.2365\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1201 - val_loss: 0.2367\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1168 - val_loss: 0.2369\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1136 - val_loss: 0.2372\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1106 - val_loss: 0.2379\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1077 - val_loss: 0.2383\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1048 - val_loss: 0.2390\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1021 - val_loss: 0.2398\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0994 - val_loss: 0.2406\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0969 - val_loss: 0.2414\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0944 - val_loss: 0.2425\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0920 - val_loss: 0.2439\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0897 - val_loss: 0.2447\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0874 - val_loss: 0.2458\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0852 - val_loss: 0.2472\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0831 - val_loss: 0.2485\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0810 - val_loss: 0.2499\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0790 - val_loss: 0.2514\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0770 - val_loss: 0.2528\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0751 - val_loss: 0.2544\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0732 - val_loss: 0.2562\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0714 - val_loss: 0.2578\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0697 - val_loss: 0.2595\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0680 - val_loss: 0.2613\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0663 - val_loss: 0.2635\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0647 - val_loss: 0.2649\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0630 - val_loss: 0.2668\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0616 - val_loss: 0.2686\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0600 - val_loss: 0.2706\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0586 - val_loss: 0.2727\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0571 - val_loss: 0.2746\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0557 - val_loss: 0.2771\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0543 - val_loss: 0.2792\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0529 - val_loss: 0.2815\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0517 - val_loss: 0.2832\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0504 - val_loss: 0.2855\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0491 - val_loss: 0.2879\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0479 - val_loss: 0.2900\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0467 - val_loss: 0.2922\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0456 - val_loss: 0.2947\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0444 - val_loss: 0.2973\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0433 - val_loss: 0.2997\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0422 - val_loss: 0.3022\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0412 - val_loss: 0.3046\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0401 - val_loss: 0.3075\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0391 - val_loss: 0.3100\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0381 - val_loss: 0.3124\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0371 - val_loss: 0.3151\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0362 - val_loss: 0.3177\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0353 - val_loss: 0.3207\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0344 - val_loss: 0.3233\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0335 - val_loss: 0.3260\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0326 - val_loss: 0.3288\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0318 - val_loss: 0.3316\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0310 - val_loss: 0.3345\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0302 - val_loss: 0.3373\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0294 - val_loss: 0.3401\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0286 - val_loss: 0.3432\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.3458\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0271 - val_loss: 0.3490\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0264 - val_loss: 0.3520\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0257 - val_loss: 0.3550\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0250 - val_loss: 0.3576\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0244 - val_loss: 0.3611\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0237 - val_loss: 0.3642\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0231 - val_loss: 0.3669\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0224 - val_loss: 0.3703\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0219 - val_loss: 0.3734\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0213 - val_loss: 0.3762\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0207 - val_loss: 0.3794\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0201 - val_loss: 0.3830\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0196 - val_loss: 0.3859\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0190 - val_loss: 0.3889\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0185 - val_loss: 0.3918\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0180 - val_loss: 0.3953\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0175 - val_loss: 0.3985\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0170 - val_loss: 0.4018\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0165 - val_loss: 0.4050\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0161 - val_loss: 0.4085\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0156 - val_loss: 0.4114\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0152 - val_loss: 0.4147\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0147 - val_loss: 0.4183\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0143 - val_loss: 0.4213\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0139 - val_loss: 0.4248\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0135 - val_loss: 0.4287\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0131 - val_loss: 0.4318\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0127 - val_loss: 0.4352\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0124 - val_loss: 0.4383\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0120 - val_loss: 0.4419\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0117 - val_loss: 0.4455\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0113 - val_loss: 0.4485\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0110 - val_loss: 0.4522\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0106 - val_loss: 0.4557\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0103 - val_loss: 0.4594\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0100 - val_loss: 0.4626\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0097 - val_loss: 0.4663\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0094 - val_loss: 0.4698\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0091 - val_loss: 0.4731\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0089 - val_loss: 0.4774\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0086 - val_loss: 0.4803\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0083 - val_loss: 0.4840\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0081 - val_loss: 0.4875\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0078 - val_loss: 0.4908\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0076 - val_loss: 0.4948\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0074 - val_loss: 0.4981\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0071 - val_loss: 0.5015\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0069 - val_loss: 0.5053\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0067 - val_loss: 0.5089\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0065 - val_loss: 0.5137\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0063 - val_loss: 0.5164\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0061 - val_loss: 0.5204\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0059 - val_loss: 0.5238\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0057 - val_loss: 0.5274\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0055 - val_loss: 0.5309\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0054 - val_loss: 0.5349\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0052 - val_loss: 0.5389\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0050 - val_loss: 0.5424\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0049 - val_loss: 0.5458\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0047 - val_loss: 0.5494\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0046 - val_loss: 0.5536\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0044 - val_loss: 0.5568\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0043 - val_loss: 0.5607\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0042 - val_loss: 0.5650\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0040 - val_loss: 0.5683\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0039 - val_loss: 0.5720\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0038 - val_loss: 0.5758\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0037 - val_loss: 0.5795\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0036 - val_loss: 0.5837\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0035 - val_loss: 0.5874\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0033 - val_loss: 0.5909\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0032 - val_loss: 0.5947\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0031 - val_loss: 0.5983\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0030 - val_loss: 0.6027\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0030 - val_loss: 0.6065\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0029 - val_loss: 0.6105\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0028 - val_loss: 0.6138\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0027 - val_loss: 0.6176\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6214\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0025 - val_loss: 0.6255\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0025 - val_loss: 0.6292\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0024 - val_loss: 0.6333\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0023 - val_loss: 0.6371\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6413\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6449\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0021 - val_loss: 0.6486\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0020 - val_loss: 0.6527\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0020 - val_loss: 0.6562\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.6602\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.6645\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.6687\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0017 - val_loss: 0.6725\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0017 - val_loss: 0.6763\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6804\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6841\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6874\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6913\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6952\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.6996\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.7036\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7074\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7113\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0012 - val_loss: 0.7150\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7196\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0012 - val_loss: 0.7232\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7267\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7313\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0011 - val_loss: 0.7344\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0010 - val_loss: 0.7393\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7426\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 9.7612e-04 - val_loss: 0.7467\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.4726e-04 - val_loss: 0.7508\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 9.2176e-04 - val_loss: 0.7545\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.9360e-04 - val_loss: 0.7583\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6620e-04 - val_loss: 0.7621\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.4014e-04 - val_loss: 0.7664\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.1484e-04 - val_loss: 0.7701\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9036e-04 - val_loss: 0.7739\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.6704e-04 - val_loss: 0.7783\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.4531e-04 - val_loss: 0.7819\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.2301e-04 - val_loss: 0.7856\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.0153e-04 - val_loss: 0.7900\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.8095e-04 - val_loss: 0.7935\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.6052e-04 - val_loss: 0.7973\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.4160e-04 - val_loss: 0.8013\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.2244e-04 - val_loss: 0.8052\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 6.0435e-04 - val_loss: 0.8091\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 5.8632e-04 - val_loss: 0.8135\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 5.6954e-04 - val_loss: 0.8174\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.5293e-04 - val_loss: 0.8209\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3692e-04 - val_loss: 0.8247\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.2073e-04 - val_loss: 0.8286\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.0534e-04 - val_loss: 0.8327\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.9065e-04 - val_loss: 0.8365\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.7677e-04 - val_loss: 0.8407\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.6289e-04 - val_loss: 0.8442\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4936e-04 - val_loss: 0.8484\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.3642e-04 - val_loss: 0.8524\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.2351e-04 - val_loss: 0.8559\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.1157e-04 - val_loss: 0.8600\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.9939e-04 - val_loss: 0.8639\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.8800e-04 - val_loss: 0.8680\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 3.7668e-04 - val_loss: 0.8716\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 3.6549e-04 - val_loss: 0.8757\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5601e-04 - val_loss: 0.8798\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4515e-04 - val_loss: 0.8834\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 3.3521e-04 - val_loss: 0.8867\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2585e-04 - val_loss: 0.8911\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 3.1610e-04 - val_loss: 0.8955\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0738e-04 - val_loss: 0.8992\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9859e-04 - val_loss: 0.9028\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9030e-04 - val_loss: 0.9064\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8145e-04 - val_loss: 0.9104\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.7375e-04 - val_loss: 0.9145\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.6554e-04 - val_loss: 0.9178\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.5846e-04 - val_loss: 0.9220\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.5094e-04 - val_loss: 0.9257\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.4357e-04 - val_loss: 0.9297\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.3661e-04 - val_loss: 0.9335\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.3016e-04 - val_loss: 0.9382\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 2.2344e-04 - val_loss: 0.9411\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1737e-04 - val_loss: 0.9451\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.1102e-04 - val_loss: 0.9491\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.0534e-04 - val_loss: 0.9526\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.9940e-04 - val_loss: 0.9570\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.9376e-04 - val_loss: 0.9600\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.8850e-04 - val_loss: 0.9645\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.8301e-04 - val_loss: 0.9679\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.7809e-04 - val_loss: 0.9718\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.7319e-04 - val_loss: 0.9755\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.6836e-04 - val_loss: 0.9794\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.6379e-04 - val_loss: 0.9838\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.5911e-04 - val_loss: 0.9876\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5463e-04 - val_loss: 0.9908\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.5057e-04 - val_loss: 0.9952\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.4626e-04 - val_loss: 0.9985\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.4236e-04 - val_loss: 1.0026\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.3851e-04 - val_loss: 1.0068\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3471e-04 - val_loss: 1.0101\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.3094e-04 - val_loss: 1.0137\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.2729e-04 - val_loss: 1.0179\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.2392e-04 - val_loss: 1.0215\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.2052e-04 - val_loss: 1.0259\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1731e-04 - val_loss: 1.0290\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1408e-04 - val_loss: 1.0332\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.1099e-04 - val_loss: 1.0369\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0801e-04 - val_loss: 1.0403\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 1.0504e-04 - val_loss: 1.0445\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 1.0227e-04 - val_loss: 1.0482\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9586e-05 - val_loss: 1.0519\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.6794e-05 - val_loss: 1.0553\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 9.4178e-05 - val_loss: 1.0598\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1713e-05 - val_loss: 1.0626\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.9305e-05 - val_loss: 1.0667\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6868e-05 - val_loss: 1.0707\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.4592e-05 - val_loss: 1.0742\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.2365e-05 - val_loss: 1.0782\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 8.0179e-05 - val_loss: 1.0814\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8047e-05 - val_loss: 1.0854\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.5906e-05 - val_loss: 1.0887\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.4004e-05 - val_loss: 1.0925\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.2048e-05 - val_loss: 1.0967\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.0096e-05 - val_loss: 1.1000\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.8309e-05 - val_loss: 1.1040\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 6.6516e-05 - val_loss: 1.1071\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 6.4796e-05 - val_loss: 1.1110\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3144e-05 - val_loss: 1.1147\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1384e-05 - val_loss: 1.1189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8622\n",
            "Roc-Auc score: 0.8622938860948608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bC_vuDyjVjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2508377-21bf-4e3f-b5cb-65b4c4690a06"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(200, activation='relu')(X_input_1)\n",
        "X_dense_1 = Dropout(0.5)(X_dense_1)\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(200, activation='relu')(X_input_2)\n",
        "X_dense_2 = Dropout(0.5)(X_dense_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=300,\n",
        "                 batch_size=300,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200)          4906200     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200)          80200       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 200)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 400)          0           dropout[0][0]                    \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            401         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 4,986,801\n",
            "Trainable params: 4,986,801\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "67/67 [==============================] - 1s 19ms/step - loss: 0.6790 - val_loss: 0.6602\n",
            "Epoch 2/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.6373 - val_loss: 0.6129\n",
            "Epoch 3/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.5881 - val_loss: 0.5658\n",
            "Epoch 4/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.5404 - val_loss: 0.5227\n",
            "Epoch 5/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.4972 - val_loss: 0.4849\n",
            "Epoch 6/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.4595 - val_loss: 0.4524\n",
            "Epoch 7/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.4265 - val_loss: 0.4242\n",
            "Epoch 8/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3975 - val_loss: 0.4000\n",
            "Epoch 9/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3724 - val_loss: 0.3792\n",
            "Epoch 10/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3504 - val_loss: 0.3612\n",
            "Epoch 11/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.3306 - val_loss: 0.3456\n",
            "Epoch 12/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.3133 - val_loss: 0.3320\n",
            "Epoch 13/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2982 - val_loss: 0.3201\n",
            "Epoch 14/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2843 - val_loss: 0.3097\n",
            "Epoch 15/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2716 - val_loss: 0.3006\n",
            "Epoch 16/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2604 - val_loss: 0.2925\n",
            "Epoch 17/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2501 - val_loss: 0.2853\n",
            "Epoch 18/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2409 - val_loss: 0.2789\n",
            "Epoch 19/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2312 - val_loss: 0.2733\n",
            "Epoch 20/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2241 - val_loss: 0.2684\n",
            "Epoch 21/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.2162 - val_loss: 0.2639\n",
            "Epoch 22/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2091 - val_loss: 0.2601\n",
            "Epoch 23/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.2027 - val_loss: 0.2566\n",
            "Epoch 24/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1968 - val_loss: 0.2535\n",
            "Epoch 25/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1909 - val_loss: 0.2508\n",
            "Epoch 26/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1855 - val_loss: 0.2483\n",
            "Epoch 27/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1811 - val_loss: 0.2462\n",
            "Epoch 28/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1748 - val_loss: 0.2444\n",
            "Epoch 29/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1716 - val_loss: 0.2427\n",
            "Epoch 30/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1673 - val_loss: 0.2414\n",
            "Epoch 31/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1624 - val_loss: 0.2401\n",
            "Epoch 32/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1580 - val_loss: 0.2390\n",
            "Epoch 33/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1552 - val_loss: 0.2381\n",
            "Epoch 34/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1507 - val_loss: 0.2372\n",
            "Epoch 35/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1482 - val_loss: 0.2367\n",
            "Epoch 36/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1437 - val_loss: 0.2362\n",
            "Epoch 37/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1416 - val_loss: 0.2359\n",
            "Epoch 38/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1374 - val_loss: 0.2356\n",
            "Epoch 39/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1343 - val_loss: 0.2353\n",
            "Epoch 40/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1318 - val_loss: 0.2354\n",
            "Epoch 41/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1284 - val_loss: 0.2353\n",
            "Epoch 42/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1255 - val_loss: 0.2354\n",
            "Epoch 43/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1231 - val_loss: 0.2356\n",
            "Epoch 44/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1204 - val_loss: 0.2358\n",
            "Epoch 45/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1178 - val_loss: 0.2363\n",
            "Epoch 46/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1158 - val_loss: 0.2367\n",
            "Epoch 47/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1134 - val_loss: 0.2372\n",
            "Epoch 48/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1099 - val_loss: 0.2376\n",
            "Epoch 49/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.1081 - val_loss: 0.2383\n",
            "Epoch 50/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1063 - val_loss: 0.2389\n",
            "Epoch 51/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1039 - val_loss: 0.2398\n",
            "Epoch 52/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1021 - val_loss: 0.2404\n",
            "Epoch 53/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.1005 - val_loss: 0.2413\n",
            "Epoch 54/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0979 - val_loss: 0.2422\n",
            "Epoch 55/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0959 - val_loss: 0.2430\n",
            "Epoch 56/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0939 - val_loss: 0.2439\n",
            "Epoch 57/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0917 - val_loss: 0.2450\n",
            "Epoch 58/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0905 - val_loss: 0.2460\n",
            "Epoch 59/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0875 - val_loss: 0.2470\n",
            "Epoch 60/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0866 - val_loss: 0.2481\n",
            "Epoch 61/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0853 - val_loss: 0.2492\n",
            "Epoch 62/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0830 - val_loss: 0.2504\n",
            "Epoch 63/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0811 - val_loss: 0.2516\n",
            "Epoch 64/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0792 - val_loss: 0.2532\n",
            "Epoch 65/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0779 - val_loss: 0.2543\n",
            "Epoch 66/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0766 - val_loss: 0.2556\n",
            "Epoch 67/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0747 - val_loss: 0.2569\n",
            "Epoch 68/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0734 - val_loss: 0.2584\n",
            "Epoch 69/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0722 - val_loss: 0.2598\n",
            "Epoch 70/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0711 - val_loss: 0.2614\n",
            "Epoch 71/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0693 - val_loss: 0.2627\n",
            "Epoch 72/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0681 - val_loss: 0.2643\n",
            "Epoch 73/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0667 - val_loss: 0.2656\n",
            "Epoch 74/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0646 - val_loss: 0.2671\n",
            "Epoch 75/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0641 - val_loss: 0.2690\n",
            "Epoch 76/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0627 - val_loss: 0.2705\n",
            "Epoch 77/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0613 - val_loss: 0.2722\n",
            "Epoch 78/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0592 - val_loss: 0.2739\n",
            "Epoch 79/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0594 - val_loss: 0.2755\n",
            "Epoch 80/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0578 - val_loss: 0.2774\n",
            "Epoch 81/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0561 - val_loss: 0.2790\n",
            "Epoch 82/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0552 - val_loss: 0.2807\n",
            "Epoch 83/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0544 - val_loss: 0.2827\n",
            "Epoch 84/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0534 - val_loss: 0.2843\n",
            "Epoch 85/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0515 - val_loss: 0.2864\n",
            "Epoch 86/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0516 - val_loss: 0.2882\n",
            "Epoch 87/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0495 - val_loss: 0.2901\n",
            "Epoch 88/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0486 - val_loss: 0.2918\n",
            "Epoch 89/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0473 - val_loss: 0.2940\n",
            "Epoch 90/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0467 - val_loss: 0.2958\n",
            "Epoch 91/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0453 - val_loss: 0.2976\n",
            "Epoch 92/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0450 - val_loss: 0.2998\n",
            "Epoch 93/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0441 - val_loss: 0.3016\n",
            "Epoch 94/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0425 - val_loss: 0.3038\n",
            "Epoch 95/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0417 - val_loss: 0.3061\n",
            "Epoch 96/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0410 - val_loss: 0.3078\n",
            "Epoch 97/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0400 - val_loss: 0.3100\n",
            "Epoch 98/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0394 - val_loss: 0.3120\n",
            "Epoch 99/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0385 - val_loss: 0.3142\n",
            "Epoch 100/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0376 - val_loss: 0.3164\n",
            "Epoch 101/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0369 - val_loss: 0.3189\n",
            "Epoch 102/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0361 - val_loss: 0.3207\n",
            "Epoch 103/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0353 - val_loss: 0.3230\n",
            "Epoch 104/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0343 - val_loss: 0.3252\n",
            "Epoch 105/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0341 - val_loss: 0.3277\n",
            "Epoch 106/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0329 - val_loss: 0.3297\n",
            "Epoch 107/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0321 - val_loss: 0.3319\n",
            "Epoch 108/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.3343\n",
            "Epoch 109/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0308 - val_loss: 0.3367\n",
            "Epoch 110/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0302 - val_loss: 0.3387\n",
            "Epoch 111/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0294 - val_loss: 0.3415\n",
            "Epoch 112/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0287 - val_loss: 0.3437\n",
            "Epoch 113/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0278 - val_loss: 0.3460\n",
            "Epoch 114/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0272 - val_loss: 0.3485\n",
            "Epoch 115/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0267 - val_loss: 0.3510\n",
            "Epoch 116/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0261 - val_loss: 0.3532\n",
            "Epoch 117/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0256 - val_loss: 0.3559\n",
            "Epoch 118/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0252 - val_loss: 0.3581\n",
            "Epoch 119/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0247 - val_loss: 0.3602\n",
            "Epoch 120/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0240 - val_loss: 0.3630\n",
            "Epoch 121/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0230 - val_loss: 0.3655\n",
            "Epoch 122/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0229 - val_loss: 0.3680\n",
            "Epoch 123/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0223 - val_loss: 0.3707\n",
            "Epoch 124/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0218 - val_loss: 0.3731\n",
            "Epoch 125/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0212 - val_loss: 0.3758\n",
            "Epoch 126/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0209 - val_loss: 0.3781\n",
            "Epoch 127/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0201 - val_loss: 0.3807\n",
            "Epoch 128/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0198 - val_loss: 0.3835\n",
            "Epoch 129/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0192 - val_loss: 0.3859\n",
            "Epoch 130/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0187 - val_loss: 0.3888\n",
            "Epoch 131/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0186 - val_loss: 0.3910\n",
            "Epoch 132/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0183 - val_loss: 0.3937\n",
            "Epoch 133/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0177 - val_loss: 0.3961\n",
            "Epoch 134/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0172 - val_loss: 0.3989\n",
            "Epoch 135/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0168 - val_loss: 0.4016\n",
            "Epoch 136/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0164 - val_loss: 0.4041\n",
            "Epoch 137/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0161 - val_loss: 0.4072\n",
            "Epoch 138/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0155 - val_loss: 0.4101\n",
            "Epoch 139/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0152 - val_loss: 0.4126\n",
            "Epoch 140/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0149 - val_loss: 0.4149\n",
            "Epoch 141/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0145 - val_loss: 0.4177\n",
            "Epoch 142/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0143 - val_loss: 0.4207\n",
            "Epoch 143/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0138 - val_loss: 0.4229\n",
            "Epoch 144/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0136 - val_loss: 0.4256\n",
            "Epoch 145/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0129 - val_loss: 0.4286\n",
            "Epoch 146/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0127 - val_loss: 0.4314\n",
            "Epoch 147/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0124 - val_loss: 0.4345\n",
            "Epoch 148/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0121 - val_loss: 0.4370\n",
            "Epoch 149/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0118 - val_loss: 0.4399\n",
            "Epoch 150/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0113 - val_loss: 0.4426\n",
            "Epoch 151/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0111 - val_loss: 0.4458\n",
            "Epoch 152/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0108 - val_loss: 0.4482\n",
            "Epoch 153/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0107 - val_loss: 0.4514\n",
            "Epoch 154/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0105 - val_loss: 0.4540\n",
            "Epoch 155/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0101 - val_loss: 0.4566\n",
            "Epoch 156/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0099 - val_loss: 0.4599\n",
            "Epoch 157/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0095 - val_loss: 0.4626\n",
            "Epoch 158/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0094 - val_loss: 0.4660\n",
            "Epoch 159/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0091 - val_loss: 0.4692\n",
            "Epoch 160/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0089 - val_loss: 0.4717\n",
            "Epoch 161/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0087 - val_loss: 0.4752\n",
            "Epoch 162/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0084 - val_loss: 0.4777\n",
            "Epoch 163/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0083 - val_loss: 0.4812\n",
            "Epoch 164/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0081 - val_loss: 0.4841\n",
            "Epoch 165/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0079 - val_loss: 0.4869\n",
            "Epoch 166/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0075 - val_loss: 0.4896\n",
            "Epoch 167/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0075 - val_loss: 0.4927\n",
            "Epoch 168/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0072 - val_loss: 0.4956\n",
            "Epoch 169/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0069 - val_loss: 0.4985\n",
            "Epoch 170/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0068 - val_loss: 0.5014\n",
            "Epoch 171/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0068 - val_loss: 0.5048\n",
            "Epoch 172/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0065 - val_loss: 0.5081\n",
            "Epoch 173/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0063 - val_loss: 0.5108\n",
            "Epoch 174/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0062 - val_loss: 0.5138\n",
            "Epoch 175/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0059 - val_loss: 0.5170\n",
            "Epoch 176/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0060 - val_loss: 0.5203\n",
            "Epoch 177/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0055 - val_loss: 0.5229\n",
            "Epoch 178/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0054 - val_loss: 0.5261\n",
            "Epoch 179/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0054 - val_loss: 0.5294\n",
            "Epoch 180/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0052 - val_loss: 0.5324\n",
            "Epoch 181/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0051 - val_loss: 0.5356\n",
            "Epoch 182/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0048 - val_loss: 0.5394\n",
            "Epoch 183/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0048 - val_loss: 0.5420\n",
            "Epoch 184/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0047 - val_loss: 0.5449\n",
            "Epoch 185/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0046 - val_loss: 0.5482\n",
            "Epoch 186/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0044 - val_loss: 0.5516\n",
            "Epoch 187/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0043 - val_loss: 0.5551\n",
            "Epoch 188/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0042 - val_loss: 0.5582\n",
            "Epoch 189/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0041 - val_loss: 0.5614\n",
            "Epoch 190/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0041 - val_loss: 0.5651\n",
            "Epoch 191/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0040 - val_loss: 0.5677\n",
            "Epoch 192/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0038 - val_loss: 0.5708\n",
            "Epoch 193/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0036 - val_loss: 0.5750\n",
            "Epoch 194/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0035 - val_loss: 0.5777\n",
            "Epoch 195/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0035 - val_loss: 0.5807\n",
            "Epoch 196/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0034 - val_loss: 0.5838\n",
            "Epoch 197/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0033 - val_loss: 0.5872\n",
            "Epoch 198/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0033 - val_loss: 0.5905\n",
            "Epoch 199/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0031 - val_loss: 0.5940\n",
            "Epoch 200/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0031 - val_loss: 0.5968\n",
            "Epoch 201/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0030 - val_loss: 0.5998\n",
            "Epoch 202/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0028 - val_loss: 0.6038\n",
            "Epoch 203/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0028 - val_loss: 0.6067\n",
            "Epoch 204/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0027 - val_loss: 0.6096\n",
            "Epoch 205/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6131\n",
            "Epoch 206/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0026 - val_loss: 0.6166\n",
            "Epoch 207/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0025 - val_loss: 0.6198\n",
            "Epoch 208/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0025 - val_loss: 0.6233\n",
            "Epoch 209/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0024 - val_loss: 0.6266\n",
            "Epoch 210/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0023 - val_loss: 0.6296\n",
            "Epoch 211/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0024 - val_loss: 0.6327\n",
            "Epoch 212/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6359\n",
            "Epoch 213/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6392\n",
            "Epoch 214/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0022 - val_loss: 0.6427\n",
            "Epoch 215/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0021 - val_loss: 0.6463\n",
            "Epoch 216/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0021 - val_loss: 0.6496\n",
            "Epoch 217/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0019 - val_loss: 0.6526\n",
            "Epoch 218/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0020 - val_loss: 0.6553\n",
            "Epoch 219/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0019 - val_loss: 0.6582\n",
            "Epoch 220/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.6616\n",
            "Epoch 221/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.6646\n",
            "Epoch 222/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0018 - val_loss: 0.6682\n",
            "Epoch 223/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.6713\n",
            "Epoch 224/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.6749\n",
            "Epoch 225/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6778\n",
            "Epoch 226/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6812\n",
            "Epoch 227/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0016 - val_loss: 0.6848\n",
            "Epoch 228/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6877\n",
            "Epoch 229/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.6914\n",
            "Epoch 230/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0015 - val_loss: 0.6951\n",
            "Epoch 231/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.6974\n",
            "Epoch 232/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.7009\n",
            "Epoch 233/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0013 - val_loss: 0.7043\n",
            "Epoch 234/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7072\n",
            "Epoch 235/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0013 - val_loss: 0.7107\n",
            "Epoch 236/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7140\n",
            "Epoch 237/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0012 - val_loss: 0.7173\n",
            "Epoch 238/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7209\n",
            "Epoch 239/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7243\n",
            "Epoch 240/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7277\n",
            "Epoch 241/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0011 - val_loss: 0.7309\n",
            "Epoch 242/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7340\n",
            "Epoch 243/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0010 - val_loss: 0.7376\n",
            "Epoch 244/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0010 - val_loss: 0.7409\n",
            "Epoch 245/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.9483e-04 - val_loss: 0.7437\n",
            "Epoch 246/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.3739e-04 - val_loss: 0.7465\n",
            "Epoch 247/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.6511e-04 - val_loss: 0.7504\n",
            "Epoch 248/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.0807e-04 - val_loss: 0.7535\n",
            "Epoch 249/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 9.1660e-04 - val_loss: 0.7569\n",
            "Epoch 250/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.9956e-04 - val_loss: 0.7598\n",
            "Epoch 251/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.6167e-04 - val_loss: 0.7632\n",
            "Epoch 252/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 8.4883e-04 - val_loss: 0.7665\n",
            "Epoch 253/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.9870e-04 - val_loss: 0.7701\n",
            "Epoch 254/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 7.7590e-04 - val_loss: 0.7731\n",
            "Epoch 255/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.8838e-04 - val_loss: 0.7761\n",
            "Epoch 256/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.7797e-04 - val_loss: 0.7796\n",
            "Epoch 257/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.5412e-04 - val_loss: 0.7822\n",
            "Epoch 258/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.4638e-04 - val_loss: 0.7869\n",
            "Epoch 259/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 7.2352e-04 - val_loss: 0.7899\n",
            "Epoch 260/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.9663e-04 - val_loss: 0.7937\n",
            "Epoch 261/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.7004e-04 - val_loss: 0.7959\n",
            "Epoch 262/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 6.3167e-04 - val_loss: 0.7995\n",
            "Epoch 263/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.3008e-04 - val_loss: 0.8032\n",
            "Epoch 264/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 6.1819e-04 - val_loss: 0.8059\n",
            "Epoch 265/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 6.1262e-04 - val_loss: 0.8096\n",
            "Epoch 266/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.8168e-04 - val_loss: 0.8125\n",
            "Epoch 267/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 5.7390e-04 - val_loss: 0.8167\n",
            "Epoch 268/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.7636e-04 - val_loss: 0.8190\n",
            "Epoch 269/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.6889e-04 - val_loss: 0.8231\n",
            "Epoch 270/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.2744e-04 - val_loss: 0.8262\n",
            "Epoch 271/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.3893e-04 - val_loss: 0.8293\n",
            "Epoch 272/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1098e-04 - val_loss: 0.8329\n",
            "Epoch 273/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 5.1735e-04 - val_loss: 0.8359\n",
            "Epoch 274/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.8747e-04 - val_loss: 0.8396\n",
            "Epoch 275/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7158e-04 - val_loss: 0.8429\n",
            "Epoch 276/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.7929e-04 - val_loss: 0.8453\n",
            "Epoch 277/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.6940e-04 - val_loss: 0.8491\n",
            "Epoch 278/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4935e-04 - val_loss: 0.8521\n",
            "Epoch 279/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.4213e-04 - val_loss: 0.8549\n",
            "Epoch 280/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.5274e-04 - val_loss: 0.8579\n",
            "Epoch 281/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.3563e-04 - val_loss: 0.8618\n",
            "Epoch 282/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.1482e-04 - val_loss: 0.8652\n",
            "Epoch 283/300\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 4.1104e-04 - val_loss: 0.8678\n",
            "Epoch 284/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7969e-04 - val_loss: 0.8727\n",
            "Epoch 285/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 4.0269e-04 - val_loss: 0.8756\n",
            "Epoch 286/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.7896e-04 - val_loss: 0.8790\n",
            "Epoch 287/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6945e-04 - val_loss: 0.8823\n",
            "Epoch 288/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.6686e-04 - val_loss: 0.8858\n",
            "Epoch 289/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.5721e-04 - val_loss: 0.8886\n",
            "Epoch 290/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3440e-04 - val_loss: 0.8922\n",
            "Epoch 291/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.4152e-04 - val_loss: 0.8957\n",
            "Epoch 292/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.3894e-04 - val_loss: 0.8976\n",
            "Epoch 293/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1718e-04 - val_loss: 0.9019\n",
            "Epoch 294/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.2558e-04 - val_loss: 0.9051\n",
            "Epoch 295/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0986e-04 - val_loss: 0.9075\n",
            "Epoch 296/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.0517e-04 - val_loss: 0.9103\n",
            "Epoch 297/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.8858e-04 - val_loss: 0.9154\n",
            "Epoch 298/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.9134e-04 - val_loss: 0.9178\n",
            "Epoch 299/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 3.1558e-04 - val_loss: 0.9208\n",
            "Epoch 300/300\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 2.7033e-04 - val_loss: 0.9244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8646\n",
            "Roc-Auc score: 0.8646909845512676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odgk6aGgjg5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0143f676-7e44-44b3-e8db-fb83c9575f87"
      },
      "source": [
        "# 학습 : early stopping 없이\n",
        "K.clear_session()\n",
        "\n",
        "# FFN 네트워크 설정\n",
        "X_input_1 = Input(batch_shape=(None, tfidf_vec.shape[1])) # TFIDF 입력\n",
        "X_dense_1 = Dense(200, activation='relu')(X_input_1)\n",
        "X_dense_1 = Dropout(0.5)(X_dense_1)\n",
        "X_input_2 = Input(batch_shape=(None, doc2vec_vec.shape[1])) # Doc2Vec 입력\n",
        "X_dense_2 = Dense(200, activation='relu')(X_input_2)\n",
        "X_dense_2 = Dropout(0.5)(X_dense_2)\n",
        "X_concat = Concatenate()([X_dense_1, X_dense_2])\n",
        "y_output = Dense(1, activation='sigmoid')(X_concat)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model([X_input_1, X_input_2], y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001))\n",
        "print(\"====== 전체 모델 구조 확인 ======\")\n",
        "print(model.summary())\n",
        "\n",
        "# 모델 학습\n",
        "hist = model.fit([X_train_tf, X_train_doc], y_train,\n",
        "                 epochs=500,\n",
        "                 batch_size=512,\n",
        "                 validation_data=([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# loss 시각화\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss history\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "# 예측 및 결과 확인\n",
        "y_pred = model.predict([X_test_tf, X_test_doc])\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "rocauc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== 전체 모델 구조 확인 ======\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24530)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200)          4906200     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200)          80200       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 200)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 400)          0           dropout[0][0]                    \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            401         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 4,986,801\n",
            "Trainable params: 4,986,801\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "40/40 [==============================] - 1s 30ms/step - loss: 0.6844 - val_loss: 0.6729\n",
            "Epoch 2/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.6594 - val_loss: 0.6436\n",
            "Epoch 3/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.6277 - val_loss: 0.6120\n",
            "Epoch 4/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5952 - val_loss: 0.5815\n",
            "Epoch 5/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5645 - val_loss: 0.5531\n",
            "Epoch 6/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5355 - val_loss: 0.5264\n",
            "Epoch 7/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5075 - val_loss: 0.5017\n",
            "Epoch 8/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4823 - val_loss: 0.4791\n",
            "Epoch 9/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4585 - val_loss: 0.4582\n",
            "Epoch 10/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4364 - val_loss: 0.4394\n",
            "Epoch 11/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4168 - val_loss: 0.4222\n",
            "Epoch 12/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3980 - val_loss: 0.4065\n",
            "Epoch 13/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3808 - val_loss: 0.3923\n",
            "Epoch 14/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3659 - val_loss: 0.3793\n",
            "Epoch 15/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3515 - val_loss: 0.3674\n",
            "Epoch 16/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3386 - val_loss: 0.3567\n",
            "Epoch 17/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3259 - val_loss: 0.3468\n",
            "Epoch 18/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3144 - val_loss: 0.3378\n",
            "Epoch 19/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3043 - val_loss: 0.3294\n",
            "Epoch 20/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2950 - val_loss: 0.3218\n",
            "Epoch 21/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2858 - val_loss: 0.3148\n",
            "Epoch 22/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2763 - val_loss: 0.3083\n",
            "Epoch 23/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2679 - val_loss: 0.3022\n",
            "Epoch 24/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2607 - val_loss: 0.2968\n",
            "Epoch 25/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2531 - val_loss: 0.2917\n",
            "Epoch 26/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2465 - val_loss: 0.2871\n",
            "Epoch 27/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2407 - val_loss: 0.2828\n",
            "Epoch 28/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2344 - val_loss: 0.2787\n",
            "Epoch 29/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2292 - val_loss: 0.2750\n",
            "Epoch 30/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2237 - val_loss: 0.2715\n",
            "Epoch 31/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2186 - val_loss: 0.2683\n",
            "Epoch 32/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2135 - val_loss: 0.2654\n",
            "Epoch 33/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2088 - val_loss: 0.2626\n",
            "Epoch 34/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2038 - val_loss: 0.2601\n",
            "Epoch 35/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2003 - val_loss: 0.2578\n",
            "Epoch 36/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1954 - val_loss: 0.2557\n",
            "Epoch 37/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1925 - val_loss: 0.2536\n",
            "Epoch 38/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1887 - val_loss: 0.2518\n",
            "Epoch 39/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1843 - val_loss: 0.2500\n",
            "Epoch 40/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1809 - val_loss: 0.2484\n",
            "Epoch 41/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1784 - val_loss: 0.2470\n",
            "Epoch 42/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1751 - val_loss: 0.2457\n",
            "Epoch 43/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1724 - val_loss: 0.2444\n",
            "Epoch 44/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1687 - val_loss: 0.2433\n",
            "Epoch 45/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1654 - val_loss: 0.2424\n",
            "Epoch 46/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1626 - val_loss: 0.2414\n",
            "Epoch 47/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1588 - val_loss: 0.2406\n",
            "Epoch 48/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1574 - val_loss: 0.2399\n",
            "Epoch 49/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1552 - val_loss: 0.2391\n",
            "Epoch 50/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1521 - val_loss: 0.2385\n",
            "Epoch 51/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1503 - val_loss: 0.2379\n",
            "Epoch 52/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1469 - val_loss: 0.2374\n",
            "Epoch 53/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1446 - val_loss: 0.2370\n",
            "Epoch 54/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1428 - val_loss: 0.2366\n",
            "Epoch 55/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1402 - val_loss: 0.2362\n",
            "Epoch 56/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1381 - val_loss: 0.2360\n",
            "Epoch 57/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1357 - val_loss: 0.2359\n",
            "Epoch 58/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1342 - val_loss: 0.2356\n",
            "Epoch 59/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1323 - val_loss: 0.2355\n",
            "Epoch 60/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1297 - val_loss: 0.2354\n",
            "Epoch 61/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1282 - val_loss: 0.2354\n",
            "Epoch 62/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1262 - val_loss: 0.2354\n",
            "Epoch 63/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1241 - val_loss: 0.2354\n",
            "Epoch 64/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1217 - val_loss: 0.2355\n",
            "Epoch 65/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1207 - val_loss: 0.2356\n",
            "Epoch 66/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1185 - val_loss: 0.2358\n",
            "Epoch 67/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1172 - val_loss: 0.2360\n",
            "Epoch 68/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1155 - val_loss: 0.2362\n",
            "Epoch 69/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1139 - val_loss: 0.2363\n",
            "Epoch 70/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1124 - val_loss: 0.2366\n",
            "Epoch 71/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1098 - val_loss: 0.2367\n",
            "Epoch 72/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1089 - val_loss: 0.2370\n",
            "Epoch 73/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1073 - val_loss: 0.2373\n",
            "Epoch 74/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1056 - val_loss: 0.2378\n",
            "Epoch 75/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1039 - val_loss: 0.2383\n",
            "Epoch 76/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1035 - val_loss: 0.2387\n",
            "Epoch 77/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1027 - val_loss: 0.2391\n",
            "Epoch 78/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1000 - val_loss: 0.2396\n",
            "Epoch 79/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0993 - val_loss: 0.2401\n",
            "Epoch 80/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0975 - val_loss: 0.2406\n",
            "Epoch 81/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0963 - val_loss: 0.2411\n",
            "Epoch 82/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0945 - val_loss: 0.2418\n",
            "Epoch 83/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0935 - val_loss: 0.2425\n",
            "Epoch 84/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0928 - val_loss: 0.2429\n",
            "Epoch 85/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0913 - val_loss: 0.2436\n",
            "Epoch 86/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0894 - val_loss: 0.2440\n",
            "Epoch 87/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0881 - val_loss: 0.2447\n",
            "Epoch 88/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0869 - val_loss: 0.2453\n",
            "Epoch 89/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0866 - val_loss: 0.2460\n",
            "Epoch 90/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0855 - val_loss: 0.2466\n",
            "Epoch 91/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0835 - val_loss: 0.2474\n",
            "Epoch 92/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0827 - val_loss: 0.2480\n",
            "Epoch 93/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0816 - val_loss: 0.2488\n",
            "Epoch 94/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0805 - val_loss: 0.2497\n",
            "Epoch 95/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0797 - val_loss: 0.2505\n",
            "Epoch 96/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0776 - val_loss: 0.2513\n",
            "Epoch 97/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0769 - val_loss: 0.2521\n",
            "Epoch 98/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0765 - val_loss: 0.2531\n",
            "Epoch 99/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0751 - val_loss: 0.2539\n",
            "Epoch 100/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0738 - val_loss: 0.2549\n",
            "Epoch 101/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0736 - val_loss: 0.2556\n",
            "Epoch 102/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0720 - val_loss: 0.2565\n",
            "Epoch 103/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0712 - val_loss: 0.2575\n",
            "Epoch 104/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0705 - val_loss: 0.2584\n",
            "Epoch 105/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0692 - val_loss: 0.2594\n",
            "Epoch 106/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0685 - val_loss: 0.2603\n",
            "Epoch 107/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0679 - val_loss: 0.2612\n",
            "Epoch 108/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0662 - val_loss: 0.2621\n",
            "Epoch 109/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0653 - val_loss: 0.2632\n",
            "Epoch 110/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0649 - val_loss: 0.2642\n",
            "Epoch 111/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0637 - val_loss: 0.2650\n",
            "Epoch 112/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0627 - val_loss: 0.2661\n",
            "Epoch 113/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0622 - val_loss: 0.2673\n",
            "Epoch 114/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0613 - val_loss: 0.2682\n",
            "Epoch 115/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0606 - val_loss: 0.2694\n",
            "Epoch 116/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0600 - val_loss: 0.2703\n",
            "Epoch 117/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0587 - val_loss: 0.2715\n",
            "Epoch 118/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0581 - val_loss: 0.2726\n",
            "Epoch 119/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0573 - val_loss: 0.2737\n",
            "Epoch 120/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0564 - val_loss: 0.2747\n",
            "Epoch 121/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0556 - val_loss: 0.2759\n",
            "Epoch 122/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0555 - val_loss: 0.2769\n",
            "Epoch 123/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0543 - val_loss: 0.2782\n",
            "Epoch 124/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0532 - val_loss: 0.2791\n",
            "Epoch 125/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0527 - val_loss: 0.2803\n",
            "Epoch 126/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0525 - val_loss: 0.2815\n",
            "Epoch 127/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0509 - val_loss: 0.2826\n",
            "Epoch 128/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0508 - val_loss: 0.2838\n",
            "Epoch 129/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0496 - val_loss: 0.2851\n",
            "Epoch 130/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0493 - val_loss: 0.2863\n",
            "Epoch 131/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0483 - val_loss: 0.2876\n",
            "Epoch 132/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0481 - val_loss: 0.2888\n",
            "Epoch 133/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0473 - val_loss: 0.2905\n",
            "Epoch 134/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0468 - val_loss: 0.2915\n",
            "Epoch 135/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0457 - val_loss: 0.2929\n",
            "Epoch 136/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0455 - val_loss: 0.2940\n",
            "Epoch 137/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0450 - val_loss: 0.2953\n",
            "Epoch 138/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0437 - val_loss: 0.2968\n",
            "Epoch 139/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0437 - val_loss: 0.2982\n",
            "Epoch 140/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0430 - val_loss: 0.2995\n",
            "Epoch 141/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0419 - val_loss: 0.3008\n",
            "Epoch 142/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0416 - val_loss: 0.3021\n",
            "Epoch 143/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0411 - val_loss: 0.3034\n",
            "Epoch 144/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0402 - val_loss: 0.3046\n",
            "Epoch 145/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0397 - val_loss: 0.3058\n",
            "Epoch 146/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0401 - val_loss: 0.3070\n",
            "Epoch 147/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0388 - val_loss: 0.3083\n",
            "Epoch 148/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0379 - val_loss: 0.3102\n",
            "Epoch 149/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0378 - val_loss: 0.3114\n",
            "Epoch 150/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0368 - val_loss: 0.3125\n",
            "Epoch 151/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0364 - val_loss: 0.3140\n",
            "Epoch 152/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0361 - val_loss: 0.3156\n",
            "Epoch 153/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0357 - val_loss: 0.3170\n",
            "Epoch 154/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0355 - val_loss: 0.3184\n",
            "Epoch 155/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0344 - val_loss: 0.3198\n",
            "Epoch 156/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0338 - val_loss: 0.3213\n",
            "Epoch 157/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0334 - val_loss: 0.3228\n",
            "Epoch 158/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0331 - val_loss: 0.3242\n",
            "Epoch 159/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0329 - val_loss: 0.3256\n",
            "Epoch 160/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0320 - val_loss: 0.3270\n",
            "Epoch 161/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0317 - val_loss: 0.3283\n",
            "Epoch 162/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0313 - val_loss: 0.3300\n",
            "Epoch 163/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0307 - val_loss: 0.3316\n",
            "Epoch 164/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0305 - val_loss: 0.3331\n",
            "Epoch 165/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0302 - val_loss: 0.3343\n",
            "Epoch 166/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0296 - val_loss: 0.3359\n",
            "Epoch 167/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0292 - val_loss: 0.3371\n",
            "Epoch 168/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0287 - val_loss: 0.3388\n",
            "Epoch 169/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0285 - val_loss: 0.3403\n",
            "Epoch 170/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0279 - val_loss: 0.3417\n",
            "Epoch 171/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0274 - val_loss: 0.3432\n",
            "Epoch 172/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0271 - val_loss: 0.3445\n",
            "Epoch 173/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0263 - val_loss: 0.3460\n",
            "Epoch 174/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0260 - val_loss: 0.3475\n",
            "Epoch 175/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0255 - val_loss: 0.3491\n",
            "Epoch 176/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0258 - val_loss: 0.3508\n",
            "Epoch 177/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0249 - val_loss: 0.3525\n",
            "Epoch 178/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0248 - val_loss: 0.3539\n",
            "Epoch 179/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0246 - val_loss: 0.3555\n",
            "Epoch 180/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0238 - val_loss: 0.3574\n",
            "Epoch 181/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0235 - val_loss: 0.3587\n",
            "Epoch 182/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0233 - val_loss: 0.3603\n",
            "Epoch 183/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0231 - val_loss: 0.3620\n",
            "Epoch 184/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0227 - val_loss: 0.3636\n",
            "Epoch 185/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0224 - val_loss: 0.3648\n",
            "Epoch 186/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0221 - val_loss: 0.3664\n",
            "Epoch 187/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0216 - val_loss: 0.3681\n",
            "Epoch 188/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0213 - val_loss: 0.3699\n",
            "Epoch 189/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0212 - val_loss: 0.3715\n",
            "Epoch 190/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0209 - val_loss: 0.3730\n",
            "Epoch 191/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0202 - val_loss: 0.3748\n",
            "Epoch 192/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0199 - val_loss: 0.3766\n",
            "Epoch 193/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0198 - val_loss: 0.3782\n",
            "Epoch 194/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0198 - val_loss: 0.3799\n",
            "Epoch 195/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0192 - val_loss: 0.3816\n",
            "Epoch 196/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0190 - val_loss: 0.3833\n",
            "Epoch 197/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0184 - val_loss: 0.3845\n",
            "Epoch 198/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0180 - val_loss: 0.3862\n",
            "Epoch 199/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0181 - val_loss: 0.3884\n",
            "Epoch 200/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0177 - val_loss: 0.3898\n",
            "Epoch 201/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0173 - val_loss: 0.3917\n",
            "Epoch 202/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0172 - val_loss: 0.3935\n",
            "Epoch 203/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0170 - val_loss: 0.3949\n",
            "Epoch 204/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0165 - val_loss: 0.3966\n",
            "Epoch 205/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0160 - val_loss: 0.3986\n",
            "Epoch 206/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0160 - val_loss: 0.3999\n",
            "Epoch 207/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.4018\n",
            "Epoch 208/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0156 - val_loss: 0.4032\n",
            "Epoch 209/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.4049\n",
            "Epoch 210/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0149 - val_loss: 0.4068\n",
            "Epoch 211/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0148 - val_loss: 0.4087\n",
            "Epoch 212/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0145 - val_loss: 0.4104\n",
            "Epoch 213/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0141 - val_loss: 0.4122\n",
            "Epoch 214/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0141 - val_loss: 0.4138\n",
            "Epoch 215/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0138 - val_loss: 0.4160\n",
            "Epoch 216/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0138 - val_loss: 0.4175\n",
            "Epoch 217/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0136 - val_loss: 0.4194\n",
            "Epoch 218/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0132 - val_loss: 0.4209\n",
            "Epoch 219/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0130 - val_loss: 0.4229\n",
            "Epoch 220/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0128 - val_loss: 0.4246\n",
            "Epoch 221/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0126 - val_loss: 0.4263\n",
            "Epoch 222/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0123 - val_loss: 0.4279\n",
            "Epoch 223/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0122 - val_loss: 0.4297\n",
            "Epoch 224/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0122 - val_loss: 0.4314\n",
            "Epoch 225/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0118 - val_loss: 0.4331\n",
            "Epoch 226/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0117 - val_loss: 0.4348\n",
            "Epoch 227/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0115 - val_loss: 0.4366\n",
            "Epoch 228/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0113 - val_loss: 0.4384\n",
            "Epoch 229/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0110 - val_loss: 0.4400\n",
            "Epoch 230/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0105 - val_loss: 0.4420\n",
            "Epoch 231/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0105 - val_loss: 0.4439\n",
            "Epoch 232/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0105 - val_loss: 0.4459\n",
            "Epoch 233/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0104 - val_loss: 0.4476\n",
            "Epoch 234/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0102 - val_loss: 0.4494\n",
            "Epoch 235/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0100 - val_loss: 0.4513\n",
            "Epoch 236/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0100 - val_loss: 0.4530\n",
            "Epoch 237/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0097 - val_loss: 0.4546\n",
            "Epoch 238/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0094 - val_loss: 0.4568\n",
            "Epoch 239/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0093 - val_loss: 0.4584\n",
            "Epoch 240/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0091 - val_loss: 0.4606\n",
            "Epoch 241/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0088 - val_loss: 0.4624\n",
            "Epoch 242/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0088 - val_loss: 0.4647\n",
            "Epoch 243/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0088 - val_loss: 0.4664\n",
            "Epoch 244/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0085 - val_loss: 0.4683\n",
            "Epoch 245/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0083 - val_loss: 0.4707\n",
            "Epoch 246/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0083 - val_loss: 0.4722\n",
            "Epoch 247/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0082 - val_loss: 0.4743\n",
            "Epoch 248/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0080 - val_loss: 0.4761\n",
            "Epoch 249/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0078 - val_loss: 0.4778\n",
            "Epoch 250/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0075 - val_loss: 0.4797\n",
            "Epoch 251/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0075 - val_loss: 0.4817\n",
            "Epoch 252/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0073 - val_loss: 0.4839\n",
            "Epoch 253/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0073 - val_loss: 0.4853\n",
            "Epoch 254/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0072 - val_loss: 0.4871\n",
            "Epoch 255/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0070 - val_loss: 0.4891\n",
            "Epoch 256/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0069 - val_loss: 0.4914\n",
            "Epoch 257/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0070 - val_loss: 0.4936\n",
            "Epoch 258/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0068 - val_loss: 0.4953\n",
            "Epoch 259/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0066 - val_loss: 0.4972\n",
            "Epoch 260/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0066 - val_loss: 0.4988\n",
            "Epoch 261/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0064 - val_loss: 0.5010\n",
            "Epoch 262/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0062 - val_loss: 0.5033\n",
            "Epoch 263/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0062 - val_loss: 0.5051\n",
            "Epoch 264/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0060 - val_loss: 0.5068\n",
            "Epoch 265/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0060 - val_loss: 0.5088\n",
            "Epoch 266/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0060 - val_loss: 0.5107\n",
            "Epoch 267/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0058 - val_loss: 0.5124\n",
            "Epoch 268/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0056 - val_loss: 0.5149\n",
            "Epoch 269/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0055 - val_loss: 0.5169\n",
            "Epoch 270/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0055 - val_loss: 0.5190\n",
            "Epoch 271/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0054 - val_loss: 0.5205\n",
            "Epoch 272/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0054 - val_loss: 0.5229\n",
            "Epoch 273/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0051 - val_loss: 0.5248\n",
            "Epoch 274/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0051 - val_loss: 0.5269\n",
            "Epoch 275/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0050 - val_loss: 0.5289\n",
            "Epoch 276/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0051 - val_loss: 0.5307\n",
            "Epoch 277/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0048 - val_loss: 0.5330\n",
            "Epoch 278/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0048 - val_loss: 0.5350\n",
            "Epoch 279/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0046 - val_loss: 0.5371\n",
            "Epoch 280/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0046 - val_loss: 0.5394\n",
            "Epoch 281/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0045 - val_loss: 0.5410\n",
            "Epoch 282/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0045 - val_loss: 0.5429\n",
            "Epoch 283/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0044 - val_loss: 0.5447\n",
            "Epoch 284/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0042 - val_loss: 0.5468\n",
            "Epoch 285/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0043 - val_loss: 0.5488\n",
            "Epoch 286/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0041 - val_loss: 0.5509\n",
            "Epoch 287/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0041 - val_loss: 0.5531\n",
            "Epoch 288/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0041 - val_loss: 0.5550\n",
            "Epoch 289/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0039 - val_loss: 0.5573\n",
            "Epoch 290/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0039 - val_loss: 0.5594\n",
            "Epoch 291/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0038 - val_loss: 0.5615\n",
            "Epoch 292/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0038 - val_loss: 0.5637\n",
            "Epoch 293/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0038 - val_loss: 0.5655\n",
            "Epoch 294/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0037 - val_loss: 0.5678\n",
            "Epoch 295/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0035 - val_loss: 0.5697\n",
            "Epoch 296/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0035 - val_loss: 0.5723\n",
            "Epoch 297/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0036 - val_loss: 0.5736\n",
            "Epoch 298/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0033 - val_loss: 0.5757\n",
            "Epoch 299/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0033 - val_loss: 0.5779\n",
            "Epoch 300/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0033 - val_loss: 0.5804\n",
            "Epoch 301/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.5820\n",
            "Epoch 302/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.5838\n",
            "Epoch 303/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.5858\n",
            "Epoch 304/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.5877\n",
            "Epoch 305/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0031 - val_loss: 0.5893\n",
            "Epoch 306/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0030 - val_loss: 0.5917\n",
            "Epoch 307/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0030 - val_loss: 0.5933\n",
            "Epoch 308/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0028 - val_loss: 0.5956\n",
            "Epoch 309/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0029 - val_loss: 0.5975\n",
            "Epoch 310/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0028 - val_loss: 0.5997\n",
            "Epoch 311/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0027 - val_loss: 0.6015\n",
            "Epoch 312/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0028 - val_loss: 0.6038\n",
            "Epoch 313/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0027 - val_loss: 0.6059\n",
            "Epoch 314/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0026 - val_loss: 0.6073\n",
            "Epoch 315/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0026 - val_loss: 0.6095\n",
            "Epoch 316/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.6121\n",
            "Epoch 317/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.6139\n",
            "Epoch 318/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0024 - val_loss: 0.6158\n",
            "Epoch 319/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0024 - val_loss: 0.6184\n",
            "Epoch 320/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0024 - val_loss: 0.6208\n",
            "Epoch 321/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 0.6226\n",
            "Epoch 322/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6246\n",
            "Epoch 323/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6265\n",
            "Epoch 324/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6292\n",
            "Epoch 325/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0022 - val_loss: 0.6302\n",
            "Epoch 326/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6328\n",
            "Epoch 327/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.6350\n",
            "Epoch 328/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.6367\n",
            "Epoch 329/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.6392\n",
            "Epoch 330/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.6410\n",
            "Epoch 331/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6431\n",
            "Epoch 332/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6452\n",
            "Epoch 333/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6472\n",
            "Epoch 334/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6492\n",
            "Epoch 335/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6511\n",
            "Epoch 336/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6528\n",
            "Epoch 337/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.6547\n",
            "Epoch 338/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.6573\n",
            "Epoch 339/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0017 - val_loss: 0.6593\n",
            "Epoch 340/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0017 - val_loss: 0.6616\n",
            "Epoch 341/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0017 - val_loss: 0.6636\n",
            "Epoch 342/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0017 - val_loss: 0.6661\n",
            "Epoch 343/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0017 - val_loss: 0.6681\n",
            "Epoch 344/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0017 - val_loss: 0.6696\n",
            "Epoch 345/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6716\n",
            "Epoch 346/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6740\n",
            "Epoch 347/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6758\n",
            "Epoch 348/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0016 - val_loss: 0.6783\n",
            "Epoch 349/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6799\n",
            "Epoch 350/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6820\n",
            "Epoch 351/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6845\n",
            "Epoch 352/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6861\n",
            "Epoch 353/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6873\n",
            "Epoch 354/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6905\n",
            "Epoch 355/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6923\n",
            "Epoch 356/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6948\n",
            "Epoch 357/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6960\n",
            "Epoch 358/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6986\n",
            "Epoch 359/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6999\n",
            "Epoch 360/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.7022\n",
            "Epoch 361/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.7047\n",
            "Epoch 362/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7061\n",
            "Epoch 363/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7086\n",
            "Epoch 364/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7109\n",
            "Epoch 365/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7127\n",
            "Epoch 366/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7145\n",
            "Epoch 367/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7169\n",
            "Epoch 368/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7186\n",
            "Epoch 369/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7209\n",
            "Epoch 370/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7236\n",
            "Epoch 371/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7245\n",
            "Epoch 372/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 0.0011 - val_loss: 0.7272\n",
            "Epoch 373/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0010 - val_loss: 0.7289\n",
            "Epoch 374/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0010 - val_loss: 0.7315\n",
            "Epoch 375/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0010 - val_loss: 0.7335\n",
            "Epoch 376/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0010 - val_loss: 0.7353\n",
            "Epoch 377/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.7894e-04 - val_loss: 0.7369\n",
            "Epoch 378/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.6258e-04 - val_loss: 0.7393\n",
            "Epoch 379/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.5219e-04 - val_loss: 0.7419\n",
            "Epoch 380/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.1490e-04 - val_loss: 0.7440\n",
            "Epoch 381/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.3413e-04 - val_loss: 0.7458\n",
            "Epoch 382/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.0145e-04 - val_loss: 0.7470\n",
            "Epoch 383/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.9954e-04 - val_loss: 0.7501\n",
            "Epoch 384/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.0301e-04 - val_loss: 0.7525\n",
            "Epoch 385/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.8456e-04 - val_loss: 0.7541\n",
            "Epoch 386/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.4788e-04 - val_loss: 0.7563\n",
            "Epoch 387/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.2214e-04 - val_loss: 0.7584\n",
            "Epoch 388/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.5612e-04 - val_loss: 0.7613\n",
            "Epoch 389/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.2785e-04 - val_loss: 0.7632\n",
            "Epoch 390/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.1835e-04 - val_loss: 0.7645\n",
            "Epoch 391/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.9605e-04 - val_loss: 0.7664\n",
            "Epoch 392/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.7393e-04 - val_loss: 0.7684\n",
            "Epoch 393/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.6718e-04 - val_loss: 0.7710\n",
            "Epoch 394/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.7612e-04 - val_loss: 0.7725\n",
            "Epoch 395/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.7171e-04 - val_loss: 0.7748\n",
            "Epoch 396/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.3863e-04 - val_loss: 0.7771\n",
            "Epoch 397/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.1831e-04 - val_loss: 0.7791\n",
            "Epoch 398/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.1417e-04 - val_loss: 0.7817\n",
            "Epoch 399/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.0838e-04 - val_loss: 0.7828\n",
            "Epoch 400/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.7994e-04 - val_loss: 0.7854\n",
            "Epoch 401/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.1081e-04 - val_loss: 0.7874\n",
            "Epoch 402/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.8203e-04 - val_loss: 0.7895\n",
            "Epoch 403/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.0019e-04 - val_loss: 0.7915\n",
            "Epoch 404/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 6.8681e-04 - val_loss: 0.7935\n",
            "Epoch 405/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.5470e-04 - val_loss: 0.7963\n",
            "Epoch 406/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.3762e-04 - val_loss: 0.7979\n",
            "Epoch 407/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.2440e-04 - val_loss: 0.8000\n",
            "Epoch 408/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.0699e-04 - val_loss: 0.8015\n",
            "Epoch 409/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.1584e-04 - val_loss: 0.8045\n",
            "Epoch 410/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.9455e-04 - val_loss: 0.8058\n",
            "Epoch 411/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.1010e-04 - val_loss: 0.8078\n",
            "Epoch 412/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.8183e-04 - val_loss: 0.8103\n",
            "Epoch 413/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.7411e-04 - val_loss: 0.8127\n",
            "Epoch 414/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.7170e-04 - val_loss: 0.8140\n",
            "Epoch 415/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.6482e-04 - val_loss: 0.8164\n",
            "Epoch 416/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.6201e-04 - val_loss: 0.8185\n",
            "Epoch 417/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.5805e-04 - val_loss: 0.8205\n",
            "Epoch 418/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.4906e-04 - val_loss: 0.8227\n",
            "Epoch 419/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.2813e-04 - val_loss: 0.8243\n",
            "Epoch 420/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.2845e-04 - val_loss: 0.8276\n",
            "Epoch 421/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.1723e-04 - val_loss: 0.8285\n",
            "Epoch 422/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 5.2665e-04 - val_loss: 0.8313\n",
            "Epoch 423/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.1290e-04 - val_loss: 0.8334\n",
            "Epoch 424/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.1290e-04 - val_loss: 0.8350\n",
            "Epoch 425/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.0051e-04 - val_loss: 0.8380\n",
            "Epoch 426/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 4.8672e-04 - val_loss: 0.8402\n",
            "Epoch 427/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7670e-04 - val_loss: 0.8409\n",
            "Epoch 428/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7084e-04 - val_loss: 0.8437\n",
            "Epoch 429/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7356e-04 - val_loss: 0.8448\n",
            "Epoch 430/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.6802e-04 - val_loss: 0.8471\n",
            "Epoch 431/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.5255e-04 - val_loss: 0.8497\n",
            "Epoch 432/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.6632e-04 - val_loss: 0.8510\n",
            "Epoch 433/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.4691e-04 - val_loss: 0.8534\n",
            "Epoch 434/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.3678e-04 - val_loss: 0.8552\n",
            "Epoch 435/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.2866e-04 - val_loss: 0.8572\n",
            "Epoch 436/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.3686e-04 - val_loss: 0.8593\n",
            "Epoch 437/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.2510e-04 - val_loss: 0.8612\n",
            "Epoch 438/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.1905e-04 - val_loss: 0.8636\n",
            "Epoch 439/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.1543e-04 - val_loss: 0.8663\n",
            "Epoch 440/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.0078e-04 - val_loss: 0.8672\n",
            "Epoch 441/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.9865e-04 - val_loss: 0.8701\n",
            "Epoch 442/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.0030e-04 - val_loss: 0.8714\n",
            "Epoch 443/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.8580e-04 - val_loss: 0.8746\n",
            "Epoch 444/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.7911e-04 - val_loss: 0.8754\n",
            "Epoch 445/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.9487e-04 - val_loss: 0.8777\n",
            "Epoch 446/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.8556e-04 - val_loss: 0.8793\n",
            "Epoch 447/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.8226e-04 - val_loss: 0.8816\n",
            "Epoch 448/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.6947e-04 - val_loss: 0.8839\n",
            "Epoch 449/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.7315e-04 - val_loss: 0.8859\n",
            "Epoch 450/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.6506e-04 - val_loss: 0.8881\n",
            "Epoch 451/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.5880e-04 - val_loss: 0.8903\n",
            "Epoch 452/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.5170e-04 - val_loss: 0.8916\n",
            "Epoch 453/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.5029e-04 - val_loss: 0.8944\n",
            "Epoch 454/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.4874e-04 - val_loss: 0.8960\n",
            "Epoch 455/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.3057e-04 - val_loss: 0.8987\n",
            "Epoch 456/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.2673e-04 - val_loss: 0.9006\n",
            "Epoch 457/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.2874e-04 - val_loss: 0.9023\n",
            "Epoch 458/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.3206e-04 - val_loss: 0.9048\n",
            "Epoch 459/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.1684e-04 - val_loss: 0.9063\n",
            "Epoch 460/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.1414e-04 - val_loss: 0.9091\n",
            "Epoch 461/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 3.2828e-04 - val_loss: 0.9104\n",
            "Epoch 462/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.0545e-04 - val_loss: 0.9128\n",
            "Epoch 463/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.1059e-04 - val_loss: 0.9147\n",
            "Epoch 464/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.0526e-04 - val_loss: 0.9169\n",
            "Epoch 465/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.0193e-04 - val_loss: 0.9191\n",
            "Epoch 466/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.9109e-04 - val_loss: 0.9203\n",
            "Epoch 467/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.9760e-04 - val_loss: 0.9225\n",
            "Epoch 468/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.9277e-04 - val_loss: 0.9248\n",
            "Epoch 469/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7735e-04 - val_loss: 0.9266\n",
            "Epoch 470/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7993e-04 - val_loss: 0.9291\n",
            "Epoch 471/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7997e-04 - val_loss: 0.9315\n",
            "Epoch 472/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7476e-04 - val_loss: 0.9331\n",
            "Epoch 473/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6583e-04 - val_loss: 0.9354\n",
            "Epoch 474/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6868e-04 - val_loss: 0.9362\n",
            "Epoch 475/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6795e-04 - val_loss: 0.9394\n",
            "Epoch 476/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7497e-04 - val_loss: 0.9406\n",
            "Epoch 477/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6730e-04 - val_loss: 0.9437\n",
            "Epoch 478/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6304e-04 - val_loss: 0.9456\n",
            "Epoch 479/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5238e-04 - val_loss: 0.9469\n",
            "Epoch 480/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5622e-04 - val_loss: 0.9485\n",
            "Epoch 481/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5942e-04 - val_loss: 0.9517\n",
            "Epoch 482/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.4274e-04 - val_loss: 0.9535\n",
            "Epoch 483/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.4523e-04 - val_loss: 0.9563\n",
            "Epoch 484/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5168e-04 - val_loss: 0.9575\n",
            "Epoch 485/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.3574e-04 - val_loss: 0.9600\n",
            "Epoch 486/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.3083e-04 - val_loss: 0.9628\n",
            "Epoch 487/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.3828e-04 - val_loss: 0.9641\n",
            "Epoch 488/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.2655e-04 - val_loss: 0.9667\n",
            "Epoch 489/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.2945e-04 - val_loss: 0.9677\n",
            "Epoch 490/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.2826e-04 - val_loss: 0.9701\n",
            "Epoch 491/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 2.3025e-04 - val_loss: 0.9714\n",
            "Epoch 492/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.2721e-04 - val_loss: 0.9738\n",
            "Epoch 493/500\n",
            "40/40 [==============================] - 1s 22ms/step - loss: 2.1386e-04 - val_loss: 0.9762\n",
            "Epoch 494/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1074e-04 - val_loss: 0.9781\n",
            "Epoch 495/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1889e-04 - val_loss: 0.9813\n",
            "Epoch 496/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.2255e-04 - val_loss: 0.9825\n",
            "Epoch 497/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0998e-04 - val_loss: 0.9843\n",
            "Epoch 498/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1154e-04 - val_loss: 0.9861\n",
            "Epoch 499/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0274e-04 - val_loss: 0.9885\n",
            "Epoch 500/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0613e-04 - val_loss: 0.9911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3daXgUVfr38e+drUN2srAlhl0UAgQIICCKoKOCguI+6sCIOjoqOuM+Ouo4Oo+OM+7+VXRwV1xREBQUQVGRTZFVBMIWtiyQfU+f50VVMECAAKlUuvv+XFeudFdVV9+FsX9ddeqcI8YYlFJKBa4gtwtQSinlLg0CpZQKcBoESikV4DQIlFIqwGkQKKVUgNMgUEqpAKdBoNRREpFXReShQ6wvFpFOTVmTUkdDg0D5PBHZJCKnu13H/owxUcaYzENtIyLDRCSrqWpSqj4aBEr5MBEJcbsG5fs0CJTfEhGPiDwpItvtnydFxGOvSxSRT0UkX0R2i8h8EQmy190pIttEpEhE1orIiEO8TUsRmWFvu1BEOtd5fyMiXezHI0Vktb3dNhG5TUQigc+AdvZlpGIRaXeYuoeJSJZd407gFRFZKSLn1nnfUBHJFZE+jf+vqvyRBoHyZ/cAJwHpQG9gAHCvve5WIAtIAloDfwOMiHQDbgT6G2OigTOBTYd4j0uBfwAtgfXAwwfZ7n/An+x9pgFfGWNKgLOB7fZlpChjzPbD1A3QBogH2gPXAq8DV9RZPxLYYYz56RB1K7WXBoHyZ5cDDxpjso0xOVgf2Ffa66qAtkB7Y0yVMWa+sQbeqgE8QHcRCTXGbDLGbDjEe0w1xiwyxlQDb2F9eNenyt5njDFmjzHmx6OsG8AL3G+MqTDGlAFvAiNFJMZefyXwxiH2r9Q+NAiUP2sHbK7zfLO9DOAxrG/ws0UkU0TuAjDGrAduAR4AskVkioi04+B21nlcCkQdZLsLsL6pbxaRr0Vk0FHWDZBjjCmvfWKfRXwHXCAicVhnGW8dYv9K7UODQPmz7ViXT2ql2sswxhQZY241xnQCRgN/rW0LMMa8bYw52X6tAR491kKMMYuNMWOAVsDHwHu1q46k7kO85jWsy0MXAQuMMduOtWYVODQIlL8IFZHwOj8hwDvAvSKSJCKJwH1Yl1EQkXNEpIuICFCAdUnIKyLdRGS43ThbDpRhXYo5aiISJiKXi0isMaYKKKyzz11AgojE1nnJQes+hI+BvsDNWG0GSjWYBoHyFzOxPrRrfx4AHgKWAMuBFcCP9jKArsCXQDGwAPg/Y8xcrPaBR4BcrMs+rYC7G6G+K4FNIlIIXIfVDoAx5hesD/5M+w6mdoepu152W8GHQEfgo0aoVwUQ0YlplPIPInIfcLwx5orDbqxUHdoZRSk/ICLxwAT2vbtIqQbRS0NK+TgRuQbYCnxmjPnG7XqU79FLQ0opFeD0jEAppQKcz7URJCYmmg4dOrhdhlJK+ZSlS5fmGmOS6lvnc0HQoUMHlixZ4nYZSinlU0Rk88HW6aUhpZQKcBoESikV4DQIlFIqwDnWRiAik4FzgGxjTFo96wV4CmtExlJg/GGG5j2oqqoqsrKyKC8vP/zG6qDCw8NJSUkhNDTU7VKUUk3IycbiV4FnOfgAWGdjjffSFRgIPG//PmJZWVlER0fToUMHrHxRR8oYQ15eHllZWXTs2NHtcpRSTcixS0N2D8fdh9hkDPC6sfwAxIlI26N5r/LychISEjQEjoGIkJCQoGdVSgUgN9sIkrG6xdfKspcdQESuFZElIrIkJyen3p1pCBw7/TdUKjD5RGOxMWaSMSbDGJORlFRvfwillPJf1ZUw+14oyHJk924GwTbguDrPU+xlPicvL4/09HTS09Np06YNycnJe59XVlYe8rVLlixh4sSJR/R+HTp0IDc391hKVkr5iuxf4JWz4Ptn4NdZjryFmz2LpwE3isgUrEbiAmPMDhfrOWoJCQksW7YMgAceeICoqChuu+22veurq6sJCan/nzojI4OMjIwmqVMp5UMqimDuv2DxyxAWBRe9Cj3Od+StHDsjEJF3sGZ+6iYiWSIyQUSuE5Hr7E1mAplYE4i/BPzZqVrcMH78eK677joGDhzIHXfcwaJFixg0aBB9+vRh8ODBrF27FoB58+ZxzjnnAFaIXHXVVQwbNoxOnTrx9NNPH/Z9Hn/8cdLS0khLS+PJJ58EoKSkhFGjRtG7d2/S0tJ49913Abjrrrvo3r07vXr12ieolFLNzO5MePkMWPgCpF0INyx0LATAwTMCY8xlh1lvgBsa+33/MX0Vq7cXNuo+u7eL4f5zexzx67Kysvj+++8JDg6msLCQ+fPnExISwpdffsnf/vY3PvzwwwNe88svvzB37lyKioro1q0b119//UHv61+6dCmvvPIKCxcuxBjDwIEDOfXUU8nMzKRdu3bMmDEDgIKCAvLy8pg6dSq//PILIkJ+fv4RH49SymFeLyyfAp/fDSJw5VToNMzxt/WJxmJfddFFFxEcHAxYH8YXXXQRaWlp/OUvf2HVqlX1vmbUqFF4PB4SExNp1aoVu3btOuj+v/32W84//3wiIyOJiopi7NixzJ8/n549e/LFF19w5513Mn/+fGJjY4mNjSU8PJwJEybw0UcfERER4cgxK6WOUtZSmPw7+Ph6iEuFa+Y2SQiAD44+ejhH883dKZGRkXsf//3vf+e0005j6tSpbNq0iWHDhtX7Go/Hs/dxcHAw1dXVR/y+xx9/PD/++CMzZ87k3nvvZcSIEdx3330sWrSIOXPm8MEHH/Dss8/y1VdfHfG+lVKNrKYavvi7dRkoqjWMehwyrrLOCJqI3wVBc1VQUEBystVN4tVXX22UfQ4dOpTx48dz1113YYxh6tSpvPHGG2zfvp34+HiuuOIK4uLiePnllykuLqa0tJSRI0cyZMgQOnXq1Cg1KKWOwc6VMOOvsHUh9BsPw++DyIQmL0ODoInccccdjBs3joceeohRo0Y1yj779u3L+PHjGTBgAABXX301ffr0YdasWdx+++0EBQURGhrK888/T1FREWPGjKG8vBxjDI8//nij1KCUOko/T4EZt0JIOIx+Fvpe6VopPjdncUZGhtl/Ypo1a9Zw4oknulSRf9F/S6Uclr8Fvn4UfnoT2p8MY1+E2BTH31ZElhpj6r1XXc8IlFKqKRgDKz+ET24A44WTboAR90FouNuVaRAopZTjyvJh9j3WWUCbnnDpOxB33OFf10Q0CJRSyiler3U30NePQHkBDLkZTrsXQsLcrmwfGgRKKeWE4myY+ifY8BV0OR1G3A9te7ldVb00CJRSqjF5vbB2Bnz6V6gohHOetG4NbcbDvGsQKKVUYynIgqnXwab5kHQC/OETaN3d7aoOS4OgEeTl5TFixAgAdu7cSXBwMLXzJixatIiwsENfD5w3bx5hYWEMHjz4gHWvvvoqS5Ys4dlnn238wpVSjcNbA8vfg8/vtHoKj3oc+lzZ7NoCDkaDoBEcbhjqw5k3bx5RUVH1BoFSqpkrzrHGB1r/BSRnwNhJkNDZ7aqOiA4655ClS5dy6qmn0q9fP84880x27LCmWnj66af3DgV96aWXsmnTJl544QWeeOIJ0tPTmT9//kH3uWnTJoYPH06vXr0YMWIEW7ZsAeD9998nLS2N3r17c8oppwCwatUqBgwYQHp6Or169WLdunXOH7RSgcTrhSWvwLP9IHMenP1vmDDb50IA/PGM4LO7YOeKxt1nm55w9iMN3twYw0033cQnn3xCUlIS7777Lvfccw+TJ0/mkUceYePGjXg8HvLz84mLi+O6665r0FnETTfdxLhx4xg3bhyTJ09m4sSJfPzxxzz44IPMmjWL5OTkvcNLv/DCC9x8881cfvnlVFZWUlNTc0z/BEqpOgp3wKe3wK+fQ4eh1qWgpOPdruqo+V8QNAMVFRWsXLmSM844A4Camhratm0LQK9evbj88ss577zzOO+8845ovwsWLOCjjz4C4Morr+SOO+4AYMiQIYwfP56LL76YsWPHAjBo0CAefvhhsrKyGDt2LF27dm2sw1MqcBkDqz6Cz+6Esj3WWcCAa5v1HUEN4X9BcATf3J1ijKFHjx4sWLDggHUzZszgm2++Yfr06Tz88MOsWHHsZy8vvPACCxcuZMaMGfTr14+lS5fy+9//noEDBzJjxgxGjhzJiy++yPDhw4/5vZQKWKW7YfrNsGaadUfQuE+h1QluV9UotI3AAR6Ph5ycnL1BUFVVxapVq/B6vWzdupXTTjuNRx99lIKCAoqLi4mOjqaoqOiw+x08eDBTpkwB4K233mLo0KEAbNiwgYEDB/Lggw+SlJTE1q1byczMpFOnTkycOJExY8awfPly5w5YKX+36Vt44WRYOxPOeBCuX+A3IQAaBI4ICgrigw8+4M4776R3796kp6fz/fffU1NTwxVXXEHPnj3p06cPEydOJC4ujnPPPZepU6cetrH4mWee4ZVXXqFXr1688cYbPPXUUwDcfvvt9OzZk7S0NAYPHkzv3r157733SEtLIz09nZUrV/KHP/yhqQ5fKf+xOxOmXg+vjrKGi57whTVMRJB/fXTqMNRqH/pvqRRWW8DP78CM26C6DAZPhFNuB0+U25UdNR2GWimlGqpoJ3z8Z9gwx7pj8KLXfPKW0COhQaCUUrXWTIdpE6GqDEb9FzIm+PwdQQ3hN0FgjEEC4D+Yk3ztMqFSjaYkF2bfa10Oatsbxr7s0/0CjpRfBEF4eDh5eXkkJCRoGBwlYwx5eXmEh7s/W5JSTaa2X8DM2635AobeBqfe6TNjBDUWvwiClJQUsrKyyMnJcbsUnxYeHk5KivNzpyrVLBRnWx3DVn0ESSfC+BnQKjBvlPCLIAgNDaVjx45ul6GU8gVVZbD4fzD3X9YdQac/AIP975bQI+EXQaCUUg1StBPeugh2Lod2feC85wP2LKAuDQKlVGBYPc26LbSm0po8/oSRblfUbGgQKKX8W3khvHs5bPwGkvvB2Jf8vl/AkdIgUEr5r3Vfwoy/QME2a/L4QTdAiMftqpodDQKllP+pKoNZf4MlkyHxeOuOoPaD3K6q2XK0mVxEzhKRtSKyXkTuqmd9qojMFZGfRGS5iOhFO6XUsfl1NjzZywqBwRPhum81BA7DsTMCEQkGngPOALKAxSIyzRizus5m9wLvGWOeF5HuwEygg1M1KaX8WEkefP0oLHoRWveEi16FDkPcrsonOHlpaACw3hiTCSAiU4AxQN0gMECM/TgW2O5gPUopf7VlIUz9E+zZBP2vgd89BKHaS76hnAyCZGBrnedZwMD9tnkAmC0iNwGRwOn17UhErgWuBUhNTW30QpVSPqqyxJo1bMX7EN0WrvocUk9yuyqf43ZXusuAV40xKcBI4A0ROaAmY8wkY0yGMSYjKSmpyYtUSjVD236El0bAyg/hlDvgpqUaAkfJyTOCbcBxdZ6n2MvqmgCcBWCMWSAi4UAikO1gXUopX+atgW/+Y7UHRLWGy9+HLvVeTFAN5GQQLAa6ikhHrAC4FPj9fttsAUYAr4rIiUA4oCPHKaXqt+lba6TQ7NXQ82IY+Ri0iHO7Kp/nWBAYY6pF5EZgFhAMTDbGrBKRB4ElxphpwK3ASyLyF6yG4/FGB8VXSu3PWwPfPgFzH4a49nD+JOh1cUBMGtMUHO1QZoyZiXVLaN1l99V5vBrQ+7uUUgeXt8EaI2jrD9BjLIx+GjzRblflV7RnsVKqeaquhKWvwpf3Q1AonPcC9L5UzwIcoEGglGp+dvwM7/8Rdm+AziNg9DMQm+x2VX5Lg0Ap1Xx4vbD4ZZh9D0QkWMNFdztbzwIcpkGglGoeSnfDO5dZbQFdzoDzX4DIRLerCggaBEopdxkDy9+1po4s2gmjn4X0ywN66simpkGglHJPeQF8cgOsmQ6t06yzgPaD3a4q4GgQKKXcsWa61TmsONsaJO6kG/QswCUaBEqpplVZAjNug5/ftoaLvvQtawpJ5RoNAqVU0zAGMufCJzdC4XY49S445TYIDnW7soCnQaCUcl5lidUWsGoqxKXqcNHNjAaBUspZmfPg07/A7o1w2r1w0nU6REQzo0GglHJGeQF8dif8/A7Ed4Jx06HjULerUvXQIFBKNb4tC+HDq6FwGwy9zWoLCG3hdlXqIDQIlFKNp6IYvn8avnnMbguYBcf1d7sqdRgaBEqpxrHuC6tfwJ6N0OsSGPkfCI9xuyrVABoESqljU1UOn90BP74Gsal2W8ApbleljkDABMHG3BKWZ+UzJl2HslWq0az/0hojaNtSGHwTDL8PQsLcrkodoYDpzz171U5unrKMwvIqt0tRyveV7YFP/wpvXmANETH2ZWuYCA0BnxQwZwTJceFEUcq2PWXEtNWejEodtY3fwEfXQvEuOOnPcPo/NAB8XMCcEfTd+iorw69mR26+26Uo5ZvKC2DWPfDaaAiLhGu+grP+n4aAHwiYM4KoBKttYPfOzdAz1eVqlPIxmV/D1OugaAf0/QOc+S/wRLldlWokARME0UnHAVCat9XlSpTyITVVMPdh+PZJSOgCV8+BFB0p1N8ETBBIjHVGUJ2/zeVKlPIRezbBh9dA1iLoO866DBQW6XZVygEBEwTEtAUgpGSHy4Uo1czVVMG3T8D8/0JQKFw4GdIucLsq5aDACQJPDBUSTouyXW5XolTztX2ZNV/ArhXQ43z43cMQq31v/F3gBIEIRZ5WRJflYIxBRNyuSKnmo6YKvv63dRYQmQiXvgMnjHS7KtVEAicIgIoWrUkq3U1RRTUx4dqXQCkAtv1ozRewYxn0utRqC4iId7sq1YQCKghqotrRZncm2YXlGgRKlRfC4pdg3iPQIh4ueg16nOd2VcoFARUEQbHtaEU+i/NL6dJKZ0hSAWzLQvjoasjfAl3OgLGT9CwggAVUEITFpxAqNRTk7QBau12OUk2vptpqB/j6UYhNgT9+Du0HuV2VcllABUF0otWprCxvK5DubjFKNbX8LdYYQVsWQM+LYdR/db4ABTg81pCInCUia0VkvYjcdZBtLhaR1SKySkTedrKe8AQrCKr3ZDn5Nko1Pys/hOdPhp0r4fxJcMFLGgJqL8fOCEQkGHgOOAPIAhaLyDRjzOo623QF7gaGGGP2iEgrp+qB33oXU6SdylSA2L3RmkB+3SxIzoALXob4jm5XpZoZJy8NDQDWG2MyAURkCjAGWF1nm2uA54wxewCMMdkO1gORSVQTTGjJTkffRinXGQOLX4Yv7oegYBhxvzVxTLDeLacO5GQQJAN1R3jLAgbut83xACLyHRAMPGCM+Xz/HYnItcC1AKmpxzByaFAQhSGJRFZo72Llx4p2wSd/tmYP6zwCRj9tNQwrdRBuNxaHAF2BYUAK8I2I9DTG7DNpgDFmEjAJICMjwxzLG5Z4kogpzj2WXSjVPBkDS1+B2feBt8qaPL7/1aC96NVhOBkE24Dj6jxPsZfVlQUsNMZUARtF5FesYFjsVFGVEW1IKlpFSUU1kR63c1CpRrJzBcy4Dbb+AB1Pte4ISuzqdlXKRzh519BioKuIdBSRMOBSYNp+23yMdTaAiCRiXSrKdLAmvNFtaSO7yS4sd/JtlGoaVeUw+1548RTIWwejn4UrP9YQUEfEsa/ExphqEbkRmIV1/X+yMWaViDwILDHGTLPX/U5EVgM1wO3GmDynagIIiU0mUirIzcuhY5LOsKR82LalMPV6yF0L/cbD6Q9Ai5YuF6V8kaPXRowxM4GZ+y27r85jA/zV/mkStX0JinO2wAl6G53yQdWV8M1jVg/hqNZwxYfQ5XS3q1I+LOAukkfZU1aW52mnMuWDdq6Ej6+z2gR6/94aKbRFnNtVKR8XgEFg3X5aU6BTViofUlMN3z8Fc+0P/kvfhhNGuV2V8hMBFwQSbU1ZGaS9i5WvyPnVOgvYttSaNWzkfyEywe2qlB8JuCAgNJwCicFTqr2LVTPn9cLC52HOgxDaQucOVo4JvCAACkNbEVnp7GgWSh2T3PUwfSJs/g6OPxvOfQqideh05YyADILS8FbEFWobgWqGKkusuQIW/J91FnDe89D7Mu0drBwVkEFQHdmGNgUrKK+qITw02O1ylLJsWWi1BezOhPQrYMR9ehagmoSj8xE0WzHtSJAiduXlH35bpZxWXWGNEvrKWeCthnGfwnnPaQioJhOQZwSeltZIjLk7N9O+jd59oVy0YzlM/RNkr4a+4+DMh8Gj82mrptWgMwIRuVlEYsTyPxH5UUR+53RxTolqZfUlKMreepgtlXJIZYnVJ+Cl4VC6G37/vjVctIaAckFDzwiuMsY8JSJnAi2BK4E3gNmOVeaglm3aA1Cet8XlSlRA2vEzfHAV5K23+gWMehwi4t2uSgWwhgZB7S0LI4E37MHjfPY2Bk+8NcyEt3C7y5WogFJVDt8+Ad8+DhGJMG46dDzF7aqUanAQLBWR2UBH4G4RiQa8zpXlME8MZYQTXKy9i1UT2TAXZtwKuzdA2oVw9r+1d7BqNhoaBBOAdCDTGFMqIvHAH50ry2EiFIQm0qJMp6xUDivOhll/gxXvQ3wnuHIqdB7udlVK7aOhQTAIWGaMKRGRK4C+wFPOleW84hbJxBfsxBiDD1/lUs2V1ws/vgZf3g9VZXDqnXDyXyE03O3KlDpAQ/sRPA+Uikhv4FZgA/C6Y1U1gYroDqSyk8KyKrdLUf4m51d47Vz49BZo0wuu/x5O+5uGgGq2GhoE1fYkMmOAZ40xzwG+fZ9bQkdipJTsXdpgrBpJVRl89RA8Pxh2rYTRz1gNwjptpGrmGnppqEhE7sa6bXSoiAQBoc6V5TxPK+t/zvystdCxg6u1KD+w4Sv49K+wZyP0ugR+9zBEJbldlVIN0tAzgkuACqz+BDuBFOAxx6pqAi1TugFQvmudy5Uon1a0Ez6YAG+cDxIEf/gExk7SEFA+pUFnBMaYnSLyFtBfRM4BFhljfLqNID65K14jePMy3S5F+SJjYMlka4ygmgoYdjcMuUXbAZRPalAQiMjFWGcA87A6lz0jIrcbYz5wsDZHSWg4u4KS8BRtcrsU5Wt2Z8Knf4HMedDpNBj1X0jo7HZVSh21hrYR3AP0N8ZkA4hIEvAl4LNBALDbk0xsmU5irxqouhK+fxq+eQyCw2DkfyBjAgQF5iC+yn80NAiCakPAlocfDGFdGtWerjlfYLxeRP9nVoeycb7VMzh3LZw42uoZHNPW7aqUahQNDYLPRWQW8I79/BJgpjMlNZ3qhOOJzf2YvJwsElqnul2Oao6Ks2H232H5FIhrb40SerzPDryrVL0a2lh8u4hcAAyxF00yxkx1rqymEda2O6yF3Zk/axCofXlrrMbgOf+EqlIYeisMvQ3CItyuTKlG1+CJaYwxHwIfOlhLk4tr3xuA8u2rgHPdLUY1H5u/h8/uhJ3LodMwqy1AO4UpP3bIIBCRIsDUtwowxpgYR6pqIm3bpZJnopGcNW6XopqD/C3wxX2wairEpMCFk6HHWJ04Xvm9QwaBMca3h5E4jBaeENYEpdKyQDuVBbTyAmuegAX/Z3UKG3Y3DJ6ol4FUwAjIOYvr2h3ZmW4lX1odhPSbX2Dx1lgjhH71MJTmWkNDDP87xB3ndmVKNamAD4LK+G5EFk+jJn8rwS21wThgbJwPn98Nu1ZA6mA46wNo18ftqpRyRcAHQWhKOmyB3esWkzRAg8Dv5W+F2ffC6o8hNhUueg26j9GzQRXQHO1FJSJnichaEVkvIncdYrsLRMSISIaT9dQnoXM/qkwwxRsXNfVbq6ZUVQ5f/xue7Q+/zoLT7oEbF0GP8zQEVMBz7IxARIKB54AzgCxgsYhMM8as3m+7aOBmYKFTtRxKl+REfjUpRO9c5sbbK6d5vda3/y8fgPzN1rf/3z2s7QBK1eHkGcEAYL0xJtMYUwlMwZrYZn//BB4Fyh2s5aBiwkNZH9KVhMLVVoOx8h/r58CLp8AHf4TQCGuI6Itf1xBQaj9OBkEysLXO8yx72V4i0hc4zhgz41A7EpFrRWSJiCzJyclp9ELzYrsTWVMIezY1+r6VC3auhDfGwptjoaIAxr4E139ndQ5TSh3AtcZie5azx4Hxh9vWGDMJmASQkZHR6F/bq1unwx7wZi0lKL5jY+9eNZXC7TD3YfjpLQiPtS4BDbgGQjxuV6ZUs+ZkEGwD6p6Dp9jLakUDacA8sRrr2gDTRGS0MWaJg3UdIKZDOiVrPHjXzSe614VN+daqMVQUwXdPw/fPgKmBQTdYYwNFxLtdmVI+wckgWAx0FZGOWAFwKfD72pXGmAIgsfa5iMwDbmvqEADo0qYlS73H03fLd0391upY1FRZHcLmPQol2ZB2AYy4D1p2cLsypXyKY0FgjKkWkRuBWUAwMNkYs0pEHgSWGGOmOfXeR6pbm2heMCdySsF7UJIHkQlul6QOxRhYMx3m/APy1lsdwi6bAin93K5MKZ/kaBuBMWYm+81bYIy57yDbDnOylkOJDg9lW2w/KHkPNn8H3Ue7VYo6nI3z4at/wtaFkNjNCoDjz9K+AEodg4DvWVyrRfsMylZ7CN/4DaJB0Pxs/h7m/gs2zYeoNnDu05B+OQTrn7BSx0rnZ7T16tCK72q6U712lvYnaE62/ACvjYZXzoactXDWI3DzMug3TkNAqUai/yfZ0o+L4w1vX04v/B/k/gpJ3dwuKbBtXQzz/gUbvoKIROtW0IyrdGhopRygQWA7vnU0C4L7Af+DXz/XIHDLtqUw9//B+i8gIgHOeBD6Xw1hkW5XppTf0iCwBQcJbVI6k7mrE53WfgZDbna7pMCy/SeY94gVwi1awoj7YcC14IlyuzKl/J4GQR3pqXF8sqUft2z5ACnIgtgUt0vyfzt+tgJg7UwIj4Ph98KAP0G4T8+CqpRP0cbiOvqmtuTjmkEIBlZ+5HY5/m3HcphyuTUo3KbvYNjf4JblcMrtGgJKNTE9I6ijf4eWbKENO6N60GbFezBkotsl+Z8tC+Gbx6w2AE8MnHoXnHQ9tIhzuzKlApYGQR1xEWGc2CaGz2tOZfzO/4Pty6Bduttl+T5jYMMc+PZJqx9ARIJ1Caj/NRoASjUDemloP4M6J/DM7n6Y0JuaYyUAABWOSURBVAhY/JLb5fi2mmpY8QG8OBTevADyNsCZ/4JbVliXgDQElGoWNAj2M6hTAnnVLcjuMMb6ECvd7XZJvqeiGBa+CM/2gw8nQHUFjPk/uPlna2RQvRVUqWZFg2A/AzrFExIkTAs/B6rLYdEkt0vyHeUF8NVD8Hh3+OwOiEyCS96CPy+EPpdDSJjbFSql6qFtBPuJCQ9lUOcE3sos5epuI5EfnoeT/qx3shxK1lJYMhlWfgjVZdD9PBh0IxzX3+3KlFINoGcE9TgrrQ2b8krZ3OPPUJ4PPzzvdknNT3UlLH8fXhoOLw+HVVOh18Vw7ddw8WsaAkr5EA2CevyuextE4OPs1nDiaPjuSSjYdvgXBoLibGsimCfT4KOroSwfzn4Mbv0FRj+td1kp5YP00lA9kqI99G8fz+crd3LLuH/Cr7Ng1t1w0WuBOe69MbBjGSx6CVa8DzWV0OV0GPgcdB4BQfp9QilfpkFwEGemteGfn65mY00SHYfdZc2G9fMUSL/M7dKaTtFOWP4eLHsbctZAaAT0/QMMvA4Su7pdnVKqkWgQHMTZdhDMXLGDG069GdZ9ATNvh9STIL6j2+U5p6ocfv3M+vBfP8eaDD5lAJzzJPQ4X+/9V8oPaRAcRLu4FvRr35LpP2/nhtO6wNgX4fmT4Z3L4KrP/esD0RjY9iP8/LbVd6I8H2KSrRFY03+v3/6V8nMaBIdwTq+2/GP6atZnF9OlVSpc8jq8eaE1WNoVH0JouNslHpv8rdYtn8vehty1EBIOJ55rffh3PBWCgt2uUCnVBLSV7xBG9mxLkMAHS7OsBZ2GwXnPw+Zv4a0LrQ5UviZ3Hcz/L0waZt358+X91vj/5z4Ft/0KF7wMnYdrCCgVQPSM4BBax4RzZo82vL1wMzcN70KkJwR6XQQY+PjPMPlsuOQNSOjsdqkH5/Vad/ysnQlrpkPOL9by5Aw4/QHr9tjmXL9SynEaBIdx9dBOfLZyJ+8v2cr4IXYjca+LreET3h8PLwyFMx+GvuOaz22UJbnWKJ/rvrB+SrJBgqD9EMiYACeMgthkt6tUSjUTGgSH0a99S/qmxjH5u01cOagDwUF2P4LOp8H138PH18Gnt8CPr1nz63YY2rR9DYyBPZtgywLrZ/MCyFtnrQuPgy4joOuZ1n3/kQlNV5dSymdoEDTANUM7cf1bPzJ71U7O7tn2txWxyXDlJ7DiPfjyH/DaudCmJ/T7o9XoGtWq8YupLLEu72QthS3fw5YfoGiHtS48FlIHWQO8tR8C7fpCsP4nVkodmhhj3K7hiGRkZJglS5Y06XvWeA2n/Wce8ZFhTP3zYKS+b/xVZbD8XfjhBavzlQRBm17QfjCk9LduwYzv1LAhmKsroWwPFG6zPvSz10DOWmu/+Vt+2y4m2frgbz/I+p10YvO5PKWUalZEZKkxJqPedRoEDfPWws3cM3Ul/xuXwYgTWx98Q2OsD+41063r9FmLreGsa0UkQliE1Us3tIV1y2Z1uRUklSVWAFQW77vP4DBI6AqtToAk+6dtb4hLDcwhL5RSR0yDoBFU1Xg54/Gv8YQEM/Pmob+1FRxOdYX1rT5vg/VTmGX13q0qtT78q8utQAhtYYVDi5bQIh4iWkJUa+tDv2VHvcSjlDomhwoC/XRpoNDgIG47sxs3vv0T7y3ZymUDUhv2whCP9e29bW9nC1RKqaOkF5SPwKiebenfoSWPzVpLQVmV2+UopVSj0CA4AiLC/ef2YE9pJU9++avb5SilVKNwNAhE5CwRWSsi60XkrnrW/1VEVovIchGZIyLtnaynMaQlx3LZgFReX7CZZVvz3S5HKaWOmWNBICLBwHPA2UB34DIR6b7fZj8BGcaYXsAHwL+dqqcx3XX2CbSK9nDre8sor6pxuxyllDomTp4RDADWG2MyjTGVwBRgTN0NjDFzjTGl9tMfgBQH62k0MeGhPHpBLzbklPDf2WvdLkcppY6Jk0GQDGyt8zzLXnYwE4DP6lshIteKyBIRWZKTk9OIJR69U45P4oqTUnn5243M/SXb7XKUUuqoNYvGYhG5AsgAHqtvvTFmkjEmwxiTkZSU1LTFHcLdZ5/IiW1i+NObS1mepe0FSinf5GQQbAOOq/M8xV62DxE5HbgHGG2MqXCwnkYX6QnhzasHkhTl4bo3lpJX7FPlK6UU4GwQLAa6ikhHEQkDLgWm1d1ARPoAL2KFgE9eX4mPDOOFK/qRW1LJNa8vYU9JpdslKaXUEXEsCIwx1cCNwCxgDfCeMWaViDwoIqPtzR4DooD3RWSZiEw7yO6atZ4psTx1STortxdy5eSFFJZrZzOllO/QsYYa0Ve/7OLa15fSt31LXr9qAOGhOt2jUqp5ONRYQ82isdhfDD+hNY9fks7iTbu5/s2lVFZ73S5JKaUOS4OgkY3u3Y6Hzktj7tocbnn3JwpK9TKRUqp509FHHXD5wPaUVFTzr5m/sGTTHv43rj89U2LdLksppeqlZwQOufaUzky/8WRCg4O4+MUFTP0py+2SlFKqXhoEDuqZEssH1w+iV0ost72/nOfmrsfr9a3GeaWU/9MgcFjb2BZMHt+fs9Pa8Nistfxh8iKyi8oP/0KllGoiGgRNINITwjOX9eHRC3qyZPNuRj41n/cWb6VGzw6UUs2ABkETEREu6Z/K9BtP5rj4CO74cDkTXlvMtvwyt0tTSgU4DYIm1rV1NB9dP5h/junBD5l5DP/PPF77fhO+1rFPKeU/NAhcICJcOagDc24dxqDOCdw/bRUjHv+a79bnul2aUioAaRC4KDmuBZPH9eeJS3pTVePl8pcXcumkBSzauNvt0pRSAUSDwGVBQcL5fVL44i+nct853dmQU8LFLy7gtvd/Zn12sdvlKaUCgA4618yUVdbw6Oe/MGXxFqpqDBf0Tea0bq0YfmIrPCE6iJ1S6ugcatA5DYJmKre4gmfmrOOdRVuprPFyQptoJpzckfP7JBMSrCdySqkjo0Hgwyqqa5i5YgfPzFlPZm4JnZMiGT+kI6N6tiU+Mszt8pRSPkKDwA8YY5i1ahdPzVnHmh2FhAQJF2WkcNWQjnRKiiI4SNwuUSnVjGkQ+BFjDL/sLOLthVv2tiMkRoVx3amdGZOeTFK0x+0SlVLNkAaBn9pVWM5Xv2TzybJt/JC5GxHoc1wc1w/rwrBuSYRqW4JSyqZB4OdqzxJmrdrJ1J+2sTmvlMiwYIZ1a8Wgzgmc3yeZSI9OPaFUINMgCCDlVTXMW5vNN+ty+WzFDvaUVhEaLPRJbcnJXRI5p1dbOiVFuV2mUqqJaRAEsKWb9zB79U6+W5/Lqu2FCND7uDiGdE5kcJcE+rVvqf0TlAoAGgQKgOyict78YQvz1+WwPKuAGq8hPDSI3ilxjOrVlsGdE+icFIWI3oGklL/RIFAHKCyvYmHmbr7fkMv8dbl7h7OIiwjl+NbRnJ3Whi6tohjUKUE7sCnlBzQI1CEZY9icV8oPmXn8nFXA4k279wZDRFgw/dq3pE9qS7q1juakTvEkROktqkr5mkMFgd5KohAROiRG0iExkksHWMGwo6CcZVvzmb8ulyWbdvPMV+uo/c7QPiGCrq2i6J0Sx4ltY0hPjSNRw0Epn6VBoA4gIrSLa0G7uBaM7NkWgJKKan7dVcS363JZu6uIpZv38OWa7L2vSWnZgjYx4aQlx9K/Qzzd28WQGh+hPZ6V8gF6aUgdtbLKGlZuL2DZlnyWbc1nR0EZK7cXUlntBSA4SGgTE07f9i3pmRxDanwkqfERtE+I0H4NSjUxvTSkHNEiLJj+HeLp3yF+77Liimo25ZawclsBW/eUsnV3GQs35jH95+37vDY+MozU+Ag6JkbSMTGSTkmRdEqMomNiJC3C9HZWpZqSBoFqVFGeENKSY0lLjt1neUFpFVt2l7J5dwlbdpeydXcpm/NKWZiZx9Sftu2zbUJkGHERobSNbcFx8S3onBRFYpSHxCgPSdEeWsd4iG0Rqre5KtVINAhUk4iNCKVnRCw9U2IPWFdaWc3G3BI25paQmVPCjoIycooqyC2uZOaKnRSUVR3wmmhPCO0TI0iM8pAQ6SExKoyEqDDrcbSHhMgwEqM8xEeGERait78qdSgaBMp1EWEh9GgXS492B4aEMYbC8mpyiyvILaogu6iCXYXle88q8koqWbermJziir1tE/uLCQ8hMdpDYqTHCouoMKLDQ4kJD9179hHTIpTYFtbvmPAQojwhesahAoajQSAiZwFPAcHAy8aYR/Zb7wFeB/oBecAlxphNTtakfIuIEGt/SHc+xBhJxhhKKmvILaogr8Q6m8grriS3uIK84gpySyrJK65gfXYxP2RWUFReTbX34DdKBAcJLSPCiAgLpkVoMFF2OESHWz9RnhAiwkKICAsmwhNCZFgwEWEhRHqC9y73hAThCbV/hwThCQkmNFg0YFSz41gQiEgw8BxwBpAFLBaRacaY1XU2mwDsMcZ0EZFLgUeBS5yqSfkvESHKY31Ad0iMbNBrSiurySuupKCsisKyKut3ufU7v7SKPaWVlFd5Ka2spqSihvzSSrbuKaWovJri8mrKqmqOok7whAQRHhpMeEgwntAgwoKDCAuxf4KtdaHBQYQECcFBQlCQEBIkBIkQHATBQUHWb6mzLkgIln0fBwfbv+3XhgTX7sP+qfs46Ld1++xj73r7fUUICuK37fbbR93X1N1HkB1+tRko9n+z2sf7rNOgbHJOnhEMANYbYzIBRGQKMAaoGwRjgAfsxx8Az4qIGF+7p1X5pIiwECLiQzjuKF/v9RrKqmooqaymrLKGkooaKzQqayitqKayxktFlZeK6hoqqr3WT1UN5dVeyqtqKKusobLGS2W1l6oa795t8ksrqaj24jWGGq/Ba6DGa377Mb899noN1fay2sf+5HDBIdTZYO92tY+lzna1r6mzj31f+tu6g2z/Wz7Vt91v7ykH2e8+28m+9R3sGNlv3cQRXRndux2NzckgSAa21nmeBQw82DbGmGoRKQASgNy6G4nItcC1AKmpqU7Vq9QRCQoSIj0hza5PhHe/sKgbEges8xq8xg4Tr8HrhWpvbQhR7z7qvsZbXzgZQ3WN2RtkteFU+/3OGKiNq9qvfIYD19WuNIfZbp91ex/XrjN11lPntWaf11J3+3r2W98+2G8f9e23vtox+9Z34HYHrqtdGNciFCc0r7/ggzDGTAImgdWhzOVylGrWgoKEIIRQ7Y6hGsjJ++q2wT5n3Sn2snq3EZEQIBar0VgppVQTcTIIFgNdRaSjiIQBlwLT9ttmGjDOfnwh8JW2DyilVNNy7NKQfc3/RmAW1u2jk40xq0TkQWCJMWYa8D/gDRFZD+zGCgullFJNyNE2AmPMTGDmfsvuq/O4HLjIyRqUUkodmva9V0qpAKdBoJRSAU6DQCmlApwGgVJKBTifm6FMRHKAzUf58kT267UcAPSYA4Mec2A4lmNub4xJqm+FzwXBsRCRJQebqs1f6TEHBj3mwODUMeulIaWUCnAaBEopFeACLQgmuV2AC/SYA4Mec2Bw5JgDqo1AKaXUgQLtjEAppdR+NAiUUirABUwQiMhZIrJWRNaLyF1u19NYRGSyiGSLyMo6y+JF5AsRWWf/bmkvFxF52v43WC4ifd2r/OiJyHEiMldEVovIKhG52V7ut8ctIuEiskhEfraP+R/28o4istA+tnftId8REY/9fL29voOb9R8tEQkWkZ9E5FP7uV8fL4CIbBKRFSKyTESW2Msc/dsOiCAQkWDgOeBsoDtwmYh0d7eqRvMqcNZ+y+4C5hhjugJz7OdgHX9X++da4PkmqrGxVQO3GmO6AycBN9j/Pf35uCuA4caY3kA6cJaInAQ8CjxhjOkC7AEm2NtPAPbYy5+wt/NFNwNr6jz39+OtdZoxJr1OnwFn/7atOT39+wcYBMyq8/xu4G6362rE4+sArKzzfC3Q1n7cFlhrP34RuKy+7Xz5B/gEOCNQjhuIAH7EmgM8Fwixl+/9O8eaB2SQ/TjE3k7crv0IjzPF/tAbDnyKNYe73x5vnePeBCTut8zRv+2AOCMAkoGtdZ5n2cv8VWtjzA778U6gtf3Y7/4d7EsAfYCF+Plx25dJlgHZwBfABiDfGFNtb1L3uPYes72+AEho2oqP2ZPAHYDXfp6Afx9vLQPMFpGlInKtvczRv22fmLxeHT1jjBERv7xHWESigA+BW4wxhSKyd50/HrcxpgZIF5E4YCpwgsslOUZEzgGyjTFLRWSY2/U0sZONMdtEpBXwhYj8UnelE3/bgXJGsA04rs7zFHuZv9olIm0B7N/Z9nK/+XcQkVCsEHjLGPORvdjvjxvAGJMPzMW6NBInIrVf6Ooe195jttfHAnlNXOqxGAKMFpFNwBSsy0NP4b/Hu5cxZpv9Oxsr8Afg8N92oATBYqCrfcdBGNbcyNNcrslJ04Bx9uNxWNfQa5f/wb7T4CSgoM7pps8Q66v//4A1xpjH66zy2+MWkST7TAARaYHVJrIGKxAutDfb/5hr/y0uBL4y9kVkX2CMudsYk2KM6YD1/+tXxpjL8dPjrSUikSISXfsY+B2wEqf/tt1uGGnCBpiRwK9Y11XvcbueRjyud4AdQBXW9cEJWNdG5wDrgC+BeHtbwbp7agOwAshwu/6jPOaTsa6jLgeW2T8j/fm4gV7AT/YxrwTus5d3AhYB64H3AY+9PNx+vt5e38ntYziGYx8GfBoIx2sf38/2z6razyqn/7Z1iAmllApwgXJpSCml1EFoECilVIDTIFBKqQCnQaCUUgFOg0AppQKcBoFSTUhEhtWOpKlUc6FBoJRSAU6DQKl6iMgV9vj/y0TkRXvAt2IRecKeD2COiCTZ26aLyA/2ePBT64wV30VEvrTnEPhRRDrbu48SkQ9E5BcReUvqDpKklAs0CJTaj4icCFwCDDHGpAM1wOVAJLDEGNMD+Bq4337J68CdxpheWL07a5e/BTxnrDkEBmP1AAdrtNRbsObG6IQ1ro5SrtHRR5U60AigH7DY/rLeAmuQLy/wrr3Nm8BHIhILxBljvraXvwa8b48Xk2yMmQpgjCkHsPe3yBiTZT9fhjWfxLfOH5ZS9dMgUOpAArxmjLl7n4Uif99vu6Mdn6WizuMa9P9D5TK9NKTUgeYAF9rjwdfOF9se6/+X2pEvfw98a4wpAPaIyFB7+ZXA18aYIiBLRM6z9+ERkYgmPQqlGki/iSi1H2PMahG5F2uWqCCskV1vAEqAAfa6bKx2BLCGBX7B/qDPBP5oL78SeFFEHrT3cVETHoZSDaajjyrVQCJSbIyJcrsOpRqbXhpSSqkAp2cESikV4PSMQCmlApwGgVJKBTgNAqWUCnAaBEopFeA0CJRSKsD9f4Qj1MYuDXC5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8662\n",
            "Roc-Auc score: 0.8662941171482065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHk0vT6roWIm",
        "colab_type": "text"
      },
      "source": [
        "### 강사님 방식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ3LklEioXoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4f2e61a-6328-428b-b904-fe4b1d42a6da"
      },
      "source": [
        "x1 = Input(batch_shape=(None, tfidf_vec.shape[1]))\n",
        "x2 = Input(batch_shape=(None, doc2vec_vec.shape[1]))\n",
        "h1 = Dense(200, activation='relu')(x1)\n",
        "h1 = Dropout(0.5)(h1)\n",
        "h2 = Dense(200, activation='relu')(x2)\n",
        "h2 = Dropout(0.5)(h2)\n",
        "concat = Concatenate()([h1, h2])\n",
        "yOutput = Dense(1, activation='sigmoid')(concat)\n",
        "\n",
        "model = Model([x1, x2], yOutput)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001))\n",
        "\n",
        "model.fit([X_train_tf, X_train_doc], y_train,\n",
        "          batch_size = 512,\n",
        "          epochs = 500,\n",
        "          shuffle = True,\n",
        "          validation_data = ([X_test_tf, X_test_doc], y_test))\n",
        "\n",
        "# 시험 데이터로 학습 성능을 평가한다\n",
        "predicted = model.predict([X_test_tf, X_test_doc])\n",
        "test_pred = np.where(predicted > 0.5, 1, 0)\n",
        "accuracy = (y_test.reshape(-1,1) == test_pred).mean()\n",
        "\n",
        "print(\"\\nAccuracy = %.4f\" % accuracy)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "40/40 [==============================] - 1s 32ms/step - loss: 0.6856 - val_loss: 0.6743\n",
            "Epoch 2/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.6614 - val_loss: 0.6458\n",
            "Epoch 3/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.6305 - val_loss: 0.6150\n",
            "Epoch 4/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.5985 - val_loss: 0.5848\n",
            "Epoch 5/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5674 - val_loss: 0.5562\n",
            "Epoch 6/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.5384 - val_loss: 0.5291\n",
            "Epoch 7/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.5101 - val_loss: 0.5038\n",
            "Epoch 8/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4840 - val_loss: 0.4808\n",
            "Epoch 9/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4604 - val_loss: 0.4599\n",
            "Epoch 10/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4385 - val_loss: 0.4409\n",
            "Epoch 11/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.4186 - val_loss: 0.4237\n",
            "Epoch 12/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3998 - val_loss: 0.4079\n",
            "Epoch 13/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3824 - val_loss: 0.3936\n",
            "Epoch 14/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3676 - val_loss: 0.3806\n",
            "Epoch 15/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3524 - val_loss: 0.3686\n",
            "Epoch 16/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3390 - val_loss: 0.3577\n",
            "Epoch 17/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3275 - val_loss: 0.3477\n",
            "Epoch 18/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.3158 - val_loss: 0.3385\n",
            "Epoch 19/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.3049 - val_loss: 0.3302\n",
            "Epoch 20/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2955 - val_loss: 0.3224\n",
            "Epoch 21/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2861 - val_loss: 0.3153\n",
            "Epoch 22/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2769 - val_loss: 0.3089\n",
            "Epoch 23/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2694 - val_loss: 0.3029\n",
            "Epoch 24/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2617 - val_loss: 0.2973\n",
            "Epoch 25/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2543 - val_loss: 0.2923\n",
            "Epoch 26/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2480 - val_loss: 0.2875\n",
            "Epoch 27/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2411 - val_loss: 0.2831\n",
            "Epoch 28/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2356 - val_loss: 0.2790\n",
            "Epoch 29/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2300 - val_loss: 0.2753\n",
            "Epoch 30/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2247 - val_loss: 0.2718\n",
            "Epoch 31/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2193 - val_loss: 0.2686\n",
            "Epoch 32/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2148 - val_loss: 0.2657\n",
            "Epoch 33/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2088 - val_loss: 0.2629\n",
            "Epoch 34/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.2054 - val_loss: 0.2603\n",
            "Epoch 35/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.2003 - val_loss: 0.2580\n",
            "Epoch 36/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1970 - val_loss: 0.2558\n",
            "Epoch 37/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1925 - val_loss: 0.2538\n",
            "Epoch 38/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1890 - val_loss: 0.2519\n",
            "Epoch 39/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1852 - val_loss: 0.2501\n",
            "Epoch 40/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1814 - val_loss: 0.2485\n",
            "Epoch 41/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1790 - val_loss: 0.2470\n",
            "Epoch 42/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1750 - val_loss: 0.2457\n",
            "Epoch 43/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1724 - val_loss: 0.2444\n",
            "Epoch 44/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1694 - val_loss: 0.2433\n",
            "Epoch 45/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1664 - val_loss: 0.2422\n",
            "Epoch 46/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1644 - val_loss: 0.2413\n",
            "Epoch 47/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1605 - val_loss: 0.2405\n",
            "Epoch 48/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1578 - val_loss: 0.2396\n",
            "Epoch 49/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1559 - val_loss: 0.2389\n",
            "Epoch 50/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1525 - val_loss: 0.2382\n",
            "Epoch 51/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1503 - val_loss: 0.2376\n",
            "Epoch 52/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1472 - val_loss: 0.2369\n",
            "Epoch 53/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1460 - val_loss: 0.2365\n",
            "Epoch 54/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1437 - val_loss: 0.2360\n",
            "Epoch 55/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1409 - val_loss: 0.2356\n",
            "Epoch 56/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1392 - val_loss: 0.2354\n",
            "Epoch 57/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1364 - val_loss: 0.2352\n",
            "Epoch 58/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1345 - val_loss: 0.2350\n",
            "Epoch 59/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1333 - val_loss: 0.2349\n",
            "Epoch 60/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1312 - val_loss: 0.2347\n",
            "Epoch 61/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1283 - val_loss: 0.2346\n",
            "Epoch 62/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1271 - val_loss: 0.2345\n",
            "Epoch 63/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1244 - val_loss: 0.2345\n",
            "Epoch 64/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1224 - val_loss: 0.2346\n",
            "Epoch 65/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1211 - val_loss: 0.2347\n",
            "Epoch 66/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1194 - val_loss: 0.2348\n",
            "Epoch 67/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1174 - val_loss: 0.2351\n",
            "Epoch 68/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1162 - val_loss: 0.2351\n",
            "Epoch 69/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1152 - val_loss: 0.2353\n",
            "Epoch 70/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1127 - val_loss: 0.2355\n",
            "Epoch 71/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1111 - val_loss: 0.2359\n",
            "Epoch 72/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1099 - val_loss: 0.2361\n",
            "Epoch 73/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1084 - val_loss: 0.2364\n",
            "Epoch 74/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1068 - val_loss: 0.2368\n",
            "Epoch 75/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.1045 - val_loss: 0.2373\n",
            "Epoch 76/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1035 - val_loss: 0.2376\n",
            "Epoch 77/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1022 - val_loss: 0.2381\n",
            "Epoch 78/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.1001 - val_loss: 0.2384\n",
            "Epoch 79/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0997 - val_loss: 0.2389\n",
            "Epoch 80/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0981 - val_loss: 0.2395\n",
            "Epoch 81/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0970 - val_loss: 0.2400\n",
            "Epoch 82/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0951 - val_loss: 0.2406\n",
            "Epoch 83/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0941 - val_loss: 0.2411\n",
            "Epoch 84/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0930 - val_loss: 0.2417\n",
            "Epoch 85/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0918 - val_loss: 0.2424\n",
            "Epoch 86/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0897 - val_loss: 0.2429\n",
            "Epoch 87/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0889 - val_loss: 0.2435\n",
            "Epoch 88/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0876 - val_loss: 0.2443\n",
            "Epoch 89/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0865 - val_loss: 0.2449\n",
            "Epoch 90/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0855 - val_loss: 0.2456\n",
            "Epoch 91/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0843 - val_loss: 0.2462\n",
            "Epoch 92/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0836 - val_loss: 0.2470\n",
            "Epoch 93/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0828 - val_loss: 0.2476\n",
            "Epoch 94/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0804 - val_loss: 0.2484\n",
            "Epoch 95/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0796 - val_loss: 0.2491\n",
            "Epoch 96/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0784 - val_loss: 0.2497\n",
            "Epoch 97/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0773 - val_loss: 0.2506\n",
            "Epoch 98/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0767 - val_loss: 0.2514\n",
            "Epoch 99/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0755 - val_loss: 0.2521\n",
            "Epoch 100/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0744 - val_loss: 0.2531\n",
            "Epoch 101/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0732 - val_loss: 0.2540\n",
            "Epoch 102/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0732 - val_loss: 0.2550\n",
            "Epoch 103/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0721 - val_loss: 0.2558\n",
            "Epoch 104/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0703 - val_loss: 0.2567\n",
            "Epoch 105/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0695 - val_loss: 0.2578\n",
            "Epoch 106/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0690 - val_loss: 0.2588\n",
            "Epoch 107/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0682 - val_loss: 0.2598\n",
            "Epoch 108/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0669 - val_loss: 0.2607\n",
            "Epoch 109/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0665 - val_loss: 0.2617\n",
            "Epoch 110/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0651 - val_loss: 0.2628\n",
            "Epoch 111/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0637 - val_loss: 0.2636\n",
            "Epoch 112/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0634 - val_loss: 0.2647\n",
            "Epoch 113/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0625 - val_loss: 0.2657\n",
            "Epoch 114/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0620 - val_loss: 0.2667\n",
            "Epoch 115/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0616 - val_loss: 0.2678\n",
            "Epoch 116/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0599 - val_loss: 0.2689\n",
            "Epoch 117/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0590 - val_loss: 0.2699\n",
            "Epoch 118/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0583 - val_loss: 0.2710\n",
            "Epoch 119/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0582 - val_loss: 0.2722\n",
            "Epoch 120/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0569 - val_loss: 0.2734\n",
            "Epoch 121/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0562 - val_loss: 0.2744\n",
            "Epoch 122/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0553 - val_loss: 0.2755\n",
            "Epoch 123/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0541 - val_loss: 0.2766\n",
            "Epoch 124/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0537 - val_loss: 0.2778\n",
            "Epoch 125/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0534 - val_loss: 0.2788\n",
            "Epoch 126/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0523 - val_loss: 0.2800\n",
            "Epoch 127/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0518 - val_loss: 0.2812\n",
            "Epoch 128/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0517 - val_loss: 0.2826\n",
            "Epoch 129/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0505 - val_loss: 0.2837\n",
            "Epoch 130/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0498 - val_loss: 0.2850\n",
            "Epoch 131/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0489 - val_loss: 0.2860\n",
            "Epoch 132/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0486 - val_loss: 0.2872\n",
            "Epoch 133/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0482 - val_loss: 0.2884\n",
            "Epoch 134/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0468 - val_loss: 0.2896\n",
            "Epoch 135/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0464 - val_loss: 0.2908\n",
            "Epoch 136/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0456 - val_loss: 0.2920\n",
            "Epoch 137/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0450 - val_loss: 0.2935\n",
            "Epoch 138/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0449 - val_loss: 0.2946\n",
            "Epoch 139/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0440 - val_loss: 0.2959\n",
            "Epoch 140/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0429 - val_loss: 0.2973\n",
            "Epoch 141/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0426 - val_loss: 0.2986\n",
            "Epoch 142/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0420 - val_loss: 0.2999\n",
            "Epoch 143/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0411 - val_loss: 0.3011\n",
            "Epoch 144/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0408 - val_loss: 0.3024\n",
            "Epoch 145/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0403 - val_loss: 0.3038\n",
            "Epoch 146/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0403 - val_loss: 0.3053\n",
            "Epoch 147/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0396 - val_loss: 0.3065\n",
            "Epoch 148/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0386 - val_loss: 0.3079\n",
            "Epoch 149/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0381 - val_loss: 0.3093\n",
            "Epoch 150/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0372 - val_loss: 0.3107\n",
            "Epoch 151/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0371 - val_loss: 0.3121\n",
            "Epoch 152/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0365 - val_loss: 0.3135\n",
            "Epoch 153/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0360 - val_loss: 0.3148\n",
            "Epoch 154/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0355 - val_loss: 0.3162\n",
            "Epoch 155/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0351 - val_loss: 0.3177\n",
            "Epoch 156/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0340 - val_loss: 0.3190\n",
            "Epoch 157/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0342 - val_loss: 0.3205\n",
            "Epoch 158/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0336 - val_loss: 0.3218\n",
            "Epoch 159/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0331 - val_loss: 0.3234\n",
            "Epoch 160/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0327 - val_loss: 0.3249\n",
            "Epoch 161/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0319 - val_loss: 0.3262\n",
            "Epoch 162/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0320 - val_loss: 0.3277\n",
            "Epoch 163/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0316 - val_loss: 0.3290\n",
            "Epoch 164/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0309 - val_loss: 0.3306\n",
            "Epoch 165/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0302 - val_loss: 0.3321\n",
            "Epoch 166/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0297 - val_loss: 0.3338\n",
            "Epoch 167/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0296 - val_loss: 0.3352\n",
            "Epoch 168/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0288 - val_loss: 0.3369\n",
            "Epoch 169/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0287 - val_loss: 0.3384\n",
            "Epoch 170/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0278 - val_loss: 0.3398\n",
            "Epoch 171/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0280 - val_loss: 0.3412\n",
            "Epoch 172/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0272 - val_loss: 0.3428\n",
            "Epoch 173/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0268 - val_loss: 0.3443\n",
            "Epoch 174/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0261 - val_loss: 0.3459\n",
            "Epoch 175/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0260 - val_loss: 0.3475\n",
            "Epoch 176/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0259 - val_loss: 0.3490\n",
            "Epoch 177/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0253 - val_loss: 0.3505\n",
            "Epoch 178/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0250 - val_loss: 0.3521\n",
            "Epoch 179/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0242 - val_loss: 0.3537\n",
            "Epoch 180/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0240 - val_loss: 0.3553\n",
            "Epoch 181/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0236 - val_loss: 0.3568\n",
            "Epoch 182/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0233 - val_loss: 0.3583\n",
            "Epoch 183/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0228 - val_loss: 0.3598\n",
            "Epoch 184/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0226 - val_loss: 0.3618\n",
            "Epoch 185/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0223 - val_loss: 0.3634\n",
            "Epoch 186/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0220 - val_loss: 0.3649\n",
            "Epoch 187/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0217 - val_loss: 0.3665\n",
            "Epoch 188/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0215 - val_loss: 0.3682\n",
            "Epoch 189/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0209 - val_loss: 0.3700\n",
            "Epoch 190/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0209 - val_loss: 0.3716\n",
            "Epoch 191/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0205 - val_loss: 0.3730\n",
            "Epoch 192/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0201 - val_loss: 0.3747\n",
            "Epoch 193/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0197 - val_loss: 0.3763\n",
            "Epoch 194/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0196 - val_loss: 0.3779\n",
            "Epoch 195/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0192 - val_loss: 0.3794\n",
            "Epoch 196/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0185 - val_loss: 0.3812\n",
            "Epoch 197/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0185 - val_loss: 0.3830\n",
            "Epoch 198/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0184 - val_loss: 0.3849\n",
            "Epoch 199/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0180 - val_loss: 0.3865\n",
            "Epoch 200/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0177 - val_loss: 0.3882\n",
            "Epoch 201/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0175 - val_loss: 0.3896\n",
            "Epoch 202/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0171 - val_loss: 0.3913\n",
            "Epoch 203/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0169 - val_loss: 0.3933\n",
            "Epoch 204/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0168 - val_loss: 0.3942\n",
            "Epoch 205/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0162 - val_loss: 0.3958\n",
            "Epoch 206/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0158 - val_loss: 0.3978\n",
            "Epoch 207/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0161 - val_loss: 0.3993\n",
            "Epoch 208/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0157 - val_loss: 0.4010\n",
            "Epoch 209/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0154 - val_loss: 0.4027\n",
            "Epoch 210/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0151 - val_loss: 0.4046\n",
            "Epoch 211/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0149 - val_loss: 0.4063\n",
            "Epoch 212/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0148 - val_loss: 0.4082\n",
            "Epoch 213/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0144 - val_loss: 0.4097\n",
            "Epoch 214/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0143 - val_loss: 0.4109\n",
            "Epoch 215/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0138 - val_loss: 0.4130\n",
            "Epoch 216/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0135 - val_loss: 0.4148\n",
            "Epoch 217/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0134 - val_loss: 0.4168\n",
            "Epoch 218/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0132 - val_loss: 0.4184\n",
            "Epoch 219/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0131 - val_loss: 0.4203\n",
            "Epoch 220/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0130 - val_loss: 0.4221\n",
            "Epoch 221/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0126 - val_loss: 0.4238\n",
            "Epoch 222/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0124 - val_loss: 0.4255\n",
            "Epoch 223/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0122 - val_loss: 0.4273\n",
            "Epoch 224/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0118 - val_loss: 0.4290\n",
            "Epoch 225/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0118 - val_loss: 0.4311\n",
            "Epoch 226/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0115 - val_loss: 0.4327\n",
            "Epoch 227/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0115 - val_loss: 0.4344\n",
            "Epoch 228/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0112 - val_loss: 0.4362\n",
            "Epoch 229/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0109 - val_loss: 0.4383\n",
            "Epoch 230/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0108 - val_loss: 0.4400\n",
            "Epoch 231/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0106 - val_loss: 0.4417\n",
            "Epoch 232/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0103 - val_loss: 0.4438\n",
            "Epoch 233/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0103 - val_loss: 0.4456\n",
            "Epoch 234/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0100 - val_loss: 0.4472\n",
            "Epoch 235/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0100 - val_loss: 0.4490\n",
            "Epoch 236/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0100 - val_loss: 0.4507\n",
            "Epoch 237/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0096 - val_loss: 0.4524\n",
            "Epoch 238/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0096 - val_loss: 0.4545\n",
            "Epoch 239/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0093 - val_loss: 0.4562\n",
            "Epoch 240/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0092 - val_loss: 0.4583\n",
            "Epoch 241/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0090 - val_loss: 0.4602\n",
            "Epoch 242/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0088 - val_loss: 0.4619\n",
            "Epoch 243/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0089 - val_loss: 0.4639\n",
            "Epoch 244/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0085 - val_loss: 0.4658\n",
            "Epoch 245/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0084 - val_loss: 0.4677\n",
            "Epoch 246/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0082 - val_loss: 0.4693\n",
            "Epoch 247/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0081 - val_loss: 0.4712\n",
            "Epoch 248/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0081 - val_loss: 0.4732\n",
            "Epoch 249/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0079 - val_loss: 0.4751\n",
            "Epoch 250/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0079 - val_loss: 0.4767\n",
            "Epoch 251/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0076 - val_loss: 0.4785\n",
            "Epoch 252/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0074 - val_loss: 0.4807\n",
            "Epoch 253/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0073 - val_loss: 0.4823\n",
            "Epoch 254/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0071 - val_loss: 0.4844\n",
            "Epoch 255/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0072 - val_loss: 0.4861\n",
            "Epoch 256/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0069 - val_loss: 0.4881\n",
            "Epoch 257/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0068 - val_loss: 0.4899\n",
            "Epoch 258/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0067 - val_loss: 0.4921\n",
            "Epoch 259/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0066 - val_loss: 0.4940\n",
            "Epoch 260/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0066 - val_loss: 0.4960\n",
            "Epoch 261/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0064 - val_loss: 0.4981\n",
            "Epoch 262/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0063 - val_loss: 0.5000\n",
            "Epoch 263/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0062 - val_loss: 0.5021\n",
            "Epoch 264/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0062 - val_loss: 0.5041\n",
            "Epoch 265/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0061 - val_loss: 0.5059\n",
            "Epoch 266/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0059 - val_loss: 0.5080\n",
            "Epoch 267/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0057 - val_loss: 0.5102\n",
            "Epoch 268/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0057 - val_loss: 0.5121\n",
            "Epoch 269/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0056 - val_loss: 0.5141\n",
            "Epoch 270/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0055 - val_loss: 0.5158\n",
            "Epoch 271/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0054 - val_loss: 0.5178\n",
            "Epoch 272/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0054 - val_loss: 0.5195\n",
            "Epoch 273/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0052 - val_loss: 0.5216\n",
            "Epoch 274/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0051 - val_loss: 0.5236\n",
            "Epoch 275/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0050 - val_loss: 0.5254\n",
            "Epoch 276/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0050 - val_loss: 0.5274\n",
            "Epoch 277/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0049 - val_loss: 0.5294\n",
            "Epoch 278/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0047 - val_loss: 0.5313\n",
            "Epoch 279/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0047 - val_loss: 0.5333\n",
            "Epoch 280/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0046 - val_loss: 0.5354\n",
            "Epoch 281/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0046 - val_loss: 0.5372\n",
            "Epoch 282/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0045 - val_loss: 0.5392\n",
            "Epoch 283/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0043 - val_loss: 0.5413\n",
            "Epoch 284/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0043 - val_loss: 0.5433\n",
            "Epoch 285/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0042 - val_loss: 0.5454\n",
            "Epoch 286/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0042 - val_loss: 0.5473\n",
            "Epoch 287/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0042 - val_loss: 0.5489\n",
            "Epoch 288/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0040 - val_loss: 0.5512\n",
            "Epoch 289/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0040 - val_loss: 0.5536\n",
            "Epoch 290/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0040 - val_loss: 0.5554\n",
            "Epoch 291/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0038 - val_loss: 0.5575\n",
            "Epoch 292/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0038 - val_loss: 0.5598\n",
            "Epoch 293/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0037 - val_loss: 0.5612\n",
            "Epoch 294/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0036 - val_loss: 0.5630\n",
            "Epoch 295/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0036 - val_loss: 0.5648\n",
            "Epoch 296/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0035 - val_loss: 0.5669\n",
            "Epoch 297/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0034 - val_loss: 0.5690\n",
            "Epoch 298/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0034 - val_loss: 0.5709\n",
            "Epoch 299/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0033 - val_loss: 0.5730\n",
            "Epoch 300/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0033 - val_loss: 0.5750\n",
            "Epoch 301/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.5774\n",
            "Epoch 302/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0032 - val_loss: 0.5791\n",
            "Epoch 303/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0031 - val_loss: 0.5810\n",
            "Epoch 304/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0031 - val_loss: 0.5831\n",
            "Epoch 305/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0030 - val_loss: 0.5848\n",
            "Epoch 306/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0030 - val_loss: 0.5871\n",
            "Epoch 307/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0030 - val_loss: 0.5895\n",
            "Epoch 308/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0028 - val_loss: 0.5917\n",
            "Epoch 309/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0028 - val_loss: 0.5935\n",
            "Epoch 310/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0028 - val_loss: 0.5955\n",
            "Epoch 311/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0027 - val_loss: 0.5981\n",
            "Epoch 312/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0027 - val_loss: 0.6001\n",
            "Epoch 313/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0026 - val_loss: 0.6020\n",
            "Epoch 314/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0026 - val_loss: 0.6041\n",
            "Epoch 315/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.6060\n",
            "Epoch 316/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0026 - val_loss: 0.6078\n",
            "Epoch 317/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.6099\n",
            "Epoch 318/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0024 - val_loss: 0.6119\n",
            "Epoch 319/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0024 - val_loss: 0.6144\n",
            "Epoch 320/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0023 - val_loss: 0.6162\n",
            "Epoch 321/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0023 - val_loss: 0.6183\n",
            "Epoch 322/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0023 - val_loss: 0.6207\n",
            "Epoch 323/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6219\n",
            "Epoch 324/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0023 - val_loss: 0.6235\n",
            "Epoch 325/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.6260\n",
            "Epoch 326/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0022 - val_loss: 0.6276\n",
            "Epoch 327/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0021 - val_loss: 0.6302\n",
            "Epoch 328/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0020 - val_loss: 0.6324\n",
            "Epoch 329/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0020 - val_loss: 0.6343\n",
            "Epoch 330/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0020 - val_loss: 0.6364\n",
            "Epoch 331/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0020 - val_loss: 0.6386\n",
            "Epoch 332/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0019 - val_loss: 0.6405\n",
            "Epoch 333/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0020 - val_loss: 0.6426\n",
            "Epoch 334/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0019 - val_loss: 0.6441\n",
            "Epoch 335/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0018 - val_loss: 0.6465\n",
            "Epoch 336/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.6487\n",
            "Epoch 337/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.6506\n",
            "Epoch 338/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0018 - val_loss: 0.6528\n",
            "Epoch 339/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0017 - val_loss: 0.6549\n",
            "Epoch 340/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0017 - val_loss: 0.6567\n",
            "Epoch 341/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0017 - val_loss: 0.6586\n",
            "Epoch 342/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0016 - val_loss: 0.6605\n",
            "Epoch 343/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0017 - val_loss: 0.6631\n",
            "Epoch 344/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0016 - val_loss: 0.6649\n",
            "Epoch 345/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0016 - val_loss: 0.6673\n",
            "Epoch 346/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0016 - val_loss: 0.6689\n",
            "Epoch 347/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6711\n",
            "Epoch 348/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6735\n",
            "Epoch 349/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6749\n",
            "Epoch 350/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.6768\n",
            "Epoch 351/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0014 - val_loss: 0.6793\n",
            "Epoch 352/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0014 - val_loss: 0.6812\n",
            "Epoch 353/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6829\n",
            "Epoch 354/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6853\n",
            "Epoch 355/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6873\n",
            "Epoch 356/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.6888\n",
            "Epoch 357/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0013 - val_loss: 0.6911\n",
            "Epoch 358/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0013 - val_loss: 0.6935\n",
            "Epoch 359/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.6957\n",
            "Epoch 360/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.6979\n",
            "Epoch 361/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.7004\n",
            "Epoch 362/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7017\n",
            "Epoch 363/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0012 - val_loss: 0.7039\n",
            "Epoch 364/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7059\n",
            "Epoch 365/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7079\n",
            "Epoch 366/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 0.7098\n",
            "Epoch 367/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7119\n",
            "Epoch 368/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7146\n",
            "Epoch 369/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7159\n",
            "Epoch 370/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7182\n",
            "Epoch 371/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 0.0011 - val_loss: 0.7201\n",
            "Epoch 372/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0010 - val_loss: 0.7225\n",
            "Epoch 373/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0010 - val_loss: 0.7249\n",
            "Epoch 374/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 0.0010 - val_loss: 0.7268\n",
            "Epoch 375/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.7213e-04 - val_loss: 0.7286\n",
            "Epoch 376/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 9.7121e-04 - val_loss: 0.7313\n",
            "Epoch 377/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.7943e-04 - val_loss: 0.7323\n",
            "Epoch 378/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.5686e-04 - val_loss: 0.7345\n",
            "Epoch 379/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 9.3990e-04 - val_loss: 0.7367\n",
            "Epoch 380/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.9024e-04 - val_loss: 0.7388\n",
            "Epoch 381/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.7705e-04 - val_loss: 0.7412\n",
            "Epoch 382/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.9080e-04 - val_loss: 0.7433\n",
            "Epoch 383/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 8.7687e-04 - val_loss: 0.7451\n",
            "Epoch 384/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 8.6109e-04 - val_loss: 0.7475\n",
            "Epoch 385/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 8.4205e-04 - val_loss: 0.7492\n",
            "Epoch 386/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.2154e-04 - val_loss: 0.7512\n",
            "Epoch 387/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 8.1360e-04 - val_loss: 0.7534\n",
            "Epoch 388/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 8.0302e-04 - val_loss: 0.7555\n",
            "Epoch 389/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.6691e-04 - val_loss: 0.7576\n",
            "Epoch 390/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 7.7646e-04 - val_loss: 0.7595\n",
            "Epoch 391/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 7.6399e-04 - val_loss: 0.7615\n",
            "Epoch 392/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 7.4089e-04 - val_loss: 0.7637\n",
            "Epoch 393/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.5814e-04 - val_loss: 0.7656\n",
            "Epoch 394/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.5617e-04 - val_loss: 0.7678\n",
            "Epoch 395/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 7.0853e-04 - val_loss: 0.7700\n",
            "Epoch 396/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 6.9492e-04 - val_loss: 0.7720\n",
            "Epoch 397/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 7.1905e-04 - val_loss: 0.7739\n",
            "Epoch 398/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 7.1325e-04 - val_loss: 0.7762\n",
            "Epoch 399/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 6.7596e-04 - val_loss: 0.7777\n",
            "Epoch 400/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 6.4938e-04 - val_loss: 0.7803\n",
            "Epoch 401/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.5380e-04 - val_loss: 0.7824\n",
            "Epoch 402/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.4246e-04 - val_loss: 0.7850\n",
            "Epoch 403/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.5270e-04 - val_loss: 0.7874\n",
            "Epoch 404/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.2493e-04 - val_loss: 0.7889\n",
            "Epoch 405/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 6.5670e-04 - val_loss: 0.7911\n",
            "Epoch 406/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 6.0137e-04 - val_loss: 0.7933\n",
            "Epoch 407/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.1212e-04 - val_loss: 0.7952\n",
            "Epoch 408/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 6.0801e-04 - val_loss: 0.7968\n",
            "Epoch 409/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.8056e-04 - val_loss: 0.7992\n",
            "Epoch 410/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.7275e-04 - val_loss: 0.8006\n",
            "Epoch 411/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.7742e-04 - val_loss: 0.8031\n",
            "Epoch 412/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.5172e-04 - val_loss: 0.8052\n",
            "Epoch 413/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 5.5901e-04 - val_loss: 0.8071\n",
            "Epoch 414/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.3130e-04 - val_loss: 0.8090\n",
            "Epoch 415/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.3944e-04 - val_loss: 0.8112\n",
            "Epoch 416/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.2381e-04 - val_loss: 0.8135\n",
            "Epoch 417/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.3073e-04 - val_loss: 0.8157\n",
            "Epoch 418/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 5.1766e-04 - val_loss: 0.8179\n",
            "Epoch 419/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.9995e-04 - val_loss: 0.8199\n",
            "Epoch 420/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7305e-04 - val_loss: 0.8221\n",
            "Epoch 421/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 4.8219e-04 - val_loss: 0.8240\n",
            "Epoch 422/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.6625e-04 - val_loss: 0.8258\n",
            "Epoch 423/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 4.6564e-04 - val_loss: 0.8279\n",
            "Epoch 424/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.6074e-04 - val_loss: 0.8297\n",
            "Epoch 425/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.6038e-04 - val_loss: 0.8322\n",
            "Epoch 426/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.4487e-04 - val_loss: 0.8337\n",
            "Epoch 427/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.7916e-04 - val_loss: 0.8360\n",
            "Epoch 428/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.4688e-04 - val_loss: 0.8383\n",
            "Epoch 429/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.2250e-04 - val_loss: 0.8404\n",
            "Epoch 430/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.2558e-04 - val_loss: 0.8425\n",
            "Epoch 431/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 4.2020e-04 - val_loss: 0.8441\n",
            "Epoch 432/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 4.0345e-04 - val_loss: 0.8468\n",
            "Epoch 433/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 4.6084e-04 - val_loss: 0.8481\n",
            "Epoch 434/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 4.2773e-04 - val_loss: 0.8507\n",
            "Epoch 435/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.7646e-04 - val_loss: 0.8522\n",
            "Epoch 436/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.9659e-04 - val_loss: 0.8547\n",
            "Epoch 437/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.8304e-04 - val_loss: 0.8570\n",
            "Epoch 438/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.8114e-04 - val_loss: 0.8589\n",
            "Epoch 439/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 3.7464e-04 - val_loss: 0.8610\n",
            "Epoch 440/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.8467e-04 - val_loss: 0.8625\n",
            "Epoch 441/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.5925e-04 - val_loss: 0.8646\n",
            "Epoch 442/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.6156e-04 - val_loss: 0.8669\n",
            "Epoch 443/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.4519e-04 - val_loss: 0.8688\n",
            "Epoch 444/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.3716e-04 - val_loss: 0.8713\n",
            "Epoch 445/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.4824e-04 - val_loss: 0.8729\n",
            "Epoch 446/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.5319e-04 - val_loss: 0.8742\n",
            "Epoch 447/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.3316e-04 - val_loss: 0.8767\n",
            "Epoch 448/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.1946e-04 - val_loss: 0.8789\n",
            "Epoch 449/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.3280e-04 - val_loss: 0.8812\n",
            "Epoch 450/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.3028e-04 - val_loss: 0.8840\n",
            "Epoch 451/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.4680e-04 - val_loss: 0.8855\n",
            "Epoch 452/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.2667e-04 - val_loss: 0.8878\n",
            "Epoch 453/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.1993e-04 - val_loss: 0.8892\n",
            "Epoch 454/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.9398e-04 - val_loss: 0.8910\n",
            "Epoch 455/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.0095e-04 - val_loss: 0.8931\n",
            "Epoch 456/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.9800e-04 - val_loss: 0.8952\n",
            "Epoch 457/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.1339e-04 - val_loss: 0.8969\n",
            "Epoch 458/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.8518e-04 - val_loss: 0.8996\n",
            "Epoch 459/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 3.0330e-04 - val_loss: 0.9008\n",
            "Epoch 460/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7315e-04 - val_loss: 0.9039\n",
            "Epoch 461/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7240e-04 - val_loss: 0.9055\n",
            "Epoch 462/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.6578e-04 - val_loss: 0.9074\n",
            "Epoch 463/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.7524e-04 - val_loss: 0.9096\n",
            "Epoch 464/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6082e-04 - val_loss: 0.9113\n",
            "Epoch 465/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5919e-04 - val_loss: 0.9138\n",
            "Epoch 466/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5350e-04 - val_loss: 0.9152\n",
            "Epoch 467/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5127e-04 - val_loss: 0.9175\n",
            "Epoch 468/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5928e-04 - val_loss: 0.9189\n",
            "Epoch 469/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.5036e-04 - val_loss: 0.9206\n",
            "Epoch 470/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.4705e-04 - val_loss: 0.9231\n",
            "Epoch 471/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.3716e-04 - val_loss: 0.9247\n",
            "Epoch 472/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.4050e-04 - val_loss: 0.9271\n",
            "Epoch 473/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.3524e-04 - val_loss: 0.9287\n",
            "Epoch 474/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.6146e-04 - val_loss: 0.9315\n",
            "Epoch 475/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.4059e-04 - val_loss: 0.9333\n",
            "Epoch 476/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.3088e-04 - val_loss: 0.9348\n",
            "Epoch 477/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.2110e-04 - val_loss: 0.9365\n",
            "Epoch 478/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1840e-04 - val_loss: 0.9382\n",
            "Epoch 479/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1738e-04 - val_loss: 0.9405\n",
            "Epoch 480/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0977e-04 - val_loss: 0.9425\n",
            "Epoch 481/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.1175e-04 - val_loss: 0.9436\n",
            "Epoch 482/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0298e-04 - val_loss: 0.9462\n",
            "Epoch 483/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 2.2039e-04 - val_loss: 0.9479\n",
            "Epoch 484/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.0348e-04 - val_loss: 0.9507\n",
            "Epoch 485/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8721e-04 - val_loss: 0.9525\n",
            "Epoch 486/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.9758e-04 - val_loss: 0.9544\n",
            "Epoch 487/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8320e-04 - val_loss: 0.9561\n",
            "Epoch 488/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8123e-04 - val_loss: 0.9590\n",
            "Epoch 489/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.8208e-04 - val_loss: 0.9613\n",
            "Epoch 490/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 2.1859e-04 - val_loss: 0.9624\n",
            "Epoch 491/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8111e-04 - val_loss: 0.9642\n",
            "Epoch 492/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8179e-04 - val_loss: 0.9662\n",
            "Epoch 493/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8868e-04 - val_loss: 0.9688\n",
            "Epoch 494/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.8586e-04 - val_loss: 0.9705\n",
            "Epoch 495/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.7270e-04 - val_loss: 0.9724\n",
            "Epoch 496/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6966e-04 - val_loss: 0.9745\n",
            "Epoch 497/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6707e-04 - val_loss: 0.9761\n",
            "Epoch 498/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.6868e-04 - val_loss: 0.9792\n",
            "Epoch 499/500\n",
            "40/40 [==============================] - 1s 24ms/step - loss: 1.8007e-04 - val_loss: 0.9812\n",
            "Epoch 500/500\n",
            "40/40 [==============================] - 1s 23ms/step - loss: 1.6870e-04 - val_loss: 0.9831\n",
            "\n",
            "Accuracy = 0.8654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JUPrbTLo-Nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de5ab98a-3529-4365-aa11-5964e020a9f9"
      },
      "source": [
        "print(\"AUC score = %.4f\" % roc_auc_score(y_test, predicted))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC score = 0.9391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96OqYxTWrdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3k1HtVDfNB",
        "colab_type": "text"
      },
      "source": [
        "## _3_. Kaggle 데이터 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFTaT6B2Dnuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8d5f4b31-3a67-4cc0-e706-0116b9de445f"
      },
      "source": [
        "# 데이터 로드 및 원본 보존\n",
        "df_pred = pd.read_csv(f\"{data_path}/IMDB-testData.tsv\",\n",
        "                     header=0,\n",
        "                     delimiter='\\t',\n",
        "                     quoting=3)\n",
        "display(df_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"12311_10\"</td>\n",
              "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"8348_2\"</td>\n",
              "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"5828_4\"</td>\n",
              "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7186_2\"</td>\n",
              "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"12128_7\"</td>\n",
              "      <td>\"A very accurate depiction of small time mob l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>\"2155_10\"</td>\n",
              "      <td>\"Sony Pictures Classics, I'm looking at you! S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>\"59_10\"</td>\n",
              "      <td>\"I always felt that Ms. Merkerson had never go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>\"2531_1\"</td>\n",
              "      <td>\"I was so disappointed in this movie. I am ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>\"7772_8\"</td>\n",
              "      <td>\"From the opening sequence, filled with black ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>\"11465_10\"</td>\n",
              "      <td>\"This is a great horror film for people who do...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               id                                             review\n",
              "0      \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
              "1        \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
              "2        \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
              "3        \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
              "4       \"12128_7\"  \"A very accurate depiction of small time mob l...\n",
              "...           ...                                                ...\n",
              "24995   \"2155_10\"  \"Sony Pictures Classics, I'm looking at you! S...\n",
              "24996     \"59_10\"  \"I always felt that Ms. Merkerson had never go...\n",
              "24997    \"2531_1\"  \"I was so disappointed in this movie. I am ver...\n",
              "24998    \"7772_8\"  \"From the opening sequence, filled with black ...\n",
              "24999  \"11465_10\"  \"This is a great horror film for people who do...\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggt02YkbHR0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5400d6b8-62d2-483a-a3aa-0e9efd2c53dc"
      },
      "source": [
        "%%time\n",
        "df_pred['text'] = df_pred['review'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 38s, sys: 53.5 s, total: 9min 31s\n",
            "Wall time: 9min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_IHYDhTPoEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9a16ef88-5958-443d-900b-fbf5c0ce41ea"
      },
      "source": [
        "# 원본 데이터 보존\n",
        "# df_pred_raw = df_pred.copy()\n",
        "# display(df_pred_raw)\n",
        "df_pred = df_pred[['text']] # 예측해야 할 텍스트\n",
        "df_pred"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>natur film main theme mortal nostalgia loss in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>movi disast within disast film full great acti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>movi kid saw tonight child love one point kid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>afraid dark left impress sever differ screenpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>accur depict small time mob life film new jers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>soni pictur classic look soni got right harri ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>alway felt ms merkerson never gotten role fit ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>disappoint movi familiar case read mark fuhrma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>open sequenc fill black white shot reminisc go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>great horror film peopl want vomit retch gore ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "0      natur film main theme mortal nostalgia loss in...\n",
              "1      movi disast within disast film full great acti...\n",
              "2      movi kid saw tonight child love one point kid ...\n",
              "3      afraid dark left impress sever differ screenpl...\n",
              "4      accur depict small time mob life film new jers...\n",
              "...                                                  ...\n",
              "24995  soni pictur classic look soni got right harri ...\n",
              "24996  alway felt ms merkerson never gotten role fit ...\n",
              "24997  disappoint movi familiar case read mark fuhrma...\n",
              "24998  open sequenc fill black white shot reminisc go...\n",
              "24999  great horror film peopl want vomit retch gore ...\n",
              "\n",
              "[25000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OEcfUiYICX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d3588fd7-99c6-47a3-f8d6-1a566b7f1898"
      },
      "source": [
        "# 토크나이징\n",
        "print(f\"기존 문장 개수: {len(df_pred['text'])}\")\n",
        "\n",
        "tokens = []\n",
        "for idx, text in enumerate(df_pred['text']):\n",
        "    temp = []\n",
        "    delete_indices = []\n",
        "\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            temp.append(word2idx[word])\n",
        "        except KeyError: # OOV\n",
        "            continue \n",
        "    \n",
        "    if len(temp) > 0:\n",
        "        tokens.append(temp)\n",
        "    else:\n",
        "        delete_indices.append(idx)\n",
        "\n",
        "print(f\"OOV 없는 문장으로만 남긴 개수: {len(tokens)}\")\n",
        "print(f\"삭제해야 할 문장 인덱스: {delete_indices}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "기존 문장 개수: 25000\n",
            "OOV 없는 문장으로만 남긴 개수: 25000\n",
            "삭제해야 할 문장 인덱스: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjHOEWMOIMEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e56e6e5c-e2d5-44c9-e6c7-4d3875fb5f82"
      },
      "source": [
        "# 문장 패딩\n",
        "# MAX_LENGTH = int(input('문장 최대 길이 설정: '))\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "X_pred = pad_sequences(tokens,\n",
        "                       maxlen=MAX_LENGTH,\n",
        "                       padding='post',\n",
        "                       truncating='post')\n",
        "\n",
        "X_pred = np.array(X_pred)\n",
        "print(f\"패딩 후 pred data: {X_pred.shape}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "패딩 후 pred data: (25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbGHaS81ILVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 원래 문장으로 바꾸기\n",
        "pred_sequences = []\n",
        "for vector in X_pred:\n",
        "    temp = []\n",
        "    for x in vector:\n",
        "        try:\n",
        "            temp.append(idx2word[x])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    pred_sequences.append(temp)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siZgX2_TIdvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "76474246-0936-4844-9433-748dd5bb186e"
      },
      "source": [
        "# Tf-IDF\n",
        "pred_corpus = [\" \".join(sequence) for sequence in pred_sequences]\n",
        "print(pred_corpus[:3])\n",
        "\n",
        "tfidf_vec_pred = vectorizer.transform(pred_corpus).toarray()\n",
        "print(f\"TF-IDF 행렬: {tfidf_vec_pred.shape}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['natur film main theme mortal nostalgia innoc perhap surpris rate highli older viewer younger complet film anyon enjoy pace steadi full engag relationship interact natur show need tear show emot scream show fear shout show show anger natur short stori lend film made perfect polish small chang huston make poem fit neatli truli masterpiec tact subtleti overwhelm beauti', 'movi disast within disast film full great action scene throw away sens realiti let see word wise lava steam stand lava minor lava flow difficult let signific scare think might actual believ saw movi even wors signific amount talent went make film mean act actual effect averag hard believ somebodi read script allow talent wast guess would movi start tv look away like train wreck aw know watch look away spend time', 'movi kid saw tonight child love point kid excit great sit imposs great fan miln book subtl wri intellig childlik qualiti lead film subtl seem shame disney see benefit make movi stori contain page although perhap permiss wish theater instead voic realli bother music loud part dialog incongru film stori bit tone overal disappoint would go see excit child face like lumpi laugh']\n",
            "TF-IDF 행렬: (25000, 24530)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikwb0BUdIrBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "2445f33e-163b-4d33-aaef-7e43ed49e074"
      },
      "source": [
        "# Doc2Vec 파라미터 설정\n",
        "# doc2vec_features = int(input('Doc2Vec 임베딩 차원 설정: '))\n",
        "doc2vec_features = 400\n",
        "\n",
        "# 사용할/로드할 모델 경로 설정\n",
        "model_path = f\"{data_path}/IMDB_pred_{doc2vec_features}features.doc2vec\"\n",
        "\n",
        "try:\n",
        "    doc_model = Doc2Vec.load(model_path)\n",
        "except: # 저장된 모델 없는 경우\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(pred_sequences)]\n",
        "    doc_model = Doc2Vec(vector_size=doc2vec_features,\n",
        "                        alpha=0.005,\n",
        "                        min_alpha=0.0001,\n",
        "                        min_count=1,\n",
        "                        workers=4,\n",
        "                        dm=1)\n",
        "    doc_model.build_vocab(documents)\n",
        "    doc_model.train(documents, total_examples=doc_model.corpus_count, epochs=10)\n",
        "    doc_model.save(model_path)\n",
        "\n",
        "# 모델 확인\n",
        "keys = list(doc_model.wv.vocab.keys())\n",
        "print(f\"단어 개수: {len(keys)}\")\n",
        "print(\"========= 샘플 확인 =========\")\n",
        "print(keys[:20])\n",
        "\n",
        "# Doc2Vec 벡터 생성\n",
        "doc2vec_vec_pred = [doc_model.docvecs[i] for i in range(len(pred_sequences))]\n",
        "doc2vec_vec_pred = np.array(doc2vec_vec_pred)\n",
        "print(doc2vec_vec_pred.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "단어 개수: 14847\n",
            "========= 샘플 확인 =========\n",
            "['natur', 'film', 'main', 'theme', 'mortal', 'nostalgia', 'innoc', 'perhap', 'surpris', 'rate', 'highli', 'older', 'viewer', 'younger', 'complet', 'anyon', 'enjoy', 'pace', 'steadi', 'full']\n",
            "(25000, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6odxIU_nIubh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "010a0e33-c9b8-4724-9fbf-b3037737f26f"
      },
      "source": [
        "# 예측 및 결과 확인\n",
        "preds = model.predict([tfidf_vec_pred, doc2vec_vec_pred])\n",
        "preds = np.where(preds > 0.5, 1, 0)\n",
        "print(preds.shape)\n",
        "# accuracy = (y_test.reshape(-1, 1) == y_pred).mean()\n",
        "# rocauc = roc_auc_score(y_test, y_pred)\n",
        "# print(f\"Test Accuracy: {accuracy}\")\n",
        "# print(f\"Roc-Auc score: {rocauc}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGTBBlu_RDvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c325b365-e1af-4908-f072-c62434279b21"
      },
      "source": [
        "preds = preds.reshape(-1, )\n",
        "preds"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhegzJsrRWZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f1c7f0c1-d163-4119-f212-e649e5e21a57"
      },
      "source": [
        "submission = pd.read_csv(f\"{data_path}/sampleSubmission_IMDB.csv\")\n",
        "submission"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12311_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8348_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5828_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7186_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12128_7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>2155_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>59_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>2531_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>7772_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>11465_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  sentiment\n",
              "0      12311_10          0\n",
              "1        8348_2          0\n",
              "2        5828_4          0\n",
              "3        7186_2          0\n",
              "4       12128_7          0\n",
              "...         ...        ...\n",
              "24995   2155_10          0\n",
              "24996     59_10          0\n",
              "24997    2531_1          0\n",
              "24998    7772_8          0\n",
              "24999  11465_10          0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4yvrV-AR7Bl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d54890f5-ca9a-4c4d-e70e-85c18c748894"
      },
      "source": [
        "submission['sentiment'] = preds\n",
        "submission"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12311_10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8348_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5828_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7186_2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12128_7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>2155_10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>59_10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>2531_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>7772_8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>11465_10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  sentiment\n",
              "0      12311_10          1\n",
              "1        8348_2          0\n",
              "2        5828_4          0\n",
              "3        7186_2          0\n",
              "4       12128_7          1\n",
              "...         ...        ...\n",
              "24995   2155_10          1\n",
              "24996     59_10          1\n",
              "24997    2531_1          0\n",
              "24998    7772_8          1\n",
              "24999  11465_10          0\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCMH6kHiR-gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# submission.to_csv(f\"{root_path}/submission_IMDB_01.csv\", index=False, encoding='utf-8-sig')\n",
        "submission.to_csv(f\"{root_path}/submission_IMDB_02.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hMLUgNncp-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}