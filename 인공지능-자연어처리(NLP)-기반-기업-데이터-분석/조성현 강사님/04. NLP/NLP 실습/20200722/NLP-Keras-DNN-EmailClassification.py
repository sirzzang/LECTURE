# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy7JpsarHpHcQ6wyYGcoE1mPWh7OMNJj

# 코드 변경

- 개행문자 제거
- 불용어 제거 및 최소 단어 길이 설정
- 어간
- functional API
    - `predict_classes` 없음.
"""

# module import
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import string
from nltk import pos_tag
from nltk.stem import PorterStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# 1) 데이터 로드
train_data = fetch_20newsgroups(subset='train')
test_data = fetch_20newsgroups(subset='test')

# 2) 데이터 준비
X_train, y_train = train_data.data, train_data.target
X_test, y_test = test_data.data, test_data.target
print(f"훈련 데이터: {len(X_train)}, 훈련 라벨: {len(y_train)}")
print(f"테스트 데이터: {len(X_test)}, 훈련 라벨: {len(y_test)}")

# 3) 샘플 데이터 확인
print('')
print("라벨 종류:")
print(train_data.target_names)
print(f"   샘플 훈련 데이터")
print(X_train[0])
print(f"   샘플 훈련 데이터 라벨")
print(y_train[0], train_data.target_names[y_train[0]])
print()

# 4) 전처리
def preprocessing(text):
    '''
    - 구둣점, 개행문자 제거.
    - 토큰화: 문서~문장~단어, 소문자화.
    - 불용어 제거, 최소 단어 길이 설정.
    - 어간 추출.
    - 품사 태그.
    - 표제어 추출.
    '''
    
    # 구둣점, 개행문자 제거
    temp = ""
    for t in text:
        if t in string.punctuation: # 구둣점 제거
            temp += " "
        else:
            temp += t
    temp = temp.strip('\n') # 개행문자 제거

    # 토큰화, 소문자
    tokens = []
    for sent in nltk.sent_tokenize(temp):
        for word in nltk.word_tokenize(sent):
            tokens.append(word.lower())
    
    # 불용어 제거, 최소 단어 길이 3 설정
    stopwords_list = stopwords.words('english')
    token_list = []
    for token in tokens:
        if token not in stopwords_list and len(token) >= 3:
            token_list.append(token)
    
    # 어간 추출
    stemmer = PorterStemmer()
    try:
        token_list = [stemmer.stem(word) for word in tokens]
    except:
        token_list = token_list
    
    # 품사 태그
    tagged_tokens = pos_tag(tokens)

    # 표제어 추출
    n_tags = ['NN','NNP','NNPS','NNS']
    v_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']

    lemmatizer = WordNetLemmatizer()

    def prat_lemmatize(token, tag):
        if tag in n_tags:
            return lemmatizer.lemmatize(token, 'n')
        elif tag in v_tags:
            return lemmatizer.lemmatize(token, 'v')
        else:
            return lemmatizer.lemmatize(token, 'n')
    
    lemmatized_tokens = [prat_lemmatize(token, tag) for token, tag in tagged_tokens]
    
    # 최종 전처리 텍스트
    preprocessed_text = " ".join(lemmatized_tokens)

    return preprocessed_text

# 데이터 전처리
X_train_preprocessed = []
for train_text in X_train:
    X_train_preprocessed.append(preprocessing(train_text))

X_test_preprocessed = []
for test_text in X_test:
    X_test_preprocessed.append(preprocessing(test_text))

# 전처리 데이터 확인
print(f"전처리된 훈련 데이터: {len(X_train_preprocessed)}")
print(f"전처리된 테스트 데이터: {len(X_test_preprocessed)}")
print()
print(f"   전처리된 샘플 훈련 데이터")
print(X_train_preprocessed[0])
print()

# 5) TFIDF 행렬 생성
vectorizer = TfidfVectorizer(min_df=2,
                             ngram_range=(1, 2),
                             stop_words='english',
                             max_features=10000)

tfidf_vectorizer = vectorizer.fit(X_train_preprocessed)
print(f"Vectorizer:\n {tfidf_vectorizer}\n")
print(f"Vectorizer에 사용된 단어 확인:\n{tfidf_vectorizer.get_feature_names()}\n")
X_train_tfidf = tfidf_vectorizer.transform(X_train_preprocessed).todense() # 밀집행렬
X_test_tfidf = tfidf_vectorizer.transform(X_test_preprocessed).todense()
print(f"훈련 데이터 tfidf: {X_train_tfidf.shape}")
print(f"테스트 데이터 tfidf: {X_test_tfidf.shape}")
print()

# 6) 모델 파라미터
np.random.seed(42) # 시드 고정
n_classes = len(np.unique(y_train)) # 라벨 수
BATCH = int(input('배치 사이즈 설정: '))
EPOCHS = int(input('학습 에폭 수 설정: '))
n_hidden = int(input('은닉 노드 수 설정: '))

# 7) 범주형 변수 원핫인코딩
Y_train = to_categorical(y_train, n_classes)
Y_test = to_categorical(y_test, n_classes)

# 8) 모델 레이어 설정
X_input = Input(batch_shape=(None, X_train_tfidf.shape[1]))
X_hidden = Dense(n_hidden, activation='relu')(X_input)
X_hidden = Dropout(0.5)(X_hidden)
y_output = Dense(n_classes, activation='softmax')(X_hidden)

# 9) 모델 컴파일
model = Model(X_input, y_output)
model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))
print("========= 모델 전체 구조 =========")
print(model.summary())
print()

# 10) 모델 학습
es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)
hist = model.fit(X_train_tfidf, Y_train,
                 batch_size=BATCH,
                 epochs=EPOCHS,
                 validation_data=(X_test_tfidf, Y_test),
                 callbacks=[es])

# 11) plot loss
plt.plot(hist.history['loss'], label='Train Loss')
plt.plot(hist.history['val_loss'], label='Test Loss')
plt.legend()
plt.title('Loss Trajectory')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# 12) 예측 및 결과 확인
y_train_pred = model.predict(X_train_tfidf)
y_train_pred = np.argmax(y_train_pred, axis=1)
y_test_pred = model.predict(X_test_tfidf)
y_test_pred = np.argmax(y_test_pred, axis=1)
print(f"Train Accuracy: {accuracy_score(y_train, y_train_pred)}")
print(f"Test Accuracy: {accuracy_score(y_test, y_test_pred)}")