# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcalWvc2Zxmx11lJbKzh51u9t2QGypZH
"""

# download
!pip install gensim
import nltk
nltk.download('stopwords')

# module import
import numpy as np
import re
import pickle
from nltk.corpus import stopwords
from gensim import corpora
from gensim.models.ldamodel import LdaModel as LDA
import random

# 경로 설정
root_path = "/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님"
data_path = f"{root_path}/dataset"

# 데이터 로드
with open(f"{data_path}/news.data", 'rb') as f:
    news_data = pickle.load(f)

# 데이터 확인
news = news_data.data
print(len(news))
print(news[0])

# 뉴스별 토픽 확인
print(len(news_data.target_names))
print(news_data.target_names)

# 전처리
# 1) 영문자 제외 모두 제거
news_1 = []
for doc in news:
    temp = re.sub("[^a-zA-Z]", " ", doc)
    news_1.append(temp)
# 2) 소문자 변환, 불용어 제거, 길이 3 이하 단어 제거
news_2 = []
stopwords_list = stopwords.words('english')
for doc in news_1:
    temp = []
    for w in doc.split():
        w = w.lower() # 소문자 변환
        if w not in stopwords_list and len(w) > 3: # 불용어가 아니고, 길이가 3을 넘을 경우에만
            temp.append(w) # 임시 배열에 저장
    news_2.append(temp)
# 3) 전처리 결과 확인
print(news_2[0])

# doc2bow 생성
vocab = corpora.Dictionary(news_2)
print(list(vocab.items())[:10]) # 리스트로 확인
news_bow = [vocab.doc2bow(s) for s in news_2]
print(news_bow[0])

# LDA
model = LDA(news_bow,
            num_topics = len(news_data.target_names),
            id2word= vocab)

# 문서별 토픽 번호 확인
doc_topic = model.get_document_topics(news_bow) # topic 리스트
sample_nums = random.sample(list(range(0, 1000)), 10)
for s in sample_nums:
    topics = np.array(doc_topic[s])
    print(topics)
    most_likely_topic = int(topics[np.argmax(topics[:, 1]), 0])
    print(f"문서: {s}, topic: {news_data.target_names[most_likely_topic]}")

# topic-term 행렬에서 topic별 중요 단어 표시: 15개까지
for i in range(len(news_data.target_names)):
    topic_term = model.get_topic_terms(i, topn=15)
    indices = [idx for idx, score in topic_term]
    print(f"토픽{i} {news_data.target_names[i]}")
    for idx in indices:
        print(f"     {vocab[idx]}")
    print(" ")

# 문서별 분류 코드 확인
def check_topic(x, y):
    print("문서 %d의 topic: %s" %(x, news_data.target_names[news_data.target[x]]))
    print("문서 %d의 topic: %s" %(y, news_data.target_names[news_data.target[y]]))

check_topic(0, 314)
check_topic(11300, 2)
check_topic(66, 1)