{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-IMDB-과제.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuXFaEeAFsgh",
        "colab_type": "text"
      },
      "source": [
        "# _1_. 동의어에 대해 Embedding Vector 수치 확인\n",
        "\n",
        "> 유의어, 동의어 구별이 안 된다. 당연히 똑같다.\n",
        "\n",
        "* 동의어 있는 문장 확인\n",
        "    - vocab_dict 생성\n",
        "    - decode 이용해서 동의어 있나 확인\n",
        "    - 동의어 있는 문장 인덱스 저장\n",
        "\n",
        "* Functional API로 임베딩 레이어 구성\n",
        "\n",
        "* Embedding latent feature 추출하여 임베딩된 수치 확인 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr23krhyFnRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈 불러오기\n",
        "\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOYC5ydYHFfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDB 데이터셋 문장 decode\n",
        "def decode_sent(sentences):\n",
        "    '''\n",
        "    - 0: padding, 1: start, 2: OOV\n",
        "    - 실제 word index에서 3을 빼고, 없으면 '*'로 채운다.\n",
        "    '''\n",
        "    temp_sent = []\n",
        "    for sent in sentences:\n",
        "        temp = imdb_vocab_dict.get(sent-3, '*')\n",
        "        temp_sent.append(temp)\n",
        "    \n",
        "    comb_sent = \" \".join(temp_sent)\n",
        "\n",
        "    return comb_sent"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYpj8VffHJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 아무 단어나 문단에서 찾아오는 함수\n",
        "def check_syn(sentences, dictionary, threshold, k=10):\n",
        "\n",
        "    syn_idx = random.randint(3, max_features) # 숫자: 찾을 동의어 인덱스.\n",
        "    synonym = dictionary[syn_idx]\n",
        "    print(f\"찾을 단어 인덱스: {syn_idx}, 찾을 단어: {synonym}\") # 확인용\n",
        "\n",
        "    common_sentences = []\n",
        "\n",
        "    cnt = 0\n",
        "    for i in range(len(sentences)):\n",
        "        if cnt == k:\n",
        "            break\n",
        "        sent = sentences[i]\n",
        "        decoded_sent = decode_sent(sent)        \n",
        "        if synonym in decoded_sent:\n",
        "            print(f\"{i}번째 문장:\\n    {decoded_sent}\") # 확인용\n",
        "            cnt += 1\n",
        "            common_sentences.append(i)\n",
        "\n",
        "    return common_sentences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGj3K5ANwLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 길이 체크\n",
        "def check_len(m, sentences):\n",
        "    cnt = 0\n",
        "    for sent in sentences:\n",
        "        if len(sent) <= m:\n",
        "            cnt += 1\n",
        "    \n",
        "    return f'전체 문장 중 길이가 {m} 이하인 샘플의 비율: {(cnt/len(sentences))*100}'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDyFmFU6NwJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1810ba3f-46f3-4fc9-a7c3-114990638e25"
      },
      "source": [
        "# 데이터 로드\n",
        "max_features = int(input('최대 단어 빈도 설정: '))\n",
        "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# 원본 데이터 보존 및 데이터 준비\n",
        "X_train = X_train_raw.copy()\n",
        "X_test = X_test_raw.copy()\n",
        "print(f\"훈련 데이터: {X_train.shape}, 훈련 라벨: {y_train.shape}\")\n",
        "print(f\"테스트 데이터: {y_train.shape}, 테스트 라벨: {y_test.shape}\")\n",
        "print()\n",
        "\n",
        "# 어휘 사전\n",
        "vocabulary = imdb.get_word_index()\n",
        "imdb_vocab_dict = dict((v, k) for k, v in vocabulary.items())\n",
        "\n",
        "# 문장 패딩 길이 설정\n",
        "for length in range(100, 1000, 50):\n",
        "    print(check_len(length, X_train))\n",
        "print()\n",
        "max_length = int(input('문장 패딩 길이 설정: '))\n",
        "\n",
        "# 패딩\n",
        "X_train = pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_length)\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "BATCH = int(input('배치 사이즈 설정: '))\n",
        "EMBED_DIM = int(input('임베딩 차원 설정: '))\n",
        "n_hidden = int(input('은닉 노드 수 설정: '))\n",
        "EPOCHS = int(input('학습 횟수 설정: '))\n",
        "\n",
        "# 모델 레이어 설정 및 구성\n",
        "X_input = Input(batch_shape=(None, max_length)) # 시퀀스 길이만큼 들어간다\n",
        "X_embed = Embedding(input_dim=max_features, output_dim=EMBED_DIM, input_length=max_length)(X_input)\n",
        "X_lstm = Bidirectional(LSTM(n_hidden))(X_embed) # many to one이므로 return sequences 없음\n",
        "y_output = Dense(1, activation='sigmoid')(X_lstm)\n",
        "\n",
        "# 모델 구성\n",
        "model = Model(X_input, y_output)\n",
        "embed_model = Model(X_input, X_embed) ### 이\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.002))\n",
        "print(\"========== 모델 전체 구조 확인 ==========\")\n",
        "print(model.summary())\n",
        "print()\n",
        "\n",
        "# 임베딩 모델 확인\n",
        "print(\"========== 임베딩 모델 구조 확인 ==========\")\n",
        "print(embed_model.summary())\n",
        "print()\n",
        "\n",
        "# 모델 훈련\n",
        "es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 batch_size=BATCH,\n",
        "                 epochs=EPOCHS,\n",
        "                 validation_data=(X_test, y_test),\n",
        "                 callbacks=[es])\n",
        "\n",
        "# 임베딩 latent feature 확인\n",
        "embedded_sentences = embed_model.predict(X_train)\n",
        "print(f\"임베딩된 문장: {embedded_sentences.shape}\")\n",
        "print()\n",
        "\n",
        "# 동의어 찾기\n",
        "synonyms = check_syn(X_train, imdb_vocab_dict, max_features)\n",
        "print()\n",
        "\n",
        "# 동음이의어 있는 문장 확인: '*'이 많아서 문장으로만 보면 다 이렇게 보인다.\n",
        "for idx in synonyms:\n",
        "    print(decode_sent(X_train_raw[idx]))\n",
        "    print(embedded_sentences[idx, :, :])\n",
        "    print()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최대 단어 빈도 설정: 6000\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "훈련 데이터: (25000,), 훈련 라벨: (25000,)\n",
            "테스트 데이터: (25000,), 테스트 라벨: (25000,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "전체 문장 중 길이가 100 이하인 샘플의 비율: 11.288\n",
            "전체 문장 중 길이가 150 이하인 샘플의 비율: 37.732\n",
            "전체 문장 중 길이가 200 이하인 샘플의 비율: 57.292\n",
            "전체 문장 중 길이가 250 이하인 샘플의 비율: 68.688\n",
            "전체 문장 중 길이가 300 이하인 샘플의 비율: 76.36800000000001\n",
            "전체 문장 중 길이가 350 이하인 샘플의 비율: 81.93599999999999\n",
            "전체 문장 중 길이가 400 이하인 샘플의 비율: 86.064\n",
            "전체 문장 중 길이가 450 이하인 샘플의 비율: 89.184\n",
            "전체 문장 중 길이가 500 이하인 샘플의 비율: 91.56800000000001\n",
            "전체 문장 중 길이가 550 이하인 샘플의 비율: 93.308\n",
            "전체 문장 중 길이가 600 이하인 샘플의 비율: 94.812\n",
            "전체 문장 중 길이가 650 이하인 샘플의 비율: 95.92399999999999\n",
            "전체 문장 중 길이가 700 이하인 샘플의 비율: 96.72800000000001\n",
            "전체 문장 중 길이가 750 이하인 샘플의 비율: 97.432\n",
            "전체 문장 중 길이가 800 이하인 샘플의 비율: 98.012\n",
            "전체 문장 중 길이가 850 이하인 샘플의 비율: 98.5\n",
            "전체 문장 중 길이가 900 이하인 샘플의 비율: 98.832\n",
            "전체 문장 중 길이가 950 이하인 샘플의 비율: 99.136\n",
            "\n",
            "문장 패딩 길이 설정: 700\n",
            "배치 사이즈 설정: 32\n",
            "임베딩 차원 설정: 60\n",
            "은닉 노드 수 설정: 128\n",
            "학습 횟수 설정: 1000\n",
            "========== 모델 전체 구조 확인 ==========\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 700)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 700, 60)           360000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               193536    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 553,793\n",
            "Trainable params: 553,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "========== 임베딩 모델 구조 확인 ==========\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 700)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 700, 60)           360000    \n",
            "=================================================================\n",
            "Total params: 360,000\n",
            "Trainable params: 360,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Epoch 1/1000\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.6179 - val_loss: 0.5668\n",
            "Epoch 2/1000\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.4035 - val_loss: 0.3479\n",
            "Epoch 3/1000\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.2595 - val_loss: 0.3430\n",
            "Epoch 4/1000\n",
            "782/782 [==============================] - 70s 90ms/step - loss: 0.1994 - val_loss: 0.2920\n",
            "Epoch 5/1000\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 0.1534 - val_loss: 0.3391\n",
            "Epoch 6/1000\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.1230 - val_loss: 0.3830\n",
            "Epoch 7/1000\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.0941 - val_loss: 0.4448\n",
            "Epoch 8/1000\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.0747 - val_loss: 0.4357\n",
            "Epoch 9/1000\n",
            "782/782 [==============================] - 72s 93ms/step - loss: 0.0630 - val_loss: 0.5617\n",
            "Epoch 00009: early stopping\n",
            "임베딩된 문장: (25000, 700, 60)\n",
            "찾을 단어 인덱스: 4749, 찾을 단어: alternate\n",
            "91번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * blood legacy starts with the arrival of lawyer tom drake norman * to the dean estate * * by the now deceased christopher dean john carradine upon his arrival he is * by mr * four children gregory jeff * his wife laura * * victoria faith * johnny richard * plus leslie * * her * carl * john smith drake plays a tape recording of they're late father's wishes after his death the estate worth * million dollars is to be split equally between his four children if any should die then the money would be split equally between the rest if all were to die the * * * * * * buck * the more * named frank john russell would * the lot well not satisfied with a * share of * million which is still almost 35 million back in 1971 which doesn't sound too bad to me someone decides they want it all for themselves it's not long before * heads are turning up in the * br br co written produced directed by roy * blood legacy disappointed me on two * for * this film's alternate much more common title is legacy of blood which is also the title of an obscure horror film directed by andy * back in * which i've always wanted to see both films are * mixed up as both have similar stories when i checked my on screen cable tv guide for legacy of blood i was excited because it said it was the * film even listed him as director so when i actually sat down to watch it i heard john * voice i then knew it wasn't the * film that i had wanted to see my heart * then of course there's the simple yet * straight forward fact that blood legacy is a total utter piece of crap that is literally painful to watch at times the script by * eric * is slow boring extremely predictable the character's are absolutely bizarre in an annoying way the freak of a servant who * his sister to * him the strange set of brother's sisters who are just downright * so far removed from reality that any tension or mystery that the simplistic * story could have achieved is sorely missing then there's the awful twist ending that you can guess within the first 10 minutes it's boring to watch it's poorly paced it's just a * to even think about it please someone save me as this is really bad stuff i could go on all day about how bad blood legacy is i really could br br director * was either working with a none existent budget or judging by this he shouldn't have even been directing traffic the entire film looks ugly it's poorly photographed there is no atmosphere or scares the blood gore is tame there's an * in a head a * head a scene when someone is * to death by * the best murder when someone's face is eaten by * however there are question marks over this scene so there's the victim right there's the tank of * right victims head is placed in * tank right * eat victims face right water remains crystal clear despite said victim having his face eaten * * the blood br br technically blood legacy is terrible it looks awful the sound was obviously shot live it's * hard to hear which considering the terrible dialogue is maybe a * in disguise the acting was not going to win anyone any awards that's for sure the least said about it the better br br blood legacy is an awful film there really isn't a single positive aspect to it or if there is i can't think of it do yourself a favour don't bother with this one there are much better films out there\n",
            "633번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * just recently i've been * over and * this movie so much that i almost had to see it well having just seen it today the 5 8 rating is completely understandable i think that if you * something so much that it becomes a dire need it turns out not to be worth it br br sure the hills have eyes 2 has its moments it has a very cool and well developed storyline that ties in well with the actual product itself but the whole thing is so self * that it becomes so hard to follow and if it weren't for wes * production on this film it wouldn't be anything to do with the original remake br br but the whole thing makes you go is this supposed to be horror or comedy because there are lots of ridiculous randomly placed jump moments and stupid one liners i e there's a hand in the sh * or you * * i'll kill you all damn sons of b * and the acting god don't even remind me how bad it was br br storyline this part contains spoilers beware the movie begins with a woman giving birth to a * baby * la la and then the screen * to black with the movie's title appearing and a * then we go to this office where there are randomly placed war veteran * we find that this is for this one scientist keeping track of people looking for * the box to keep track of audio * is gone and everyone dies after that tone setting opening you'd expect more br br then we go to this one team of military * training in * as the captain * them a good job at stupidity their last day of training is in new mexico the desert where the family in the last * had stayed because they were stuck while in training things go ultimately wrong people die and do i need to tell you any more because right now i have the attention * of a * just forcing myself to sit here and type this br br the thing that's wrong with * is that it just * work no flashbacks here and the ending is pretty safe but with a twist a stupid one that is i'm pretty sure the ultra super director's cut with a * cover and a ticket to the hills have eyes 3 will showcase all of it's alternate endings but at this point i'm not sure if i care br br so by all means if you loved the first * so much it's almost a sin not to see this then by all means see it but if else then avoid at all costs it's for your own good br br 3 10\n",
            "1465번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * no spoilers just plot details i can't understand such hatred for this episode you want to watch a bad episode of * watch * now there's a * of crap tom * gives a good performance i don't say that very often and michael * is great but he is most of the time the alternate universe scenario seems * realistic the * * who previously appeared in static returns and tells clark that the doctor that is the head of the insane asylum where they are being held at is actually a phantom from the phantom zone and if clark wants to return to his universe he must kill him an overall great episode with good acting and a decent pace\n",
            "2945번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * i * kept myself awake late last night watching this thing about the only thing i could say good about this horrid film is that it could be used by film schools to show how not to make a movie no proper character development wait i'm not even sure they were characters set ups were hokey and inane and the * of split screens was wasted since sometimes they couldn't even * with alternate shots if i could give this a zero or minus rating i would sadly it isn't even worth the time for a few laughs br br it's just a sad example of money wasted by hollywood and now i waste my time even thinking about it\n",
            "2966번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * somewhere near the bottom of the film studio ladder you can find companies like u f o * and beneath them lie * cinema br br * is a direct to video production company that * in lesbian * non hardcore erotic movies it has developed a very dedicated fan base that purchase each new title as they are released but sadly the company has become too closely associated with frequent star * * i say sadly because recent mainstream interest and her appearance on the show masters of horrors has caused her to set her * a little higher than the zero budget s c efforts which forces the company to find a new identity but back in their glory days they released this film on a very * world br br the gorgeous * * is forced to attend a * school at the * of a absent father at the school she meets her * hot room mate played by ruby * who immediately has designs on her but the * barbara * has other plans in typical s c style the movie stops every ten minutes for a extended sex scene but unlike most of their efforts this one has a somewhat interesting story and a couple of good performances ms * appears to be having a great time as the sexual * who views * as a * * and * caine makes a welcome though brief appearance as satan this is the sort of film that * franco would * out in the 70s although this one does not have the hardcore sex that franco was always willing to throw in for foreign * and fans of that * work would be wise to give this one a go br br to me as a long time zero budget cinema fan and * * i came across the * cinema films through their parody films * of the apes who wants to be a erotic * but i actually prefer their more original works you either see past the low budget and occasionally weak acting or you get hung up on these things and just hate all of these films for me the most obvious thing that * these no budget movies is a real sense of fun these low budget companies are able to create their own unique style which gives the viewer something very different from the bland by the numbers studio efforts that load up the * br br if you have never seen a * cinema film either this or sin sisters featuring both of the * sisters are excellent choices to begin with this one is a fun fast paced film although the frequent * shots of the school do get a little old and the dvd is totally loaded with extras including a ton of previews of other company * a great behind the scenes * and some deleted scenes including a alternate opening i do recommend you pass on the * bonus feature the first film by director as it is quite weak and not really worth viewing\n",
            "3048번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * not really spoilers in my opinion but i wanted to cover myself nevertheless as the executive producer morgan freeman wants the audience to ignore the numerous * of his character in 10 items or less a movie with an * indie feel and just be absorbed in the * be all that you can be theme he plays a alternate universe semi * up version of the real morgan freeman who is * in an old * by a kid all the way into * * from * to research his next movie role why * is a mystery to so * * he could have saved the trip and gone anywhere in the san * valley and found the same elements * * is pretty to watch a cross between * * and * * playing a * * * at a large but slow local market that apparently is the ultimate source for * * research his character is only known as him to * to how actors are regarded when * in real life by average people * that's * ' etc unfortunately i was too * that him had all kinds of * wisdom and advice but had no * return back to his home in * carried no cash or * card or had the wisdom to keep a cell phone with him if one has such a high opinion of their self that they believe they * an answer to everything like him does then i gotta see cash and a * which displays intelligence and good survival * to * that big ego which him definitely has nothing really happens in this movie i don't believe that either of the main character's were * changed by their encounter with each other it * with the idea of * but then that thought * this to me was similar to steve * * without the sexual affair it was self indulgent for freeman and unconvincing to the audience\n",
            "3473번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * this juvenile bland flick is strictly for teenagers in old * bodies desperate to * their * challenged teenage years how by burning up gas and * a fast * car or plane with freedom br br the plot * heavily from * * neighborhood if it were run my an oil * and * run if it were heavily * and lacked a clear sense of style br br starring lee * and * * this film is set in a post gas crisis world in which an all powerful government doesn't want you to * drive your car and burn gas sort of the opposite of today's * and bush oil * * pushing government br br this * alone makes the film laughable but wait there's more although the film is set in the future we're not shown any signs of future technology beyond a return to * * * and horses you will believe that the future looks exactly like today same clothing same * houses same green * as today and when the film was made there are no * * no * no * to alternate energy br br the acting is flat and * even scenes which could have been gritty or moving buddy flick honor romance horror all fall * than a paper doll under a * br br continuity is lacking the jet * by * * character changes colors and * from moment to moment as the filmmakers insult our intelligence with * stock footage again and again br br the plot is as moronic and only half as exciting as a * of * episode br br even die hard car film and * fans should avoid this film like month old * unless you enjoy * * * trying to make a movie as empty as the * gas tank\n",
            "3701번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * oscar * comedy of * perhaps the * play ever written is all but * at the hands of a second rate cast * is as one would expect * * brilliant in the role of lord * but the rest of the cast makes the entire * a waste of time jean * attempts a stage accent in alternate * and the other members of the cast seem to believe this is a melodrama and not a comedy indeed the entire production has * that * it to tragedy * the * office * * direction seems to lie mostly in making sure that there are plenty of * about and even the music seems banal stick with the visually perfect silent farce as directed by * or even the 2004 screen version with helen hunt as mrs * or try reading the play for the pleasure of the words but skip this version\n",
            "3973번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * also known in a different form as house of * this messy br br little film takes itself so seriously as to kill any entertainment value br br whatsoever br br the spare plot involves european * * * who has a br br chance run in with * * who looks just like the devil she br br saw on a * in the square * is given a ride to a br br mysterious house in the country where * happens to be br br butler there she is mistaken for a long dead woman and the real br br soap opera * begin the * blind * br br husband had an affair with the dead woman who was the br br * * * the couple who gave * the ride br br well the woman is giving the * uh back seat driving br br lessons and the husband knows and does not care eventually br br most of the cast is killed * is * and raped br br escapes and the viewer is taken to a climax on board an empty br br airplane which must have * the empty theaters this br br thing played in br br the alternate version of this house of * has scenes br br added involving a priest br br the vhs copy of this from elite entertainment is crystal clear and br br * there are extras after the end credits deleted sex br br and gore scenes br br mario * direction is fast and furious but his screenplay is br br awful there are half * ideas abandoned * and br br stunning * that do nothing more than * this thing br br in some sort of forward direction you have life like * for br br practice * the blind * does not act all that blind br br and * is given the same * he had in * who br br * ya baby br br the project seems like they had two name stars then wrote the br br script quickly something that happens in hollywood on a daily br br * now * looks completely lost delivering his br br lines * and wishing his character had not died in the dirty br br dozen * runs around and screams and * a lot but br br her character is a blank i use the term character loosely the br br only thing we know about her is her name br br this is a real weird film and your reaction to it might * on br br how heavily you are into * and * i for one cannot br br * lisa and the devil br br this is * and including all the extras at the end of the vhs br br copy contains strong physical violence sexual violence strong br br gore strong female nudity male nudity sexual content and adult br br situations\n",
            "4850번째 문장:\n",
            "    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * this one is considered a key pre code film  from the director who later made the musical * the * story * but also the * sci fi invasion u s a *  and features one of barbara * best early roles br br she's supported by a fine cast which includes popular actors and * character performers of the day  george * * * edward van * * * and john wayne at one point * stanwyck with the * * * from a popular song which is heard constantly throughout in the former category and in the latter robert * as * father donald cook as her most tragic * * * as her elderly *  more on this later arthur * as a * politician and henry * as * boss and father in law whom stanwyck also * * scenes in which walter * appeared were * deleted at his own * when the film ran into trouble with the * br br * by * i e typically hard * dialogue and realistic * * sets the narrative contains unexpected * of * philosophy fed to our small town heroine by the intellectual * stanwyck * to him early on that she's no ball of fire which of course * her later comedy  directed by howard * and co starring gary cooper  of that name under * * she quickly * into an essentially * character determined that nothing shall stand in her path to success the symbolic depiction of her rise in * at the new york firm she's eventually employed with is reminiscent of a similarly * one  * to an ambitious * lust for power  in * * * * * it's also interesting that stanwyck is constantly seen * her neck out for her black maid companion br br the first two * of the film are simply terrific at first i found the latter stages somewhat disappointing  because i was expecting to see stanwyck get her * by falling for the * introduced george * character while he * * just like the others he's soon under her spell on second viewing however this aspect felt less *  as it's evident that stanwyck has been affected by the two deaths her selfish behavior has caused and that her * in paris has * her even if she tries to * to her hard earned wealth for as long as it's possible br br released on dvd by * as part of their forbidden hollywood * 1 collection the film is presented in two * different *  a recently * pre release version and the * theatrical release print among the considerable footage cut from the latter is dialogue * to * life as a * from the age of 14 though it's heard in the * trailer while many other scenes have been * i e * for content the violent * which develops between stanwyck and * after she * his * the * at the * car the scene in which * is surprised with stanwyck by cook the shooting followed by a suicide only shots are heard in the shorter version stanwyck thinking about her * while the * is playing again only * appears in the version released to theaters etc * latter then * alternate takes for some scenes  and includes an * shot of the city which is missing from the longer version however we also get an obviously * on happy ending the pre release version * * on a very effective open ended note and an equally unconvincing * letter sent by * to stanwyck in new york which basically has the function of * all references to *\n",
            "* blood legacy starts with the arrival of lawyer tom drake norman * to the dean estate * * by the now deceased christopher dean john carradine upon his arrival he is * by mr * four children gregory jeff * his wife laura * * victoria faith * johnny richard * plus leslie * * her * carl * john smith drake plays a tape recording of they're late father's wishes after his death the estate worth * million dollars is to be split equally between his four children if any should die then the money would be split equally between the rest if all were to die the * * * * * * buck * the more * named frank john russell would * the lot well not satisfied with a * share of * million which is still almost 35 million back in 1971 which doesn't sound too bad to me someone decides they want it all for themselves it's not long before * heads are turning up in the * br br co written produced directed by roy * blood legacy disappointed me on two * for * this film's alternate much more common title is legacy of blood which is also the title of an obscure horror film directed by andy * back in * which i've always wanted to see both films are * mixed up as both have similar stories when i checked my on screen cable tv guide for legacy of blood i was excited because it said it was the * film even listed him as director so when i actually sat down to watch it i heard john * voice i then knew it wasn't the * film that i had wanted to see my heart * then of course there's the simple yet * straight forward fact that blood legacy is a total utter piece of crap that is literally painful to watch at times the script by * eric * is slow boring extremely predictable the character's are absolutely bizarre in an annoying way the freak of a servant who * his sister to * him the strange set of brother's sisters who are just downright * so far removed from reality that any tension or mystery that the simplistic * story could have achieved is sorely missing then there's the awful twist ending that you can guess within the first 10 minutes it's boring to watch it's poorly paced it's just a * to even think about it please someone save me as this is really bad stuff i could go on all day about how bad blood legacy is i really could br br director * was either working with a none existent budget or judging by this he shouldn't have even been directing traffic the entire film looks ugly it's poorly photographed there is no atmosphere or scares the blood gore is tame there's an * in a head a * head a scene when someone is * to death by * the best murder when someone's face is eaten by * however there are question marks over this scene so there's the victim right there's the tank of * right victims head is placed in * tank right * eat victims face right water remains crystal clear despite said victim having his face eaten * * the blood br br technically blood legacy is terrible it looks awful the sound was obviously shot live it's * hard to hear which considering the terrible dialogue is maybe a * in disguise the acting was not going to win anyone any awards that's for sure the least said about it the better br br blood legacy is an awful film there really isn't a single positive aspect to it or if there is i can't think of it do yourself a favour don't bother with this one there are much better films out there\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [-0.08150015 -0.03898849  0.02707418 ... -0.03891219 -0.08100365\n",
            "  -0.03683342]\n",
            " [-0.02508494  0.12995604 -0.00357257 ... -0.07297321  0.01810711\n",
            "  -0.00957778]\n",
            " [ 0.04817886  0.08801343  0.06744438 ... -0.03802183 -0.02157363\n",
            "  -0.01425271]]\n",
            "\n",
            "* just recently i've been * over and * this movie so much that i almost had to see it well having just seen it today the 5 8 rating is completely understandable i think that if you * something so much that it becomes a dire need it turns out not to be worth it br br sure the hills have eyes 2 has its moments it has a very cool and well developed storyline that ties in well with the actual product itself but the whole thing is so self * that it becomes so hard to follow and if it weren't for wes * production on this film it wouldn't be anything to do with the original remake br br but the whole thing makes you go is this supposed to be horror or comedy because there are lots of ridiculous randomly placed jump moments and stupid one liners i e there's a hand in the sh * or you * * i'll kill you all damn sons of b * and the acting god don't even remind me how bad it was br br storyline this part contains spoilers beware the movie begins with a woman giving birth to a * baby * la la and then the screen * to black with the movie's title appearing and a * then we go to this office where there are randomly placed war veteran * we find that this is for this one scientist keeping track of people looking for * the box to keep track of audio * is gone and everyone dies after that tone setting opening you'd expect more br br then we go to this one team of military * training in * as the captain * them a good job at stupidity their last day of training is in new mexico the desert where the family in the last * had stayed because they were stuck while in training things go ultimately wrong people die and do i need to tell you any more because right now i have the attention * of a * just forcing myself to sit here and type this br br the thing that's wrong with * is that it just * work no flashbacks here and the ending is pretty safe but with a twist a stupid one that is i'm pretty sure the ultra super director's cut with a * cover and a ticket to the hills have eyes 3 will showcase all of it's alternate endings but at this point i'm not sure if i care br br so by all means if you loved the first * so much it's almost a sin not to see this then by all means see it but if else then avoid at all costs it's for your own good br br 3 10\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [-0.00537288  0.128286    0.04025614 ... -0.03174814  0.00237459\n",
            "   0.01773592]\n",
            " [ 0.15826537 -0.21981552 -0.26211655 ...  0.32085392  0.29203877\n",
            "   0.07361341]\n",
            " [-0.21720976 -0.16202213  0.38919088 ... -0.12444372 -0.71228313\n",
            "   0.02864715]]\n",
            "\n",
            "* no spoilers just plot details i can't understand such hatred for this episode you want to watch a bad episode of * watch * now there's a * of crap tom * gives a good performance i don't say that very often and michael * is great but he is most of the time the alternate universe scenario seems * realistic the * * who previously appeared in static returns and tells clark that the doctor that is the head of the insane asylum where they are being held at is actually a phantom from the phantom zone and if clark wants to return to his universe he must kill him an overall great episode with good acting and a decent pace\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.01034986  0.11708627  0.0774816  ... -0.01097842 -0.02076415\n",
            "  -0.04901885]\n",
            " [ 0.11507132  0.12609711  0.03838791 ... -0.04275563 -0.01373793\n",
            "   0.05394944]\n",
            " [-0.15906355  0.20837028  0.22956936 ... -0.19451551 -0.11419427\n",
            "  -0.14325605]]\n",
            "\n",
            "* i * kept myself awake late last night watching this thing about the only thing i could say good about this horrid film is that it could be used by film schools to show how not to make a movie no proper character development wait i'm not even sure they were characters set ups were hokey and inane and the * of split screens was wasted since sometimes they couldn't even * with alternate shots if i could give this a zero or minus rating i would sadly it isn't even worth the time for a few laughs br br it's just a sad example of money wasted by hollywood and now i waste my time even thinking about it\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.11094615 -0.12720446 -0.15028615 ...  0.11706422  0.10991644\n",
            "   0.09323204]\n",
            " [ 0.05547794  0.00632835 -0.0464522  ... -0.01640957 -0.01061692\n",
            "   0.0090123 ]\n",
            " [-0.04670717  0.0169902   0.08957171 ... -0.08736628 -0.00813311\n",
            "  -0.08838389]]\n",
            "\n",
            "* somewhere near the bottom of the film studio ladder you can find companies like u f o * and beneath them lie * cinema br br * is a direct to video production company that * in lesbian * non hardcore erotic movies it has developed a very dedicated fan base that purchase each new title as they are released but sadly the company has become too closely associated with frequent star * * i say sadly because recent mainstream interest and her appearance on the show masters of horrors has caused her to set her * a little higher than the zero budget s c efforts which forces the company to find a new identity but back in their glory days they released this film on a very * world br br the gorgeous * * is forced to attend a * school at the * of a absent father at the school she meets her * hot room mate played by ruby * who immediately has designs on her but the * barbara * has other plans in typical s c style the movie stops every ten minutes for a extended sex scene but unlike most of their efforts this one has a somewhat interesting story and a couple of good performances ms * appears to be having a great time as the sexual * who views * as a * * and * caine makes a welcome though brief appearance as satan this is the sort of film that * franco would * out in the 70s although this one does not have the hardcore sex that franco was always willing to throw in for foreign * and fans of that * work would be wise to give this one a go br br to me as a long time zero budget cinema fan and * * i came across the * cinema films through their parody films * of the apes who wants to be a erotic * but i actually prefer their more original works you either see past the low budget and occasionally weak acting or you get hung up on these things and just hate all of these films for me the most obvious thing that * these no budget movies is a real sense of fun these low budget companies are able to create their own unique style which gives the viewer something very different from the bland by the numbers studio efforts that load up the * br br if you have never seen a * cinema film either this or sin sisters featuring both of the * sisters are excellent choices to begin with this one is a fun fast paced film although the frequent * shots of the school do get a little old and the dvd is totally loaded with extras including a ton of previews of other company * a great behind the scenes * and some deleted scenes including a alternate opening i do recommend you pass on the * bonus feature the first film by director as it is quite weak and not really worth viewing\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.00303219  0.14824899  0.02981434 ...  0.05130301  0.01336381\n",
            "  -0.01727957]\n",
            " [-0.20394157 -0.20378253  0.08454242 ... -0.11822353 -0.00677298\n",
            "  -0.11580393]\n",
            " [ 0.03207689  0.26465836  0.08475356 ... -0.13228641 -0.0745907\n",
            "  -0.08361869]]\n",
            "\n",
            "* not really spoilers in my opinion but i wanted to cover myself nevertheless as the executive producer morgan freeman wants the audience to ignore the numerous * of his character in 10 items or less a movie with an * indie feel and just be absorbed in the * be all that you can be theme he plays a alternate universe semi * up version of the real morgan freeman who is * in an old * by a kid all the way into * * from * to research his next movie role why * is a mystery to so * * he could have saved the trip and gone anywhere in the san * valley and found the same elements * * is pretty to watch a cross between * * and * * playing a * * * at a large but slow local market that apparently is the ultimate source for * * research his character is only known as him to * to how actors are regarded when * in real life by average people * that's * ' etc unfortunately i was too * that him had all kinds of * wisdom and advice but had no * return back to his home in * carried no cash or * card or had the wisdom to keep a cell phone with him if one has such a high opinion of their self that they believe they * an answer to everything like him does then i gotta see cash and a * which displays intelligence and good survival * to * that big ego which him definitely has nothing really happens in this movie i don't believe that either of the main character's were * changed by their encounter with each other it * with the idea of * but then that thought * this to me was similar to steve * * without the sexual affair it was self indulgent for freeman and unconvincing to the audience\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.01818206  0.08972902 -0.0088966  ... -0.05172307 -0.06892612\n",
            "  -0.07383515]\n",
            " [-0.02972519  0.10499098  0.07766188 ... -0.03932367 -0.02815625\n",
            "  -0.03815149]\n",
            " [ 0.047514   -0.08432466 -0.05940554 ...  0.10255075  0.00453741\n",
            "   0.22274849]]\n",
            "\n",
            "* this juvenile bland flick is strictly for teenagers in old * bodies desperate to * their * challenged teenage years how by burning up gas and * a fast * car or plane with freedom br br the plot * heavily from * * neighborhood if it were run my an oil * and * run if it were heavily * and lacked a clear sense of style br br starring lee * and * * this film is set in a post gas crisis world in which an all powerful government doesn't want you to * drive your car and burn gas sort of the opposite of today's * and bush oil * * pushing government br br this * alone makes the film laughable but wait there's more although the film is set in the future we're not shown any signs of future technology beyond a return to * * * and horses you will believe that the future looks exactly like today same clothing same * houses same green * as today and when the film was made there are no * * no * no * to alternate energy br br the acting is flat and * even scenes which could have been gritty or moving buddy flick honor romance horror all fall * than a paper doll under a * br br continuity is lacking the jet * by * * character changes colors and * from moment to moment as the filmmakers insult our intelligence with * stock footage again and again br br the plot is as moronic and only half as exciting as a * of * episode br br even die hard car film and * fans should avoid this film like month old * unless you enjoy * * * trying to make a movie as empty as the * gas tank\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.03180186  0.02782192  0.02231157 ... -0.00908176 -0.04812677\n",
            "  -0.00399159]\n",
            " [-0.01843156  0.05498071  0.14649165 ... -0.09231646 -0.1878079\n",
            "   0.03802622]\n",
            " [-0.00538088 -0.00374571  0.02748644 ...  0.0162336  -0.04268054\n",
            "   0.01029397]]\n",
            "\n",
            "* oscar * comedy of * perhaps the * play ever written is all but * at the hands of a second rate cast * is as one would expect * * brilliant in the role of lord * but the rest of the cast makes the entire * a waste of time jean * attempts a stage accent in alternate * and the other members of the cast seem to believe this is a melodrama and not a comedy indeed the entire production has * that * it to tragedy * the * office * * direction seems to lie mostly in making sure that there are plenty of * about and even the music seems banal stick with the visually perfect silent farce as directed by * or even the 2004 screen version with helen hunt as mrs * or try reading the play for the pleasure of the words but skip this version\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [ 0.26377225 -0.24806365 -0.2756912  ...  0.24529365  0.18501459\n",
            "   0.2584003 ]\n",
            " [ 0.01851839  0.1231242  -0.01561094 ...  0.01668824  0.00538373\n",
            "  -0.00407268]\n",
            " [-0.01198106 -0.1485878   0.06102163 ...  0.04601277 -0.04527538\n",
            "  -0.06931433]]\n",
            "\n",
            "* also known in a different form as house of * this messy br br little film takes itself so seriously as to kill any entertainment value br br whatsoever br br the spare plot involves european * * * who has a br br chance run in with * * who looks just like the devil she br br saw on a * in the square * is given a ride to a br br mysterious house in the country where * happens to be br br butler there she is mistaken for a long dead woman and the real br br soap opera * begin the * blind * br br husband had an affair with the dead woman who was the br br * * * the couple who gave * the ride br br well the woman is giving the * uh back seat driving br br lessons and the husband knows and does not care eventually br br most of the cast is killed * is * and raped br br escapes and the viewer is taken to a climax on board an empty br br airplane which must have * the empty theaters this br br thing played in br br the alternate version of this house of * has scenes br br added involving a priest br br the vhs copy of this from elite entertainment is crystal clear and br br * there are extras after the end credits deleted sex br br and gore scenes br br mario * direction is fast and furious but his screenplay is br br awful there are half * ideas abandoned * and br br stunning * that do nothing more than * this thing br br in some sort of forward direction you have life like * for br br practice * the blind * does not act all that blind br br and * is given the same * he had in * who br br * ya baby br br the project seems like they had two name stars then wrote the br br script quickly something that happens in hollywood on a daily br br * now * looks completely lost delivering his br br lines * and wishing his character had not died in the dirty br br dozen * runs around and screams and * a lot but br br her character is a blank i use the term character loosely the br br only thing we know about her is her name br br this is a real weird film and your reaction to it might * on br br how heavily you are into * and * i for one cannot br br * lisa and the devil br br this is * and including all the extras at the end of the vhs br br copy contains strong physical violence sexual violence strong br br gore strong female nudity male nudity sexual content and adult br br situations\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [-0.00537288  0.128286    0.04025614 ... -0.03174814  0.00237459\n",
            "   0.01773592]\n",
            " [-0.00537288  0.128286    0.04025614 ... -0.03174814  0.00237459\n",
            "   0.01773592]\n",
            " [ 0.03453939 -0.1476073  -0.01923867 ...  0.05214612 -0.00757968\n",
            "   0.1046307 ]]\n",
            "\n",
            "* this one is considered a key pre code film  from the director who later made the musical * the * story * but also the * sci fi invasion u s a *  and features one of barbara * best early roles br br she's supported by a fine cast which includes popular actors and * character performers of the day  george * * * edward van * * * and john wayne at one point * stanwyck with the * * * from a popular song which is heard constantly throughout in the former category and in the latter robert * as * father donald cook as her most tragic * * * as her elderly *  more on this later arthur * as a * politician and henry * as * boss and father in law whom stanwyck also * * scenes in which walter * appeared were * deleted at his own * when the film ran into trouble with the * br br * by * i e typically hard * dialogue and realistic * * sets the narrative contains unexpected * of * philosophy fed to our small town heroine by the intellectual * stanwyck * to him early on that she's no ball of fire which of course * her later comedy  directed by howard * and co starring gary cooper  of that name under * * she quickly * into an essentially * character determined that nothing shall stand in her path to success the symbolic depiction of her rise in * at the new york firm she's eventually employed with is reminiscent of a similarly * one  * to an ambitious * lust for power  in * * * * * it's also interesting that stanwyck is constantly seen * her neck out for her black maid companion br br the first two * of the film are simply terrific at first i found the latter stages somewhat disappointing  because i was expecting to see stanwyck get her * by falling for the * introduced george * character while he * * just like the others he's soon under her spell on second viewing however this aspect felt less *  as it's evident that stanwyck has been affected by the two deaths her selfish behavior has caused and that her * in paris has * her even if she tries to * to her hard earned wealth for as long as it's possible br br released on dvd by * as part of their forbidden hollywood * 1 collection the film is presented in two * different *  a recently * pre release version and the * theatrical release print among the considerable footage cut from the latter is dialogue * to * life as a * from the age of 14 though it's heard in the * trailer while many other scenes have been * i e * for content the violent * which develops between stanwyck and * after she * his * the * at the * car the scene in which * is surprised with stanwyck by cook the shooting followed by a suicide only shots are heard in the shorter version stanwyck thinking about her * while the * is playing again only * appears in the version released to theaters etc * latter then * alternate takes for some scenes  and includes an * shot of the city which is missing from the longer version however we also get an obviously * on happy ending the pre release version * * on a very effective open ended note and an equally unconvincing * letter sent by * to stanwyck in new york which basically has the function of * all references to *\n",
            "[[ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " [ 0.02393688  0.02826602  0.14358342 ... -0.1313001   0.03435238\n",
            "   0.02101756]\n",
            " ...\n",
            " [-0.03503944 -0.03880826  0.03772501 ...  0.01494298 -0.06522223\n",
            "  -0.03481589]\n",
            " [ 0.01818206  0.08972902 -0.0088966  ... -0.05172307 -0.06892612\n",
            "  -0.07383515]\n",
            " [ 0.03180186  0.02782192  0.02231157 ... -0.00908176 -0.04812677\n",
            "  -0.00399159]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKrTsugGf3xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 가중치 행렬을 찍고, 원핫 순서니까 father, mother 찾고 유사도 측정을 해봅시다!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ghI_xxWDpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdOy0dHVSOQ-",
        "colab_type": "text"
      },
      "source": [
        "## 유사도 측정?\n",
        "\n",
        "## 1번 문제 수정되었습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coSUoAkuV7xY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_0p7XlSV7vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OabkboJ5SRQ4",
        "colab_type": "text"
      },
      "source": [
        "# _2_. 두 모델 구성\n",
        "\n",
        "* A : IMDB -> CNN용 임베딩/ LSTM용 임베딩 -> 감성분석\n",
        "* B : IMDB -> 임베딩 -> CNN, LSTM 모델 -> 감성분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHk2vUfNSwWt",
        "colab_type": "text"
      },
      "source": [
        "## 2.0. 모듈, 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AL1w_gmSuC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈 불러오기\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D # CNN\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM # LSTM\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BImynSFTi0YQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c11abc00-a463-4b20-83d1-4b60d9c18789"
      },
      "source": [
        "# 데이터 불러 오기\n",
        "max_features = int(input('최대 단어 빈도 설정: '))\n",
        "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최대 단어 빈도 설정: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay0VrwZ1WqIO",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. 두 모델 공통 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGkuazLvWp1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1dc2e83c-2366-4832-db21-7d29bd5acb1b"
      },
      "source": [
        "# 공통 파라미터\n",
        "BATCH = int(input('배치 사이즈 설정: '))\n",
        "n_embed = int(input('임베딩 차원 설정: '))\n",
        "n_hidden = int(input('은닉 노드 수 설정: '))\n",
        "EPOCHS = int(input('학습 횟수 설정: '))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "배치 사이즈 설정: 32\n",
            "임베딩 차원 설정: 60\n",
            "은닉 노드 수 설정: 64\n",
            "학습 횟수 설정: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwp-yaOhUSvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "7aa46b4d-4780-4557-8dbf-60885b9d673c"
      },
      "source": [
        "# 최대 문장 길이 설정\n",
        "def check_len(m, sentences):\n",
        "    cnt = 0\n",
        "    for sent in sentences:\n",
        "        if len(sent) <= m:\n",
        "            cnt += 1\n",
        "    \n",
        "    return f'전체 문장 중 길이가 {m} 이하인 샘플의 비율: {(cnt/len(sentences))*100}'\n",
        "\n",
        "# 문장 길이 설정: 패딩\n",
        "for length in range(100, 1000, 50):\n",
        "    print(check_len(length, X_train_raw))\n",
        "print()\n",
        "max_length = int(input('문장 길이 설정: '))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 문장 중 길이가 100 이하인 샘플의 비율: 11.288\n",
            "전체 문장 중 길이가 150 이하인 샘플의 비율: 37.732\n",
            "전체 문장 중 길이가 200 이하인 샘플의 비율: 57.292\n",
            "전체 문장 중 길이가 250 이하인 샘플의 비율: 68.688\n",
            "전체 문장 중 길이가 300 이하인 샘플의 비율: 76.36800000000001\n",
            "전체 문장 중 길이가 350 이하인 샘플의 비율: 81.93599999999999\n",
            "전체 문장 중 길이가 400 이하인 샘플의 비율: 86.064\n",
            "전체 문장 중 길이가 450 이하인 샘플의 비율: 89.184\n",
            "전체 문장 중 길이가 500 이하인 샘플의 비율: 91.56800000000001\n",
            "전체 문장 중 길이가 550 이하인 샘플의 비율: 93.308\n",
            "전체 문장 중 길이가 600 이하인 샘플의 비율: 94.812\n",
            "전체 문장 중 길이가 650 이하인 샘플의 비율: 95.92399999999999\n",
            "전체 문장 중 길이가 700 이하인 샘플의 비율: 96.72800000000001\n",
            "전체 문장 중 길이가 750 이하인 샘플의 비율: 97.432\n",
            "전체 문장 중 길이가 800 이하인 샘플의 비율: 98.012\n",
            "전체 문장 중 길이가 850 이하인 샘플의 비율: 98.5\n",
            "전체 문장 중 길이가 900 이하인 샘플의 비율: 98.832\n",
            "전체 문장 중 길이가 950 이하인 샘플의 비율: 99.136\n",
            "\n",
            "문장 길이 설정: 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLYbLKesSq9R",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. [A] 임베딩 따로 한 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvnwtiBjBkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f6efa600-a59a-4ba4-87a3-8a809b5697c0"
      },
      "source": [
        "# 패딩 진행\n",
        "X_train_A = pad_sequences(X_train_raw, maxlen=max_length)\n",
        "X_test_A = pad_sequences(X_test_raw, maxlen=max_length)\n",
        "y_train_A = y_train.copy()\n",
        "y_test_A = y_test.copy()\n",
        "print(\"========== 패딩 후 ==========\")\n",
        "print(f\"훈련 데이터: {X_train_A.shape}\")\n",
        "print(f\"테스트 데이터: {X_test_A.shape}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== 패딩 후 ==========\n",
            "훈련 데이터: (25000, 400)\n",
            "테스트 데이터: (25000, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJYjrpLNjO57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 각 네트워크별 input\n",
        "# X_train_A_LSTM = X_train_A.copy()\n",
        "# X_test_A_LSTM = X_test_A.copy()\n",
        "# X_train_A_CNN = X_train_A.copy()\n",
        "# X_test_A_CNN = X_test_A.copy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N714IaQNpx5D",
        "colab_type": "text"
      },
      "source": [
        "### 0) Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5FfKh5LpEj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Input = Input(batch_shape=(None, max_length))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r7RhRi6WiGZ",
        "colab_type": "text"
      },
      "source": [
        "### 1) IMDB -> 임베딩 -> CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46qJvaVTYZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "eb20e0ec-7c20-4584-d651-db56635e505d"
      },
      "source": [
        "# CNN 네트워크 파라미터 설정\n",
        "n_filters = int(input('컨볼루션 필터 개수 설정: '))\n",
        "s_filters = int(input('컨볼루션 필터 사이즈 설정: '))\n",
        "\n",
        "# CNN 네트워크 레이어 구성\n",
        "X_Embed_CNN = Embedding(input_dim=max_features, output_dim=n_embed, input_length=max_length)(X_Input)\n",
        "# X_Embed_CNN_2 = Dropout(0.2)(X_Embed_CNN) # 최종 임베딩\n",
        "X_Conv = Conv1D(filters=n_filters, kernel_size=s_filters, strides=1, padding='same', activation='relu')(X_Embed_CNN)\n",
        "X_Pool = GlobalMaxPooling1D()(X_Conv)\n",
        "# X_Dense = Dense(n_hidden, activation='relu')(X_Pool) # relu가 0이 나오나?\n",
        "# X_Dense_2 = Dropout(0.5)(X_Dense)\n",
        "X_Flatten = Flatten()(X_Pool)\n",
        "\n",
        "# CNN 네트워크 구성\n",
        "cnn_model = Model(X_Input, X_Flatten)\n",
        "cnn_embed_model = Model(X_Input, X_Embed_CNN)\n",
        "\n",
        "# CNN 네트워크 확인\n",
        "# cnn_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001)) : 컴파일 여기서 안 함.\n",
        "print(\"========== CNN 모델 전체 구조 확인 ==========\")\n",
        "print(cnn_model.summary())\n",
        "print()\n",
        "print(\"========== CNN 임베딩 모델 구조 확인 ==========\")\n",
        "print(cnn_embed_model.summary())\n",
        "print()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "컨볼루션 필터 개수 설정: 260\n",
            "컨볼루션 필터 사이즈 설정: 3\n",
            "========== CNN 모델 전체 구조 확인 ==========\n",
            "Model: \"model_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 400)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_16 (Embedding)     (None, 400, 60)           360000    \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 400, 260)          47060     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_10 (Glo (None, 260)               0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 260)               0         \n",
            "=================================================================\n",
            "Total params: 407,060\n",
            "Trainable params: 407,060\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "========== CNN 임베딩 모델 구조 확인 ==========\n",
            "Model: \"model_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 400)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_16 (Embedding)     (None, 400, 60)           360000    \n",
            "=================================================================\n",
            "Total params: 360,000\n",
            "Trainable params: 360,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QscyffqSaL8D",
        "colab_type": "text"
      },
      "source": [
        "### 2) IMDB -> 임베딩 -> LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Cl75E7XUut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "304ffde5-81e1-4c56-bfb5-426671d59136"
      },
      "source": [
        "# LSTM 모델 레이어 구성\n",
        "X_Embed_LSTM = Embedding(input_dim=max_features, output_dim=n_embed, input_length=max_length)(X_Input)\n",
        "# X_Embed_LSTM_2 = Dropout(0.2)(X_Embed_LSTM) # 최종 임베딩\n",
        "X_LSTM = Bidirectional(LSTM(n_hidden, activation='relu'))(X_Embed_LSTM)\n",
        "\n",
        "# LSTM 모델 구성\n",
        "lstm_model = Model(X_Input, X_LSTM)\n",
        "lstm_embed_model = Model(X_Input, X_Embed_LSTM)\n",
        "\n",
        "# 컴파일\n",
        "print(\"========== LSTM 모델 전체 구조 확인 ==========\")\n",
        "print(lstm_model.summary())\n",
        "print()\n",
        "print(\"========== LSTM 임베딩 모델 구조 확인 ==========\")\n",
        "print(lstm_embed_model.summary())\n",
        "print()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "========== LSTM 모델 전체 구조 확인 ==========\n",
            "Model: \"model_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 400)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_17 (Embedding)     (None, 400, 60)           360000    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 128)               64000     \n",
            "=================================================================\n",
            "Total params: 424,000\n",
            "Trainable params: 424,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "========== LSTM 임베딩 모델 구조 확인 ==========\n",
            "Model: \"model_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 400)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_17 (Embedding)     (None, 400, 60)           360000    \n",
            "=================================================================\n",
            "Total params: 360,000\n",
            "Trainable params: 360,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiPoAHiaaN-x",
        "colab_type": "text"
      },
      "source": [
        "### 3) 네트워크 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kgXFExyaU3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c3367cc5-0877-40ea-e13c-f9880edf45ef"
      },
      "source": [
        "# 두 층 concat\n",
        "X_merge = Concatenate()([X_Flatten, X_LSTM])\n",
        "y_output = Dense(1, activation='sigmoid')(X_merge)\n",
        "\n",
        "model = Model(X_Input, y_output)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "print(\"========== 전체 모델 구조 확인 ==========\")\n",
        "print(model.summary())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== 전체 모델 구조 확인 ==========\n",
            "Model: \"model_40\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 400, 60)      360000      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 400, 260)     47060       embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_10 (Global (None, 260)          0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_17 (Embedding)        (None, 400, 60)      360000      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 260)          0           global_max_pooling1d_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 128)          64000       embedding_17[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 388)          0           flatten_10[0][0]                 \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            389         concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 831,449\n",
            "Trainable params: 831,449\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPydptvRdfPO",
        "colab_type": "text"
      },
      "source": [
        "### 4) 모델 학습\n",
        "\n",
        "- 0.001 : loss `nan` 실화인가?'\n",
        "- 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVqCa6IUarjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f94d3474-ffeb-4579-ff12-64e756f94790"
      },
      "source": [
        "# 모델 학습\n",
        "# es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "# hist= model.fit(X_train_A, y_train_A,\n",
        "#                 batch_size=BATCH,\n",
        "#                 epochs=EPOCHS,\n",
        "#                 validation_data=(X_test_A, y_test_A),\n",
        "#                 callbacks=[es])\n",
        "\n",
        "hist = model.fit(X_train_A, y_train_A,\n",
        "                 batch_size=BATCH,\n",
        "                 epochs=1,\n",
        "                 validation_data =(X_test_A, y_test_A),\n",
        "                 shuffle=True)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 791s 1s/step - loss: nan - val_loss: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SjwBhvL0X69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa6d4b31-e82a-4ed4-93e5-8c84f3150ae9"
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([[[nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         ...,\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan]],\n",
              " \n",
              "        [[nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         ...,\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan]],\n",
              " \n",
              "        [[nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         ...,\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan],\n",
              "         [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32),\n",
              " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "       dtype=float32),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
              " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32),\n",
              " array([[nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan],\n",
              "        [nan]], dtype=float32),\n",
              " array([nan], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMLE76iz0XwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLo_VMMyiSvF",
        "colab_type": "text"
      },
      "source": [
        "### 5) 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAyFKAB1if5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "34a179d4-b2d5-4590-eb52-57b54df456ee"
      },
      "source": [
        "# plot loss\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test Loss')\n",
        "plt.legend()\n",
        "plt.title('A 모델 Loss Trajectory')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47784 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45944 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47784 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45944 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_7toRg8ijzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "outputId": "153b39e2-8d17-4e02-9e81-a73b25204b81"
      },
      "source": [
        "# 예측 및 결과 확인: Deprecation Warning 주의\n",
        "y_train_pred = model.predict(X_train_A)\n",
        "y_test_pred = model.predict(X_test_A)\n",
        "y_train_pred = np.reshape(np.where(y_train_pred > 0.5, 1, 0), y_train.shape)\n",
        "y_test_pred = np.reshape(np.where(y_test_pred > 0.5, 1, 0), y_test.shape)\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train_A, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test_A, y_test_pred)}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9e261c09a71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 예측 및 결과 확인: Deprecation Warning 주의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:719 call\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:899 _run_internal_graph\n        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"dense_6/Identity:0\", shape=(None, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAsgWzZhgLev",
        "colab_type": "text"
      },
      "source": [
        "## 2.4. [B] 임베딩 따로 한 모델 구성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUi0gVRXgqVd",
        "colab_type": "text"
      },
      "source": [
        "### 1) IMDB -> 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QR4vbnUeFvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 임베딩 레이어 구성\n",
        "X_Input = Input(batch_shape=(None, max_length))\n",
        "X_Embed = Embedding(input_dim=max_features, output_dim=n_embed, input_length=max_length)(X_Input)\n",
        "X_Embed_2 = Dropout(0.2)(X_Embed) # 최종 임베딩\n",
        "\n",
        "# 임베딩 모델 구조 확인\n",
        "embedding_model = Model(X_Input, X_Embed_2)\n",
        "print(\"========== 임베딩 모델 구조 확인 ==========\")\n",
        "print(embedding_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqP60DXYg-62",
        "colab_type": "text"
      },
      "source": [
        "### 2) 임베딩 -> CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXei-LpIhDnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN 네트워크 파라미터 설정\n",
        "n_filters = int(input('컨볼루션 필터 개수 설정: '))\n",
        "s_filters = int(input('컨볼루션 필터 사이즈 설정: '))\n",
        "\n",
        "# CNN 네트워크 레이어 구성\n",
        "X_Conv = Conv1D(filters=n_filters, kernel_size=s_filters, strides=1, padding='valid', activation='relu')(X_Embed_2)\n",
        "X_Pool = GlobalMaxPooling1D()(X_Conv)\n",
        "X_Dense = Dense(n_hidden, activation='relu')(X_Pool)\n",
        "X_Dense_2 = Dropout(0.5)(X_Dense)\n",
        "X_Flatten = Flatten()(X_Dense_2)\n",
        "\n",
        "# CNN 네트워크 확인\n",
        "cnn_model = Model(X_Input, X_Flatten)\n",
        "print(\"========== CNN 모델 전체 구조 확인 ==========\")\n",
        "print(cnn_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9cKvbThZgk",
        "colab_type": "text"
      },
      "source": [
        "### 3) 임베딩 -> LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kneCQXUHhb8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM 네트워크 레이어 구성\n",
        "X_LSTM = Bidirectional(LSTM(n_hidden, activation='relu'))(X_Embed_2)\n",
        "\n",
        "# LSTM 네트워크 확인\n",
        "lstm_model = Model(X_Input_CNN, X_Flatten)\n",
        "print(\"========== CNN 모델 전체 구조 확인 ==========\")\n",
        "print(cnn_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL8asJ4-h4NV",
        "colab_type": "text"
      },
      "source": [
        "### 4) 네트워크 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5-RvfHUh6zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 두 층 concat\n",
        "X_merge = Concatenate()([X_Flatten, X_LSTM])\n",
        "y_output = Dense(1, activation='sigmoid')(X_merge)\n",
        "\n",
        "model = Model([X_Input_CNN, X_Input_LSTM], y_output)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "print(\"========== 전체 모델 구조 확인 ==========\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxqewqg3iKeP",
        "colab_type": "text"
      },
      "source": [
        "### 5) 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPUhh0EiP-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 학습\n",
        "es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "hist= model.fit([X_train, X_train], y_train,\n",
        "                batch_size=BATCH,\n",
        "                epochs=EPOCHS,\n",
        "                validation_data=([X_test, X_test], y_test),\n",
        "                callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyXISGx6inaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot loss\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test Loss')\n",
        "plt.legend()\n",
        "plt.title('B모델 Loss Trajectory')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWQjE-rinWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측 및 결과 확인: Deprecation Warning 주의\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_train_pred = np.reshape(np.where(y_train_pred > 0.5, 1, 0), y_train.shape)\n",
        "y_test_pred = np.reshape(np.where(y_test_pred > 0.5, 1, 0), y_test.shape)\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}