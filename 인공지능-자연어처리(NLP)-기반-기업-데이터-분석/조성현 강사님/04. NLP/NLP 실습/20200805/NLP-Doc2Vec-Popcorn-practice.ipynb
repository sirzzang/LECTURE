{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Doc2Vec-Popcorn-practice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ-YISh48xqg",
        "colab_type": "text"
      },
      "source": [
        "# 영화 리뷰 데이터 감성 분석\n",
        "\n",
        " `Doc2Vec`, `LogisticRegresion`을 이용해서 이진 분류를 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFlqgi08tKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모듈 불러 오기\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9jJaK1u9CqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 경로 설정\n",
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eGEvs8uIObQ",
        "colab_type": "text"
      },
      "source": [
        "## _1_. 로지스틱 회귀 : 0.84528"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UkqoXElISlM",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nat_F0a69XBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "466f6232-3fcf-4734-ae34-1071af90e8e4"
      },
      "source": [
        "# 데이터 로드\n",
        "df = pd.read_csv(f\"{data_path}/4-1.train_clean.csv\")\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stuff going moment mj started listening music ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>classic war worlds timothy hines entertaining ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film starts manager nicholas bell giving welco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>must assumed praised film greatest filmed oper...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>seems like consideration gone imdb reviews fil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>believe made film completely unnecessary first...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>guy loser get girls needs build picked stronge...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>minute documentary bu uel made early one spain...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>saw movie child broke heart story unfinished e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  sentiment\n",
              "0      stuff going moment mj started listening music ...          1\n",
              "1      classic war worlds timothy hines entertaining ...          1\n",
              "2      film starts manager nicholas bell giving welco...          0\n",
              "3      must assumed praised film greatest filmed oper...          0\n",
              "4      superbly trashy wondrously unpretentious explo...          1\n",
              "...                                                  ...        ...\n",
              "24995  seems like consideration gone imdb reviews fil...          0\n",
              "24996  believe made film completely unnecessary first...          0\n",
              "24997  guy loser get girls needs build picked stronge...          0\n",
              "24998  minute documentary bu uel made early one spain...          0\n",
              "24999  saw movie child broke heart story unfinished e...          1\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3lhXA9q9ist",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 분리\n",
        "reviews = list(df['review'])\n",
        "sentiments = list(df['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjU8LHhp9qpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a0c90772-2fdd-4023-a8b8-9ec6f0ba6354"
      },
      "source": [
        "# 문장 데이터 분리\n",
        "sentences = []\n",
        "for review in reviews:\n",
        "    sentences.append(review.split())\n",
        "\n",
        "print(sentences[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter'], ['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'every', 'year', 'e', 'g', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'everyone', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'critics', 'look', 'criticize', 'everything', 'others', 'rate', 'movie', 'important', 'bases', 'like', 'entertained', 'people', 'never', 'agree', 'critics', 'enjoyed', 'effort', 'mr', 'hines', 'put', 'faithful', 'h', 'g', 'wells', 'classic', 'novel', 'found', 'entertaining', 'made', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings'], ['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', 'jurassik', 'park', 'scientists', 'resurrect', 'one', 'nature', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'however', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'scientific', 'meanwhile', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'pre', 'historical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'though', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'one', 'nature', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalks', 'victims', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hair', 'raising', 'chills', 'full', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'effects', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'made', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middling', 'performances', 'though', 'players', 'reacting', 'appropriately', 'becoming', 'food', 'actors', 'give', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'running', 'bound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realistic', 'gory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', 'sabretooth', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'much', 'better', 'bc', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'many', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'many', 'others', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxanne', 'robinson', 'crusoe', 'rating', 'average', 'bottom', 'barrel'], ['must', 'assumed', 'praised', 'film', 'greatest', 'filmed', 'opera', 'ever', 'read', 'somewhere', 'either', 'care', 'opera', 'care', 'wagner', 'care', 'anything', 'except', 'desire', 'appear', 'cultured', 'either', 'representation', 'wagner', 'swan', 'song', 'movie', 'strikes', 'unmitigated', 'disaster', 'leaden', 'reading', 'score', 'matched', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allowed', 'anywhere', 'near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagner', 'text', 'decided', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter', 'stages', 'transmutes', 'kind', 'beatnik', 'babe', 'though', 'one', 'continues', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double', 'dose', 'armin', 'jordan', 'conductor', 'seen', 'face', 'heard', 'voice', 'amfortas', 'also', 'appears', 'monstrously', 'double', 'exposure', 'kind', 'batonzilla', 'conductor', 'ate', 'monsalvat', 'playing', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represented', 'scattering', 'shopworn', 'flaccid', 'crocuses', 'stuck', 'ill', 'laid', 'turf', 'expedient', 'baffles', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'think', 'syberberg', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provided', 'julie', 'andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voices', 'trumpets', 'particular', 'possessing', 'aural', 'glare', 'adds', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conducting', 'paralytic', 'unfolding', 'ritual', 'someone', 'another', 'review', 'mentioned', 'bayreuth', 'recording', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lacks', 'sense', 'pulse', 'feeling', 'ebb', 'flow', 'music', 'half', 'century', 'orchestral', 'sound', 'set', 'modern', 'pressings', 'still', 'superior', 'film'], ['superbly', 'trashy', 'wondrously', 'unpretentious', 'exploitation', 'hooray', 'pre', 'credits', 'opening', 'sequences', 'somewhat', 'give', 'false', 'impression', 'dealing', 'serious', 'harrowing', 'drama', 'need', 'fear', 'barely', 'ten', 'minutes', 'later', 'necks', 'nonsensical', 'chainsaw', 'battles', 'rough', 'fist', 'fights', 'lurid', 'dialogs', 'gratuitous', 'nudity', 'bo', 'ingrid', 'two', 'orphaned', 'siblings', 'unusually', 'close', 'even', 'slightly', 'perverted', 'relationship', 'imagine', 'playfully', 'ripping', 'towel', 'covers', 'sister', 'naked', 'body', 'stare', 'unshaven', 'genitals', 'several', 'whole', 'minutes', 'well', 'bo', 'sister', 'judging', 'dubbed', 'laughter', 'mind', 'sick', 'dude', 'anyway', 'kids', 'fled', 'russia', 'parents', 'nasty', 'soldiers', 'brutally', 'slaughtered', 'mommy', 'daddy', 'friendly', 'smuggler', 'took', 'custody', 'however', 'even', 'raised', 'trained', 'bo', 'ingrid', 'expert', 'smugglers', 'actual', 'plot', 'lifts', 'years', 'later', 'facing', 'ultimate', 'quest', 'mythical', 'incredibly', 'valuable', 'white', 'fire', 'diamond', 'coincidentally', 'found', 'mine', 'things', 'life', 'ever', 'made', 'little', 'sense', 'plot', 'narrative', 'structure', 'white', 'fire', 'sure', 'lot', 'fun', 'watch', 'time', 'clue', 'beating', 'cause', 'bet', 'actors', 'understood', 'even', 'less', 'whatever', 'violence', 'magnificently', 'grotesque', 'every', 'single', 'plot', 'twist', 'pleasingly', 'retarded', 'script', 'goes', 'totally', 'bonkers', 'beyond', 'repair', 'suddenly', 'reveal', 'reason', 'bo', 'needs', 'replacement', 'ingrid', 'fred', 'williamson', 'enters', 'scene', 'big', 'cigar', 'mouth', 'sleazy', 'black', 'fingers', 'local', 'prostitutes', 'bo', 'principal', 'opponent', 'italian', 'chick', 'big', 'breasts', 'hideous', 'accent', 'preposterous', 'catchy', 'theme', 'song', 'plays', 'least', 'dozen', 'times', 'throughout', 'film', 'obligatory', 'falling', 'love', 'montage', 'loads', 'attractions', 'god', 'brilliant', 'experience', 'original', 'french', 'title', 'translates', 'life', 'survive', 'uniquely', 'appropriate', 'makes', 'much', 'sense', 'rest', 'movie', 'none']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJTyRBTnIVUt",
        "colab_type": "text"
      },
      "source": [
        "### Doc2Vec 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIcUj8fj9sjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5e4000fe-b0b1-42ac-ed1a-f6d84b2d615d"
      },
      "source": [
        "# Doc2Vec 모델 생성\n",
        "model_path = f\"{data_path}/300features.doc2vec\"\n",
        "model_saved = True # 지금은 일단 저장된 게 없다\n",
        "\n",
        "if model_saved :\n",
        "    doc_model = Doc2Vec.load(model_path)\n",
        "else:\n",
        "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)] # 번호로 태그\n",
        "    doc_model = Doc2Vec(vector_size=300, \n",
        "                        alpha=0.025, \n",
        "                        min_alpha=0.00025,\n",
        "                        min_count=10, \n",
        "                        workers=4, \n",
        "                        dm=1)\n",
        "    doc_model.build_vocab(documents)\n",
        "    doc_model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
        "    doc_model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVt1P4GB-Q2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e076f267-3e15-4ae4-eb09-581283c6ab72"
      },
      "source": [
        "# 모델 확인\n",
        "keys = list(doc_model.wv.vocab.keys())\n",
        "print(f\"단어 개수: {len(keys)}\")\n",
        "print(\"========= 샘플 확인 =========\")\n",
        "print(keys[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 개수: 19717\n",
            "========= 샘플 확인 =========\n",
            "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTkhHYH5IYg1",
        "colab_type": "text"
      },
      "source": [
        "모델의 word vector 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88u1dbTv--aC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2bdaaa6f-6b55-4b74-e763-87f0501ae5b0"
      },
      "source": [
        "# 단어 'stuff'의 벡터 확인\n",
        "print(f\"stuff:\\n {model.wv['stuff']}\")\n",
        "\n",
        "# 단어 유사도 측정\n",
        "print(f\"dog, cat : {model.wv.similarity('dog', 'cat')}\")\n",
        "print(f\"dog, cake : {model.wv.similarity('dog', 'cake')}\")\n",
        "\n",
        "# 단어 유사도 측정: 벡터 내적\n",
        "print(f\"dog, cat : {np.dot(model.wv['dog'], model.wv['cat'])}\")\n",
        "print(f\"dog, cake : {np.dot(model.wv['dog'], model.wv['cake'])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stuff:\n",
            " [-0.18611595 -0.36705476 -0.23880133 -0.38356516 -0.1215637  -0.31198502\n",
            " -0.1462326  -0.81891495 -0.02643439 -0.06654777  0.03929293 -0.8636567\n",
            "  0.46589312  0.24193387 -0.15810192 -0.15600042  0.05049789 -0.59269065\n",
            " -0.03312543 -0.523009   -0.16240235  0.65150017 -0.13370064  0.13427418\n",
            " -0.3869068   0.01416051  0.8123483  -0.48103186  0.24293292 -0.3215318\n",
            " -0.41009226 -0.5711381   0.86399955  0.35488522  0.28767172  0.2832965\n",
            " -0.32490394 -0.6875084  -0.5823474  -0.2030511   0.06413027 -0.328916\n",
            "  0.3617928   0.3433004   0.04010413  0.18418486  0.25569075 -0.7578971\n",
            " -0.16020748  0.5819255  -0.67376256 -0.27500638 -0.14619175 -0.18352857\n",
            " -0.0070066   0.14206877  0.04845129 -0.62029594  0.06632705  0.81629825\n",
            "  1.1358607   0.01165283  0.47023997  0.29312617  0.4921738   0.5336238\n",
            "  0.18848604 -0.10422531  0.43303135  0.512983    1.2141435   0.08608381\n",
            " -0.06378851  0.2943112   0.11136171  0.19842027  1.213685   -0.9718013\n",
            "  0.36714703 -0.6624622   0.1054258   0.6882721  -0.2204971  -0.11016434\n",
            "  0.49694383  0.45360866  0.05259637 -0.05203943 -0.30080032  0.33426937\n",
            " -1.0222714   0.7454055   0.26317278 -0.12821756 -0.02335132 -0.41640028\n",
            " -0.41986457  0.7767227   0.46478602  1.1084142  -0.7160205   0.64650637\n",
            "  0.45823628 -0.3031042  -0.06899601  0.14928514  0.14694282 -0.33120978\n",
            "  0.03363563 -0.16620582 -0.3754477   0.09171297 -0.30128422  0.9008815\n",
            " -0.7829929  -0.06594033 -0.08183898 -0.2650438   0.48301002  0.7350027\n",
            "  0.3326295  -0.13273259 -0.00157758  0.21806465  0.1496187   0.6438647\n",
            " -1.0276948   0.48686737  0.16608474  0.43894812  0.19320993  0.4691142\n",
            " -0.9125739  -0.30115226  0.38660252 -0.24418908 -0.6072196  -0.44941244\n",
            "  1.25013    -0.14992076 -1.0508127   0.16194235 -0.35637093  0.02340607\n",
            "  0.2009467  -0.47059235 -0.12623417  0.3936181   0.6309231   0.06710877\n",
            "  0.69895226 -0.33408302  0.01542763  0.24761954 -0.56330174 -0.01638449\n",
            " -0.59913856 -0.69659567  0.4104608  -0.00270634 -0.67593867  1.219108\n",
            "  0.13428767 -0.37978473 -0.6944054   0.50553656 -0.08589105  1.0260879\n",
            " -0.1489248   0.05606327 -0.08093958  0.18517883 -0.62039685  0.309443\n",
            "  0.03122043 -0.17472677 -0.37949735  0.49233982 -0.03932828 -0.81107605\n",
            " -0.40863788  0.37890467 -0.02576669  0.6073105  -0.70578617 -0.28210688\n",
            "  0.01656955 -0.2867281   0.37631494  0.6778701  -0.14104286 -0.10648064\n",
            " -0.86731875  0.8110808   0.49001735  0.2090294   0.78570074 -0.42480877\n",
            " -0.21458964  0.7610215   0.4823167  -0.899397    0.66007864  0.43806648\n",
            "  0.72753245  0.17667933  0.74010813 -0.67952996 -0.24681555  0.5686908\n",
            " -0.08211119 -0.5587617   0.25519982  0.05180168 -0.27906957  0.7198982\n",
            " -0.00354864 -0.11796323 -0.11039251  0.5126888  -0.46797666 -0.31256902\n",
            " -0.12886849  0.2327794  -0.02551577 -0.02286563  0.3574976  -0.09784709\n",
            " -0.95435774 -0.68598163 -0.46900383 -0.6579349  -0.66216403 -0.38307378\n",
            " -0.02818875 -0.72632146 -0.06138476 -0.70249295 -0.05229764  0.08469548\n",
            "  0.14679283  0.05201039 -0.25639632  0.76927173  0.85233885 -0.28126004\n",
            "  0.30370963  0.63695854  0.48420814 -0.11096081 -0.04834354 -0.8223384\n",
            " -0.25458407  0.6133784  -0.73021257  0.9548984  -0.3346618  -0.447806\n",
            "  0.12439805 -0.57283     0.13363022  0.39436194 -0.256946   -0.38830715\n",
            " -0.9925529   0.02791532  0.35980988  0.40662363  0.43821907  0.09304458\n",
            " -0.21546894 -0.01339751  0.20057544  0.47015047  0.42896122  1.1548438\n",
            " -0.41136345  0.44620773  0.23603494  0.53335345 -0.5325786   1.1185176\n",
            " -0.5354428  -0.04777487  0.01630961 -0.2904512  -0.0934948   0.64982307\n",
            "  0.24659729  0.6383011  -0.3875082   0.4649687  -0.8756308  -0.44697276\n",
            "  0.27626216  0.2887431   0.7525008   0.6106265   0.13511966 -0.36076975]\n",
            "dog, cat : 0.5961354374885559\n",
            "dog, cake : 0.11458448320627213\n",
            "dog, cat : 50.4817008972168\n",
            "dog, cake : 5.5644989013671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnT3RDISIbx5",
        "colab_type": "text"
      },
      "source": [
        "모델의 document 벡터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQRNzTEuAAuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "843baa21-3b57-4872-ffa8-fc91b5ba5e25"
      },
      "source": [
        "# 첫 번째 문장의 벡터 확인\n",
        "print(reviews[0])\n",
        "print(model.docvecs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n",
            "[ 0.08058867  0.17019913  0.29712838  0.32912368  0.21150947 -0.12499653\n",
            " -0.28246233 -0.11697684  0.07201732 -0.0743681  -0.05801594 -0.17654055\n",
            " -0.04668377 -0.08541035  0.20405756 -0.12838112 -0.00471542 -0.03300128\n",
            "  0.09379468 -0.01848561 -0.22778243 -0.21978417  0.04148072  0.05278015\n",
            " -0.1920117   0.23283921  0.08945281 -0.18445224 -0.00979966 -0.17406358\n",
            "  0.00842136 -0.16566609  0.05572662  0.0584168  -0.09032757  0.1607223\n",
            " -0.10826637  0.09399588 -0.15468827 -0.038298   -0.16306412 -0.17370602\n",
            "  0.07849234 -0.09017591 -0.01308629 -0.00478676  0.28243268  0.01889832\n",
            " -0.16363181  0.12643908  0.0653094  -0.24263151 -0.127472    0.12273916\n",
            "  0.06264719  0.25237808 -0.16013813 -0.16871896 -0.16954091  0.11017347\n",
            "  0.04194959 -0.12696956  0.17883605 -0.1303392   0.1168199   0.31832808\n",
            "  0.04983575 -0.10167149 -0.19808896  0.0631009   0.0360073   0.31264454\n",
            " -0.12792225  0.0493203   0.0219451   0.03915779  0.08932218  0.00540843\n",
            "  0.12428526 -0.0594995  -0.06599904 -0.0591152  -0.00203735 -0.20025337\n",
            "  0.07653339  0.06607386  0.24676327 -0.31406143 -0.05137267 -0.03510498\n",
            "  0.14526506  0.1926677   0.07122928  0.00594439 -0.04188808 -0.02797813\n",
            " -0.11371753 -0.09533587  0.1032818  -0.05428338  0.0124649  -0.30091888\n",
            " -0.00669017  0.45962375  0.14617704 -0.21567328 -0.15208785  0.09426011\n",
            " -0.2272239   0.02594292  0.13935617  0.03181586  0.10172368  0.00648297\n",
            " -0.10885768 -0.31631997 -0.06948135  0.02368042 -0.1117681  -0.21063776\n",
            " -0.11721475  0.07543732 -0.03375394  0.08783499 -0.12839578  0.10160717\n",
            " -0.08293798 -0.1779341  -0.30350262  0.0185345   0.19613169 -0.10332937\n",
            " -0.07972651 -0.02291784  0.2076852  -0.20299666 -0.09026465 -0.29826528\n",
            "  0.11314667  0.14288576  0.0587456   0.03789134 -0.17058359  0.15530814\n",
            "  0.03764655  0.21888621 -0.13572256 -0.2540621  -0.00209931 -0.02867159\n",
            "  0.12309539 -0.08147833 -0.18365587 -0.02233711  0.3514432   0.23135796\n",
            " -0.19403118 -0.02543792  0.0823595   0.25140172 -0.04810991  0.18832889\n",
            "  0.01502081  0.09421004 -0.11810227  0.07585332 -0.08514339 -0.09215283\n",
            "  0.12485165 -0.05607359  0.02642368 -0.15288009 -0.11770321 -0.10772012\n",
            " -0.07487242 -0.06040024  0.17064852 -0.25812203 -0.07818494 -0.22183956\n",
            " -0.08138395 -0.04977869  0.02538389  0.05295653 -0.14068806 -0.01232979\n",
            "  0.05065187 -0.1865534  -0.19911046  0.06989648  0.03819894 -0.2247694\n",
            "  0.17461488 -0.06850293 -0.05344313 -0.08743121  0.08116777  0.04410265\n",
            "  0.2080683   0.0293066   0.28081417 -0.03482625 -0.10192215 -0.08297443\n",
            "  0.22118734 -0.04584894  0.13481489 -0.05877456 -0.15289982  0.06440435\n",
            " -0.21033174  0.02943812  0.03523904 -0.01128776  0.04320371 -0.04875435\n",
            "  0.03418326 -0.0507395   0.08248112 -0.0985337   0.11873756 -0.06797674\n",
            " -0.11055955 -0.09207725  0.18452032 -0.03795553 -0.00209644 -0.26239017\n",
            " -0.2502927   0.14283617  0.09493875 -0.32300204 -0.10981916 -0.01470492\n",
            " -0.1332507  -0.21298717  0.25388613  0.01047233  0.17924425 -0.04054954\n",
            "  0.10527583  0.01096929 -0.03490133 -0.01515247 -0.01288744  0.06701948\n",
            "  0.16826262  0.14518705 -0.01961372 -0.18901666  0.31743863 -0.23830684\n",
            " -0.05730121 -0.03057032 -0.065906    0.4339077  -0.04845785  0.09696621\n",
            " -0.15559718 -0.2713608   0.07417636 -0.08650466 -0.09009725 -0.26503956\n",
            "  0.15419339 -0.05560519  0.19457449  0.24359728  0.12387038 -0.04772547\n",
            "  0.02448076  0.0808204   0.14209834 -0.09891166 -0.03830253 -0.16073881\n",
            "  0.04514848 -0.03419657 -0.15605344 -0.01072253  0.03343475  0.3026792\n",
            " -0.2447494   0.3867007  -0.11732224  0.06854422 -0.14921853  0.07534476\n",
            "  0.25403172 -0.05934437 -0.09841496  0.08097155  0.07391271 -0.03340879\n",
            " -0.03818469 -0.11622377  0.13691184  0.02036     0.08954407 -0.12836902]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ULkHXDLATml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 감성분석 데이터 생성\n",
        "X_data_raw = [model.docvecs[i] for i in range(len(sentences))]\n",
        "y_data = np.array(sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jB5p1uyIewD",
        "colab_type": "text"
      },
      "source": [
        "### 로지스틱 회귀 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf0sQ0Nx--to",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "44e96af8-52f3-4024-e60e-b2196824874e"
      },
      "source": [
        "# 데이터 분리\n",
        "X_data = X_data_raw.copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "print(f\"Train: {len(X_train)}, {y_train.shape}\")\n",
        "print(f\"Test: {len(X_test)}, {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 18750, (18750,)\n",
            "Test: 6250, (6250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G6KWUHu--rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "860ddc31-4816-4e68-d188-e2bc30749931"
      },
      "source": [
        "# 로지스틱 회귀 분석\n",
        "lr_model = LogisticRegression(class_weight='balanced', solver='newton-cg')\n",
        "lr_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0zIsPke--cY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e6cb3079-54d0-4804-c3a6-08543d795dda"
      },
      "source": [
        "# 정확도 측정\n",
        "predicted = lr_model.predict(X_test)\n",
        "print(f\"Sample Predicted: {predicted[:20]}\")\n",
        "print(f\"Test Accuracy: {lr_model.score(X_test, y_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Predicted: [0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1]\n",
            "Test Accuracy: 0.84528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anDHhnfs-Qz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8894e353-46fa-4ab9-ddbf-824cfedf98fe"
      },
      "source": [
        "# 새로운 문장에 대한 예측\n",
        "new_sentence = model.infer_vector(['system', 'repsponse', 'cpu', 'compute'])\n",
        "print(new_sentence)\n",
        "new_sentence_pred = lr_model.predict(new_sentence.reshape(1, -1)) # 문장 하나이므로 reshape\n",
        "print(new_sentence_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4.93865088e-03  1.98567472e-02  3.61140780e-02  3.82797755e-02\n",
            " -3.80598125e-03 -3.15198675e-02  2.69053131e-03 -2.52771042e-02\n",
            " -1.97380092e-02 -8.14476609e-02 -6.34776652e-02  7.86244491e-05\n",
            "  3.03491391e-02 -1.17518324e-02  1.59860216e-02 -1.96323320e-02\n",
            " -2.27632467e-02 -1.17330188e-02  2.62095705e-02  1.23778824e-02\n",
            "  4.95460536e-03 -2.13385969e-02  6.64318970e-04  4.00892682e-02\n",
            " -2.29727174e-03 -8.89134873e-03  1.28722815e-02 -1.51277091e-02\n",
            " -1.44082997e-02  3.46479677e-02 -1.69262812e-02  1.79347582e-03\n",
            "  1.52955633e-02 -2.22750735e-02 -4.83419280e-03  4.72374894e-02\n",
            "  6.39771635e-04  2.60303058e-02 -5.02311252e-03 -2.79587507e-02\n",
            "  3.08272969e-02 -1.88137882e-03 -1.31927747e-02  4.24919128e-02\n",
            " -1.57636944e-02 -1.82467997e-02  1.64566077e-02 -3.69514562e-02\n",
            " -6.04261756e-02 -3.68457958e-02 -3.09357122e-02 -2.92656869e-02\n",
            " -9.16608237e-03  4.28173095e-02  2.62064356e-02  1.51867578e-02\n",
            " -2.33184304e-02  1.95562318e-02  6.28911611e-03  8.42724927e-03\n",
            "  1.46305906e-02 -1.97559055e-02  7.74773117e-03 -3.38801257e-02\n",
            "  8.93715583e-03  3.47977169e-02  1.51250046e-03 -3.24629471e-02\n",
            " -7.99956545e-03 -3.69401160e-03 -2.54171379e-02  1.64595768e-02\n",
            " -4.26923409e-02 -1.94814596e-02  1.86196156e-02  2.18987726e-02\n",
            "  2.24233177e-02  9.33374744e-03  2.08758283e-03  3.86658497e-03\n",
            " -4.13889028e-02 -1.67033002e-02 -4.71181469e-03 -5.11215115e-03\n",
            "  1.85546000e-02  2.67622601e-02  1.50304136e-03 -4.15747846e-03\n",
            " -5.54751558e-03 -2.56453212e-02  7.13381916e-03 -8.19978956e-03\n",
            "  2.10710503e-02  6.21717563e-03 -3.22578736e-02 -4.66860598e-03\n",
            "  3.10586989e-02 -1.02310767e-02  1.10436007e-02  1.46976737e-02\n",
            " -1.42098209e-02 -2.95431993e-04 -3.06916926e-02  3.53651657e-03\n",
            " -3.30536366e-02 -5.32220565e-02  2.31045689e-02 -2.96486123e-03\n",
            " -3.60034853e-02  1.20642381e-02  7.80513417e-03 -1.86036862e-02\n",
            " -2.54432335e-02  3.41599341e-04 -2.54805957e-04  1.53747399e-03\n",
            "  4.01935466e-02  1.71840247e-02 -2.49081366e-02 -1.29649602e-02\n",
            " -4.52946685e-02  2.10942943e-02  2.24789418e-02  1.24988556e-02\n",
            " -1.75421592e-02  1.84511039e-02  1.73179561e-03  3.13899899e-03\n",
            " -5.58439791e-02 -2.58091595e-02  3.43494816e-03 -6.34182896e-03\n",
            "  3.11925299e-02  2.61467956e-02  4.50050347e-02 -1.50963459e-02\n",
            " -3.75917964e-02  7.67347775e-03 -1.63344685e-02  2.94592883e-02\n",
            " -1.33006822e-03  1.93231739e-02 -9.08017997e-03  1.49812084e-02\n",
            "  2.18156446e-02  2.30316650e-02 -2.63083018e-02 -3.64057510e-03\n",
            "  2.61440687e-02 -1.36563219e-02 -2.01000404e-02 -1.88826304e-02\n",
            "  1.55237876e-02 -1.00412797e-02  5.15506528e-02  4.06996273e-02\n",
            "  2.51565818e-02  4.35193181e-02  1.13291442e-02  1.50789591e-02\n",
            "  1.16944211e-02  7.03246444e-02 -8.92387889e-03 -1.55580451e-03\n",
            "  2.55823769e-02 -5.01474645e-03 -4.93422113e-02 -1.15760025e-02\n",
            "  5.22862747e-03 -1.58837326e-02  7.98496511e-03  1.21079274e-02\n",
            "  1.99870113e-02 -3.84996012e-02 -1.57813672e-02 -4.38884348e-02\n",
            " -1.00324797e-02 -3.57246734e-02 -2.40052473e-02 -3.75396833e-02\n",
            " -6.13837969e-03  3.45452279e-02  1.13612330e-02  3.23577188e-02\n",
            " -2.87892539e-02  3.12909186e-02 -1.27725210e-02  6.91194832e-03\n",
            "  4.96117622e-02  1.47769013e-02 -3.52282310e-03 -4.78491373e-02\n",
            "  3.82163301e-02  2.90333610e-02  1.26335933e-03 -3.78710544e-03\n",
            "  3.11164446e-02  6.70377514e-04  5.14391111e-03 -9.22793057e-03\n",
            "  8.47620016e-04 -1.59815867e-02 -3.45851742e-02 -1.67751510e-03\n",
            "  3.52835618e-02  1.98330544e-02  9.18179471e-03  5.14101237e-03\n",
            "  7.60094915e-03 -1.39093315e-02 -3.75856571e-02 -5.12207970e-02\n",
            " -1.64479725e-02  1.28569873e-02  7.12512573e-03 -1.95781961e-02\n",
            " -1.78434011e-02  7.05262832e-03 -1.64039731e-02  2.45136451e-02\n",
            "  1.50514301e-02  4.06083204e-02 -1.45838559e-02 -1.98056158e-02\n",
            "  2.04406567e-02  2.67401934e-02 -8.83161556e-03 -2.00526472e-02\n",
            " -6.80506183e-03  8.27085320e-03 -7.54155626e-04 -2.38900818e-02\n",
            " -1.86535215e-03  3.01884077e-02 -2.56288312e-02 -3.49418223e-02\n",
            "  2.06858777e-02 -3.02723721e-02  1.94325298e-02 -1.20545495e-02\n",
            "  5.04341461e-02  2.07002163e-02 -1.62728932e-02 -2.74961367e-02\n",
            "  9.76120401e-03 -3.04789050e-03  2.89359149e-02 -2.81838677e-03\n",
            " -5.68872551e-03 -3.10038944e-04  1.21665327e-02 -1.08354278e-02\n",
            " -1.42905368e-02  2.27142330e-02 -2.92081721e-02  2.84934081e-02\n",
            " -6.54395297e-02  4.66308370e-02 -1.73558537e-02 -1.92759801e-02\n",
            "  1.24418258e-03  3.58743891e-02 -2.85742525e-02 -5.29860146e-02\n",
            "  2.23170277e-02 -1.26316408e-02  2.42676586e-02  6.37018383e-02\n",
            " -1.44736012e-02 -1.12320865e-02 -6.04493031e-03  3.51215969e-03\n",
            "  1.13919564e-02 -1.48009909e-02  4.03150497e-03 -2.18344964e-02\n",
            " -2.96452250e-02 -6.69425074e-03  4.94221412e-03  8.67555791e-04\n",
            " -1.58879030e-02  1.62968021e-02  1.83437523e-02  4.13605236e-02\n",
            " -2.54744645e-02  7.10724946e-03 -4.69814241e-02 -4.98356633e-02\n",
            "  1.46174766e-02  3.42887454e-02 -2.10825191e-03  1.43004991e-02\n",
            " -1.96101312e-02  4.27729338e-02  2.07215641e-03 -2.15067174e-02\n",
            " -1.21690612e-02 -8.28411896e-03  2.76511977e-03 -2.20708409e-03]\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMd1r6FyCtWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f4AfPbGE4eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjOffeX5X026",
        "colab_type": "text"
      },
      "source": [
        "# ============ 과제 ============"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkqwzG2wIoVJ",
        "colab_type": "text"
      },
      "source": [
        "## _2_. FFN : 최고 0.84704"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjE4JEM5DPws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91b2a4de-78ef-4a3e-af31-ed8d1872115e"
      },
      "source": [
        "# 데이터 준비\n",
        "X_data = np.array(X_data_raw)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: (18750, 300), (18750,)\n",
            "Test: (6250, 300), (6250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkuc2Hj0C8JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "0da231ee-bec0-4a97-fbf4-5b04ab80a8e2"
      },
      "source": [
        "# 딥러닝 모델 네트워크 구성\n",
        "K.clear_session()\n",
        "\n",
        "X_input = Input(batch_shape=(None, X_train.shape[1]))\n",
        "X_hidden = Dense(128)(X_input)\n",
        "X_hidden = Dropout(0.2)(X_hidden)\n",
        "X_hidden = Dense(256)(X_hidden)\n",
        "X_hidden = Dropout(0.2)(X_hidden)\n",
        "X_hidden = Dense(128)(X_hidden)\n",
        "y_output = Dense(1, activation='sigmoid')(X_hidden)\n",
        "\n",
        "model = Model(X_input, y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "print(\"============= 모델 전체 구조 =============\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============= 모델 전체 구조 =============\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 300)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               38528     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 104,577\n",
            "Trainable params: 104,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhIjCG0iBCi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bd6b8b2-5d00-4d8e-c33d-072ce242e752"
      },
      "source": [
        "# 모델 학습\n",
        "BATCH = int(input('배치 사이즈 설정: '))\n",
        "EPOCHS = int(input('학습 에폭 설정: '))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
        "\n",
        "# hist = model.fit(X_train, y_train,\n",
        "#                  batch_size=BATCH,\n",
        "#                  validation_split = 0.1,\n",
        "#                  epochs=EPOCHS,\n",
        "#                  callbacks=[es])ㅌ\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH,\n",
        "          validation_split=0.2,\n",
        "          epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "배치 사이즈 설정: 200\n",
            "학습 에폭 설정: 300\n",
            "Epoch 1/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3755 - val_loss: 0.4013\n",
            "Epoch 2/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3744 - val_loss: 0.3977\n",
            "Epoch 3/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3748 - val_loss: 0.3999\n",
            "Epoch 4/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3739 - val_loss: 0.3986\n",
            "Epoch 5/300\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3724 - val_loss: 0.4037\n",
            "Epoch 6/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3722 - val_loss: 0.3980\n",
            "Epoch 7/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3724 - val_loss: 0.3978\n",
            "Epoch 8/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3716 - val_loss: 0.4033\n",
            "Epoch 9/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3713 - val_loss: 0.3986\n",
            "Epoch 10/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3720 - val_loss: 0.3977\n",
            "Epoch 11/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3703 - val_loss: 0.3998\n",
            "Epoch 12/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3688 - val_loss: 0.3982\n",
            "Epoch 13/300\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3727 - val_loss: 0.3976\n",
            "Epoch 14/300\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3710 - val_loss: 0.4032\n",
            "Epoch 15/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3701 - val_loss: 0.3973\n",
            "Epoch 16/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3691 - val_loss: 0.3981\n",
            "Epoch 17/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3698 - val_loss: 0.4030\n",
            "Epoch 18/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3692 - val_loss: 0.4006\n",
            "Epoch 19/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3698 - val_loss: 0.3973\n",
            "Epoch 20/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3696 - val_loss: 0.4037\n",
            "Epoch 21/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3698 - val_loss: 0.3987\n",
            "Epoch 22/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3687 - val_loss: 0.4005\n",
            "Epoch 23/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3688 - val_loss: 0.4062\n",
            "Epoch 24/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3675 - val_loss: 0.3976\n",
            "Epoch 25/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3713 - val_loss: 0.4061\n",
            "Epoch 26/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3670 - val_loss: 0.3971\n",
            "Epoch 27/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3679 - val_loss: 0.3995\n",
            "Epoch 28/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3696 - val_loss: 0.3977\n",
            "Epoch 29/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3695 - val_loss: 0.4024\n",
            "Epoch 30/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3675 - val_loss: 0.4004\n",
            "Epoch 31/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3697 - val_loss: 0.3981\n",
            "Epoch 32/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3669 - val_loss: 0.4005\n",
            "Epoch 33/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3690 - val_loss: 0.3953\n",
            "Epoch 34/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3685 - val_loss: 0.3956\n",
            "Epoch 35/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3659 - val_loss: 0.3969\n",
            "Epoch 36/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3677 - val_loss: 0.4012\n",
            "Epoch 37/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3688 - val_loss: 0.4020\n",
            "Epoch 38/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3684 - val_loss: 0.4006\n",
            "Epoch 39/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3666 - val_loss: 0.4009\n",
            "Epoch 40/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3672 - val_loss: 0.3974\n",
            "Epoch 41/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3679 - val_loss: 0.3969\n",
            "Epoch 42/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3679 - val_loss: 0.3973\n",
            "Epoch 43/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3676 - val_loss: 0.3976\n",
            "Epoch 44/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3679 - val_loss: 0.3987\n",
            "Epoch 45/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3670 - val_loss: 0.4008\n",
            "Epoch 46/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3656 - val_loss: 0.3958\n",
            "Epoch 47/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3665 - val_loss: 0.4038\n",
            "Epoch 48/300\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3664 - val_loss: 0.3956\n",
            "Epoch 49/300\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3667 - val_loss: 0.4009\n",
            "Epoch 50/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3656 - val_loss: 0.3977\n",
            "Epoch 51/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3656 - val_loss: 0.3963\n",
            "Epoch 52/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3664 - val_loss: 0.3972\n",
            "Epoch 53/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3669 - val_loss: 0.3968\n",
            "Epoch 54/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3672 - val_loss: 0.3960\n",
            "Epoch 55/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3662 - val_loss: 0.3982\n",
            "Epoch 56/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3675 - val_loss: 0.3958\n",
            "Epoch 57/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3669 - val_loss: 0.3979\n",
            "Epoch 58/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3656 - val_loss: 0.3964\n",
            "Epoch 59/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3652 - val_loss: 0.3972\n",
            "Epoch 60/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3663 - val_loss: 0.3983\n",
            "Epoch 61/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3664 - val_loss: 0.3991\n",
            "Epoch 62/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3667 - val_loss: 0.3974\n",
            "Epoch 63/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3672 - val_loss: 0.4012\n",
            "Epoch 64/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3655 - val_loss: 0.3969\n",
            "Epoch 65/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3648 - val_loss: 0.3992\n",
            "Epoch 66/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3663 - val_loss: 0.3985\n",
            "Epoch 67/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3661 - val_loss: 0.3952\n",
            "Epoch 68/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3653 - val_loss: 0.4046\n",
            "Epoch 69/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3652 - val_loss: 0.3967\n",
            "Epoch 70/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3649 - val_loss: 0.4007\n",
            "Epoch 71/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3653 - val_loss: 0.3980\n",
            "Epoch 72/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3660 - val_loss: 0.3998\n",
            "Epoch 73/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3669 - val_loss: 0.3998\n",
            "Epoch 74/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3652 - val_loss: 0.3989\n",
            "Epoch 75/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3658 - val_loss: 0.3981\n",
            "Epoch 76/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3643 - val_loss: 0.3973\n",
            "Epoch 77/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3651 - val_loss: 0.3997\n",
            "Epoch 78/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3651 - val_loss: 0.4018\n",
            "Epoch 79/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3647 - val_loss: 0.3991\n",
            "Epoch 80/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3652 - val_loss: 0.3998\n",
            "Epoch 81/300\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3650 - val_loss: 0.3976\n",
            "Epoch 82/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3645 - val_loss: 0.4019\n",
            "Epoch 83/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3663 - val_loss: 0.3998\n",
            "Epoch 84/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3636 - val_loss: 0.3978\n",
            "Epoch 85/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3648 - val_loss: 0.4028\n",
            "Epoch 86/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3642 - val_loss: 0.4046\n",
            "Epoch 87/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3660 - val_loss: 0.3988\n",
            "Epoch 88/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3658 - val_loss: 0.4036\n",
            "Epoch 89/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3652 - val_loss: 0.3996\n",
            "Epoch 90/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3645 - val_loss: 0.4039\n",
            "Epoch 91/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3642 - val_loss: 0.3966\n",
            "Epoch 92/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3642 - val_loss: 0.4000\n",
            "Epoch 93/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3646 - val_loss: 0.3955\n",
            "Epoch 94/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3643 - val_loss: 0.3961\n",
            "Epoch 95/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3637 - val_loss: 0.4035\n",
            "Epoch 96/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3641 - val_loss: 0.3980\n",
            "Epoch 97/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3655 - val_loss: 0.3977\n",
            "Epoch 98/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3640 - val_loss: 0.3944\n",
            "Epoch 99/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3641 - val_loss: 0.3927\n",
            "Epoch 100/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3646 - val_loss: 0.4009\n",
            "Epoch 101/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3639 - val_loss: 0.4083\n",
            "Epoch 102/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3629 - val_loss: 0.3996\n",
            "Epoch 103/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3642 - val_loss: 0.4015\n",
            "Epoch 104/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3646 - val_loss: 0.4010\n",
            "Epoch 105/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3645 - val_loss: 0.3969\n",
            "Epoch 106/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3637 - val_loss: 0.3944\n",
            "Epoch 107/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3632 - val_loss: 0.3955\n",
            "Epoch 108/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3644 - val_loss: 0.4072\n",
            "Epoch 109/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3648 - val_loss: 0.4029\n",
            "Epoch 110/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3645 - val_loss: 0.3968\n",
            "Epoch 111/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3634 - val_loss: 0.4068\n",
            "Epoch 112/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3654 - val_loss: 0.3954\n",
            "Epoch 113/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3636 - val_loss: 0.4017\n",
            "Epoch 114/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3634 - val_loss: 0.3973\n",
            "Epoch 115/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3634 - val_loss: 0.3935\n",
            "Epoch 116/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3625 - val_loss: 0.3983\n",
            "Epoch 117/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3643 - val_loss: 0.3976\n",
            "Epoch 118/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3647 - val_loss: 0.3975\n",
            "Epoch 119/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3638 - val_loss: 0.3983\n",
            "Epoch 120/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3652 - val_loss: 0.3950\n",
            "Epoch 121/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3625 - val_loss: 0.3982\n",
            "Epoch 122/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3618 - val_loss: 0.4023\n",
            "Epoch 123/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3634 - val_loss: 0.4003\n",
            "Epoch 124/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3624 - val_loss: 0.4001\n",
            "Epoch 125/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3637 - val_loss: 0.3967\n",
            "Epoch 126/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3651 - val_loss: 0.4048\n",
            "Epoch 127/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3645 - val_loss: 0.3982\n",
            "Epoch 128/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3630 - val_loss: 0.3997\n",
            "Epoch 129/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3634 - val_loss: 0.4029\n",
            "Epoch 130/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3630 - val_loss: 0.3984\n",
            "Epoch 131/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3642 - val_loss: 0.3973\n",
            "Epoch 132/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3625 - val_loss: 0.3934\n",
            "Epoch 133/300\n",
            "75/75 [==============================] - 1s 9ms/step - loss: 0.3631 - val_loss: 0.3964\n",
            "Epoch 134/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3641 - val_loss: 0.3965\n",
            "Epoch 135/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3612 - val_loss: 0.3978\n",
            "Epoch 136/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3643 - val_loss: 0.3958\n",
            "Epoch 137/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3644 - val_loss: 0.3999\n",
            "Epoch 138/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3635 - val_loss: 0.4075\n",
            "Epoch 139/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3624 - val_loss: 0.3988\n",
            "Epoch 140/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3646 - val_loss: 0.4001\n",
            "Epoch 141/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3639 - val_loss: 0.3959\n",
            "Epoch 142/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3634 - val_loss: 0.3962\n",
            "Epoch 143/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.3984\n",
            "Epoch 144/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3618 - val_loss: 0.4014\n",
            "Epoch 145/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3619 - val_loss: 0.3969\n",
            "Epoch 146/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3613 - val_loss: 0.3932\n",
            "Epoch 147/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3638 - val_loss: 0.3981\n",
            "Epoch 148/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3620 - val_loss: 0.3963\n",
            "Epoch 149/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3624 - val_loss: 0.4003\n",
            "Epoch 150/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3612 - val_loss: 0.4065\n",
            "Epoch 151/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3623 - val_loss: 0.4035\n",
            "Epoch 152/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3628 - val_loss: 0.4003\n",
            "Epoch 153/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3614 - val_loss: 0.3990\n",
            "Epoch 154/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3634 - val_loss: 0.3988\n",
            "Epoch 155/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.4016\n",
            "Epoch 156/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3625 - val_loss: 0.4065\n",
            "Epoch 157/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3633 - val_loss: 0.4017\n",
            "Epoch 158/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3630 - val_loss: 0.3970\n",
            "Epoch 159/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3621 - val_loss: 0.3972\n",
            "Epoch 160/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3626 - val_loss: 0.3990\n",
            "Epoch 161/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3626 - val_loss: 0.3936\n",
            "Epoch 162/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3616 - val_loss: 0.3952\n",
            "Epoch 163/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3616 - val_loss: 0.3982\n",
            "Epoch 164/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3644 - val_loss: 0.3936\n",
            "Epoch 165/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3616 - val_loss: 0.4009\n",
            "Epoch 166/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3624 - val_loss: 0.3980\n",
            "Epoch 167/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.3983\n",
            "Epoch 168/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3629 - val_loss: 0.3992\n",
            "Epoch 169/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3610 - val_loss: 0.4094\n",
            "Epoch 170/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3619 - val_loss: 0.3949\n",
            "Epoch 171/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3617 - val_loss: 0.3933\n",
            "Epoch 172/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.3959\n",
            "Epoch 173/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3625 - val_loss: 0.4010\n",
            "Epoch 174/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3628 - val_loss: 0.4002\n",
            "Epoch 175/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3613 - val_loss: 0.3964\n",
            "Epoch 176/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3621 - val_loss: 0.3960\n",
            "Epoch 177/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3626 - val_loss: 0.3952\n",
            "Epoch 178/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3613 - val_loss: 0.3985\n",
            "Epoch 179/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3610 - val_loss: 0.3960\n",
            "Epoch 180/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.4012\n",
            "Epoch 181/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3622 - val_loss: 0.3999\n",
            "Epoch 182/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3620 - val_loss: 0.3972\n",
            "Epoch 183/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3618 - val_loss: 0.3942\n",
            "Epoch 184/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3615 - val_loss: 0.3991\n",
            "Epoch 185/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.3968\n",
            "Epoch 186/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3622 - val_loss: 0.3959\n",
            "Epoch 187/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3615 - val_loss: 0.3921\n",
            "Epoch 188/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3607 - val_loss: 0.3979\n",
            "Epoch 189/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3622 - val_loss: 0.3998\n",
            "Epoch 190/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3620 - val_loss: 0.3954\n",
            "Epoch 191/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3612 - val_loss: 0.4045\n",
            "Epoch 192/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3620 - val_loss: 0.3944\n",
            "Epoch 193/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3636 - val_loss: 0.3947\n",
            "Epoch 194/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.3967\n",
            "Epoch 195/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3615 - val_loss: 0.3958\n",
            "Epoch 196/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3618 - val_loss: 0.3993\n",
            "Epoch 197/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3616 - val_loss: 0.4040\n",
            "Epoch 198/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3616 - val_loss: 0.3939\n",
            "Epoch 199/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3611 - val_loss: 0.4011\n",
            "Epoch 200/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3625 - val_loss: 0.3989\n",
            "Epoch 201/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3610 - val_loss: 0.3973\n",
            "Epoch 202/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3603 - val_loss: 0.3977\n",
            "Epoch 203/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3610 - val_loss: 0.3972\n",
            "Epoch 204/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3617 - val_loss: 0.3959\n",
            "Epoch 205/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3623 - val_loss: 0.4018\n",
            "Epoch 206/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3618 - val_loss: 0.3972\n",
            "Epoch 207/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3610 - val_loss: 0.3967\n",
            "Epoch 208/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.3995\n",
            "Epoch 209/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3608 - val_loss: 0.3956\n",
            "Epoch 210/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3605 - val_loss: 0.3937\n",
            "Epoch 211/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3609 - val_loss: 0.4009\n",
            "Epoch 212/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3629 - val_loss: 0.3969\n",
            "Epoch 213/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3603 - val_loss: 0.3940\n",
            "Epoch 214/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3629 - val_loss: 0.4052\n",
            "Epoch 215/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3623 - val_loss: 0.3925\n",
            "Epoch 216/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3613 - val_loss: 0.3963\n",
            "Epoch 217/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.3944\n",
            "Epoch 218/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3610 - val_loss: 0.3982\n",
            "Epoch 219/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3609 - val_loss: 0.3990\n",
            "Epoch 220/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3612 - val_loss: 0.3999\n",
            "Epoch 221/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3603 - val_loss: 0.4004\n",
            "Epoch 222/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3614 - val_loss: 0.3960\n",
            "Epoch 223/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.4019\n",
            "Epoch 224/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3610 - val_loss: 0.3950\n",
            "Epoch 225/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3615 - val_loss: 0.4015\n",
            "Epoch 226/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3608 - val_loss: 0.3975\n",
            "Epoch 227/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3608 - val_loss: 0.3964\n",
            "Epoch 228/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3618 - val_loss: 0.3974\n",
            "Epoch 229/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3604 - val_loss: 0.4006\n",
            "Epoch 230/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3597 - val_loss: 0.3976\n",
            "Epoch 231/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3614 - val_loss: 0.3947\n",
            "Epoch 232/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3605 - val_loss: 0.3938\n",
            "Epoch 233/300\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 0.3617 - val_loss: 0.3993\n",
            "Epoch 234/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3612 - val_loss: 0.3975\n",
            "Epoch 235/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3600 - val_loss: 0.3981\n",
            "Epoch 236/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3613 - val_loss: 0.3977\n",
            "Epoch 237/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3607 - val_loss: 0.4010\n",
            "Epoch 238/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3618 - val_loss: 0.3950\n",
            "Epoch 239/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3612 - val_loss: 0.3969\n",
            "Epoch 240/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3609 - val_loss: 0.3931\n",
            "Epoch 241/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3594 - val_loss: 0.3928\n",
            "Epoch 242/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3615 - val_loss: 0.3950\n",
            "Epoch 243/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3614 - val_loss: 0.3974\n",
            "Epoch 244/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3615 - val_loss: 0.3975\n",
            "Epoch 245/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3603 - val_loss: 0.3982\n",
            "Epoch 246/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3612 - val_loss: 0.3992\n",
            "Epoch 247/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3599 - val_loss: 0.3971\n",
            "Epoch 248/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3611 - val_loss: 0.3932\n",
            "Epoch 249/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3612 - val_loss: 0.4097\n",
            "Epoch 250/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3597 - val_loss: 0.3955\n",
            "Epoch 251/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3608 - val_loss: 0.3945\n",
            "Epoch 252/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3610 - val_loss: 0.3967\n",
            "Epoch 253/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3597 - val_loss: 0.4011\n",
            "Epoch 254/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3612 - val_loss: 0.3973\n",
            "Epoch 255/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3611 - val_loss: 0.3933\n",
            "Epoch 256/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3611 - val_loss: 0.3990\n",
            "Epoch 257/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3597 - val_loss: 0.3962\n",
            "Epoch 258/300\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 0.3600 - val_loss: 0.3983\n",
            "Epoch 259/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3586 - val_loss: 0.4045\n",
            "Epoch 260/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3609 - val_loss: 0.4020\n",
            "Epoch 261/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3617 - val_loss: 0.3970\n",
            "Epoch 262/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3616 - val_loss: 0.3964\n",
            "Epoch 263/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3608 - val_loss: 0.3998\n",
            "Epoch 264/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3600 - val_loss: 0.4001\n",
            "Epoch 265/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3603 - val_loss: 0.4013\n",
            "Epoch 266/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3602 - val_loss: 0.3953\n",
            "Epoch 267/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3609 - val_loss: 0.3992\n",
            "Epoch 268/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3607 - val_loss: 0.3967\n",
            "Epoch 269/300\n",
            "75/75 [==============================] - 1s 14ms/step - loss: 0.3605 - val_loss: 0.3984\n",
            "Epoch 270/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3592 - val_loss: 0.4030\n",
            "Epoch 271/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3603 - val_loss: 0.4001\n",
            "Epoch 272/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3589 - val_loss: 0.3942\n",
            "Epoch 273/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3601 - val_loss: 0.3955\n",
            "Epoch 274/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.4010\n",
            "Epoch 275/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3602 - val_loss: 0.3995\n",
            "Epoch 276/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3602 - val_loss: 0.3963\n",
            "Epoch 277/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3601 - val_loss: 0.3985\n",
            "Epoch 278/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3592 - val_loss: 0.4012\n",
            "Epoch 279/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3606 - val_loss: 0.3952\n",
            "Epoch 280/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3600 - val_loss: 0.3966\n",
            "Epoch 281/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3599 - val_loss: 0.3992\n",
            "Epoch 282/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3601 - val_loss: 0.3952\n",
            "Epoch 283/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3593 - val_loss: 0.3930\n",
            "Epoch 284/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3600 - val_loss: 0.3973\n",
            "Epoch 285/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3602 - val_loss: 0.3969\n",
            "Epoch 286/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3604 - val_loss: 0.3983\n",
            "Epoch 287/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3598 - val_loss: 0.4027\n",
            "Epoch 288/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3609 - val_loss: 0.3928\n",
            "Epoch 289/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3603 - val_loss: 0.4000\n",
            "Epoch 290/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3593 - val_loss: 0.3963\n",
            "Epoch 291/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3596 - val_loss: 0.3946\n",
            "Epoch 292/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3598 - val_loss: 0.3966\n",
            "Epoch 293/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3610 - val_loss: 0.3934\n",
            "Epoch 294/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3596 - val_loss: 0.4010\n",
            "Epoch 295/300\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 0.3613 - val_loss: 0.3977\n",
            "Epoch 296/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3603 - val_loss: 0.3975\n",
            "Epoch 297/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3599 - val_loss: 0.3954\n",
            "Epoch 298/300\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 0.3598 - val_loss: 0.3964\n",
            "Epoch 299/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3595 - val_loss: 0.3973\n",
            "Epoch 300/300\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 0.3599 - val_loss: 0.4014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f404a8b47b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqGS922oF6XY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae429b76-cc77-4abe-c43d-151d75a2a33d"
      },
      "source": [
        "# 예측\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred, 0).reshape(-1, )\n",
        "accuracy = (y_test == y_pred).mean()\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.84688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOzMI5PqJ_uH",
        "colab_type": "text"
      },
      "source": [
        "## _3_. LSTM 모델\n",
        "- embedding matrix 만들고 하면?\n",
        "- embedding matrix 안 만들고 하면?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unBWPiYuPQM8",
        "colab_type": "text"
      },
      "source": [
        "### embedding matrix 안 만들고 그냥 해 보기 : 최고 0.84416"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZufObJSPU-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "357ed88d-efb6-4ef7-faba-4385c63fad53"
      },
      "source": [
        "# 데이터 준비\n",
        "X_data = np.array(X_data_raw)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, -1)\n",
        "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: (18750, 1, 300), (18750,)\n",
            "Test: (6250, 1, 300), (6250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmzaSd8POtjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f633d83c-c974-42b0-bf87-4a4e7f0062fd"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "X_input = Input(batch_shape=(None, X_train.shape[1], X_train.shape[2]))\n",
        "# X_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(X_input)\n",
        "# X_flatten = Flatten()(X_lstm)\n",
        "# X_hidden = Dense(128)(X_flatten)\n",
        "X_lstm = Bidirectional(LSTM(128))(X_input)\n",
        "X_lstm = Dropout(0.2)(X_lstm)\n",
        "X_hidden = Dense(256)(X_lstm)\n",
        "X_hidden = Dropout(0.3)(X_hidden)\n",
        "y_output = Dense(1, activation='sigmoid')(X_hidden)\n",
        "\n",
        "model = Model(X_input, y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.002))\n",
        "print(\"============= 모델 전체 구조 =============\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============= 모델 전체 구조 =============\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 300)]          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 505,345\n",
            "Trainable params: 505,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeWFoAFEIMvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "6c617797-3d0b-48d3-c9ba-dc9fc40e1c90"
      },
      "source": [
        "BATCH = int(input('배치 사이즈 설정: '))\n",
        "EPOCHS = int(input('학습 에폭 설정: '))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# hist = model.fit(X_train, y_train,\n",
        "#                  batch_size=BATCH,\n",
        "#                  validation_split = 0.1,\n",
        "#                  epochs=EPOCHS,\n",
        "#                  callbacks=[es])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH,\n",
        "          validation_split=0.2,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "배치 사이즈 설정: 200\n",
            "학습 에폭 설정: 1000\n",
            "Epoch 1/1000\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 0.4256 - val_loss: 0.4071\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.3741 - val_loss: 0.3937\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3637 - val_loss: 0.4009\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.3569 - val_loss: 0.4009\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3501 - val_loss: 0.3958\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.3423 - val_loss: 0.3963\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.3383 - val_loss: 0.3952\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f400b4b5d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNq-yHc7Q19-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b141cd04-08db-4faf-e3f8-78ec268651c2"
      },
      "source": [
        "# 예측\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred, 0).reshape(-1, )\n",
        "accuracy = (y_test == y_pred).mean()\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.83904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggb8rClzQ4VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB9A-uugPIyT",
        "colab_type": "text"
      },
      "source": [
        "### embedding matrix 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwSnkOmTNcL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_len(m, sentences):\n",
        "    cnt = 0    \n",
        "    for sent in sentences:\n",
        "        if len(sent) <= m:\n",
        "            cnt += 1\n",
        "    \n",
        "    return f'전체 문장 중 길이가 {m} 이하인 샘플의 비율: {(cnt/len(sentences))*100}'\n",
        "\n",
        "def get_vector(word, pretrained):\n",
        "    if word in pretrained.wv.vocab.keys():\n",
        "        return pretrained.wv[word]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPO4Ln2FSOVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "09c42673-300d-4ab6-fce1-8a6525919407"
      },
      "source": [
        "# 문장 길이 설정\n",
        "for length in range(100, 1000, 50):\n",
        "    print(check_len(length, sentences))\n",
        "\n",
        "max_length = int(input('문장 길이 설정: '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 문장 중 길이가 100 이하인 샘플의 비율: 57.628\n",
            "전체 문장 중 길이가 150 이하인 샘플의 비율: 76.264\n",
            "전체 문장 중 길이가 200 이하인 샘플의 비율: 85.84\n",
            "전체 문장 중 길이가 250 이하인 샘플의 비율: 91.228\n",
            "전체 문장 중 길이가 300 이하인 샘플의 비율: 94.552\n",
            "전체 문장 중 길이가 350 이하인 샘플의 비율: 96.5\n",
            "전체 문장 중 길이가 400 이하인 샘플의 비율: 97.88\n",
            "전체 문장 중 길이가 450 이하인 샘플의 비율: 98.784\n",
            "전체 문장 중 길이가 500 이하인 샘플의 비율: 99.444\n",
            "전체 문장 중 길이가 550 이하인 샘플의 비율: 99.804\n",
            "전체 문장 중 길이가 600 이하인 샘플의 비율: 99.92\n",
            "전체 문장 중 길이가 650 이하인 샘플의 비율: 99.94800000000001\n",
            "전체 문장 중 길이가 700 이하인 샘플의 비율: 99.964\n",
            "전체 문장 중 길이가 750 이하인 샘플의 비율: 99.97200000000001\n",
            "전체 문장 중 길이가 800 이하인 샘플의 비율: 99.98\n",
            "전체 문장 중 길이가 850 이하인 샘플의 비율: 99.984\n",
            "전체 문장 중 길이가 900 이하인 샘플의 비율: 99.988\n",
            "전체 문장 중 길이가 950 이하인 샘플의 비율: 99.996\n",
            "문장 길이 설정: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jbm7O3ERpPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "db0a5d02-5b88-4fc3-af47-acc858fc8277"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sentences, sentiments,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=42)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, {len(y_train)}\")\n",
        "print(f\"Test: {len(X_test)}, {len(y_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 18750, 18750\n",
            "Test: 6250, 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwSva40dUlIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 토큰화\n",
        "Tokenizer().texts_to_sequences(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFOZjVSBTbNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "57342107-ebff-4cd1-fb22-fbca76d9d0a9"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_length) # truncate 되는 건가?\n",
        "X_test = pad_sequences(X_test, maxlen=max_length)\n",
        "print(\"========== 패딩 후 ==========\")\n",
        "print(f\"훈련 데이터: {X_train.shape}\")\n",
        "print(f\"테스트 데이터: {X_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-3b31e47754f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# truncate 되는 건가?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"========== 패딩 후 ==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"훈련 데이터: {X_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"테스트 데이터: {X_test.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    156\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m keras_export(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'along'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNdzQlKlTZ54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAekxIORNc9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5322456-fb22-408c-da48-f6e87e13f981"
      },
      "source": [
        "print(doc_model)\n",
        "keys = list(doc_model.wv.vocab.keys())\n",
        "vocab_size = len(keys)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Doc2Vec(dm/m,d300,n5,w5,mc10,s0.001,t4)\n",
            "19717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqsynRw-MnEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# doc2vec 가중치 행렬\n",
        "embedding_vocab = len(keys)\n",
        "embedding_dim = 300\n",
        "g_embed_300 = np.zeros((embedding_vocab, embedding_dim))\n",
        "for word, idx in enumerate(keys):\n",
        "    temp = get_vector(word, doc_model)\n",
        "    if temp is not None:\n",
        "        g_embed_300[idx] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk_jxr0wS6t1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4867256-d8fe-4063-ed02-c53d35f8a875"
      },
      "source": [
        "g_embed_300.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19717, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBrqt2eKTG48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Embedding(input_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzeR95U7RC0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "X_input = Input(batch_shape=(None, X_train.shape[1])\n",
        "X_embed = Embedding(input_dim=vocab_size, output_dim=300, input_length=)(X_input)\n",
        "X_lstm = LSTM(128)(X_embed)\n",
        "y_output = Dense(1, activation='sigmoid')(X_lstm)\n",
        "\n",
        "model = Model(X_input, y_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "print(\"============= 모델 전체 구조 =============\")\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_len, \n",
        "                    weights=[g_embed_100],\n",
        "                    mask_zero=True, trainable=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epj8JzJCIHl3",
        "colab_type": "text"
      },
      "source": [
        "# ============ 테스트 ============"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBcxyUn-RFh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "533964c5-31f6-41cf-bf05-046e45ed399f"
      },
      "source": [
        "for i, doc in enumerate(sentences):\n",
        "    print(TaggedDocument(doc, [i]))\n",
        "    if i == 5:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TaggedDocument(['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter'], [0])\n",
            "TaggedDocument(['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'every', 'year', 'e', 'g', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'everyone', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'critics', 'look', 'criticize', 'everything', 'others', 'rate', 'movie', 'important', 'bases', 'like', 'entertained', 'people', 'never', 'agree', 'critics', 'enjoyed', 'effort', 'mr', 'hines', 'put', 'faithful', 'h', 'g', 'wells', 'classic', 'novel', 'found', 'entertaining', 'made', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings'], [1])\n",
            "TaggedDocument(['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', 'jurassik', 'park', 'scientists', 'resurrect', 'one', 'nature', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'however', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'scientific', 'meanwhile', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'pre', 'historical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'though', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'one', 'nature', 'fearsome', 'predators', 'furthermore', 'third', 'sabretooth', 'dangerous', 'slow', 'stalks', 'victims', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hair', 'raising', 'chills', 'full', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'effects', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'made', 'computer', 'generator', 'seem', 'totally', 'lousy', 'middling', 'performances', 'though', 'players', 'reacting', 'appropriately', 'becoming', 'food', 'actors', 'give', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'running', 'bound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realistic', 'gory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', 'sabretooth', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'much', 'better', 'bc', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'many', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'many', 'others', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxanne', 'robinson', 'crusoe', 'rating', 'average', 'bottom', 'barrel'], [2])\n",
            "TaggedDocument(['must', 'assumed', 'praised', 'film', 'greatest', 'filmed', 'opera', 'ever', 'read', 'somewhere', 'either', 'care', 'opera', 'care', 'wagner', 'care', 'anything', 'except', 'desire', 'appear', 'cultured', 'either', 'representation', 'wagner', 'swan', 'song', 'movie', 'strikes', 'unmitigated', 'disaster', 'leaden', 'reading', 'score', 'matched', 'tricksy', 'lugubrious', 'realisation', 'text', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'one', 'shakespeare', 'allowed', 'anywhere', 'near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'without', 'smallest', 'justification', 'wagner', 'text', 'decided', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'latter', 'stages', 'transmutes', 'kind', 'beatnik', 'babe', 'though', 'one', 'continues', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'get', 'double', 'dose', 'armin', 'jordan', 'conductor', 'seen', 'face', 'heard', 'voice', 'amfortas', 'also', 'appears', 'monstrously', 'double', 'exposure', 'kind', 'batonzilla', 'conductor', 'ate', 'monsalvat', 'playing', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represented', 'scattering', 'shopworn', 'flaccid', 'crocuses', 'stuck', 'ill', 'laid', 'turf', 'expedient', 'baffles', 'theatre', 'sometimes', 'piece', 'imperfections', 'thoughts', 'think', 'syberberg', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provided', 'julie', 'andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voices', 'trumpets', 'particular', 'possessing', 'aural', 'glare', 'adds', 'another', 'sort', 'fatigue', 'impatience', 'uninspired', 'conducting', 'paralytic', 'unfolding', 'ritual', 'someone', 'another', 'review', 'mentioned', 'bayreuth', 'recording', 'knappertsbusch', 'though', 'tempi', 'often', 'slow', 'jordan', 'altogether', 'lacks', 'sense', 'pulse', 'feeling', 'ebb', 'flow', 'music', 'half', 'century', 'orchestral', 'sound', 'set', 'modern', 'pressings', 'still', 'superior', 'film'], [3])\n",
            "TaggedDocument(['superbly', 'trashy', 'wondrously', 'unpretentious', 'exploitation', 'hooray', 'pre', 'credits', 'opening', 'sequences', 'somewhat', 'give', 'false', 'impression', 'dealing', 'serious', 'harrowing', 'drama', 'need', 'fear', 'barely', 'ten', 'minutes', 'later', 'necks', 'nonsensical', 'chainsaw', 'battles', 'rough', 'fist', 'fights', 'lurid', 'dialogs', 'gratuitous', 'nudity', 'bo', 'ingrid', 'two', 'orphaned', 'siblings', 'unusually', 'close', 'even', 'slightly', 'perverted', 'relationship', 'imagine', 'playfully', 'ripping', 'towel', 'covers', 'sister', 'naked', 'body', 'stare', 'unshaven', 'genitals', 'several', 'whole', 'minutes', 'well', 'bo', 'sister', 'judging', 'dubbed', 'laughter', 'mind', 'sick', 'dude', 'anyway', 'kids', 'fled', 'russia', 'parents', 'nasty', 'soldiers', 'brutally', 'slaughtered', 'mommy', 'daddy', 'friendly', 'smuggler', 'took', 'custody', 'however', 'even', 'raised', 'trained', 'bo', 'ingrid', 'expert', 'smugglers', 'actual', 'plot', 'lifts', 'years', 'later', 'facing', 'ultimate', 'quest', 'mythical', 'incredibly', 'valuable', 'white', 'fire', 'diamond', 'coincidentally', 'found', 'mine', 'things', 'life', 'ever', 'made', 'little', 'sense', 'plot', 'narrative', 'structure', 'white', 'fire', 'sure', 'lot', 'fun', 'watch', 'time', 'clue', 'beating', 'cause', 'bet', 'actors', 'understood', 'even', 'less', 'whatever', 'violence', 'magnificently', 'grotesque', 'every', 'single', 'plot', 'twist', 'pleasingly', 'retarded', 'script', 'goes', 'totally', 'bonkers', 'beyond', 'repair', 'suddenly', 'reveal', 'reason', 'bo', 'needs', 'replacement', 'ingrid', 'fred', 'williamson', 'enters', 'scene', 'big', 'cigar', 'mouth', 'sleazy', 'black', 'fingers', 'local', 'prostitutes', 'bo', 'principal', 'opponent', 'italian', 'chick', 'big', 'breasts', 'hideous', 'accent', 'preposterous', 'catchy', 'theme', 'song', 'plays', 'least', 'dozen', 'times', 'throughout', 'film', 'obligatory', 'falling', 'love', 'montage', 'loads', 'attractions', 'god', 'brilliant', 'experience', 'original', 'french', 'title', 'translates', 'life', 'survive', 'uniquely', 'appropriate', 'makes', 'much', 'sense', 'rest', 'movie', 'none'], [4])\n",
            "TaggedDocument(['dont', 'know', 'people', 'think', 'bad', 'movie', 'got', 'pretty', 'good', 'plot', 'good', 'action', 'change', 'location', 'harry', 'hurt', 'either', 'sure', 'offensive', 'gratuitous', 'movie', 'like', 'eastwood', 'good', 'form', 'dirty', 'harry', 'liked', 'pat', 'hingle', 'movie', 'small', 'town', 'cop', 'liked', 'dirty', 'harry', 'see', 'one', 'lot', 'better', 'dead', 'pool'], [5])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}