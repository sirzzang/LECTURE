# -*- coding: utf-8 -*-
"""NLP-Keras-IMDB-CNN-Embedding-practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18oXXxFqbIFPxXXi_su8kvVE1C1WbPLct
"""

# 모듈 불러 오기
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Dropout
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics.pairwise import euclidean_distances
import numpy as np

# 파라미터 설정
max_features = int(input('어휘 집합 수 설정: '))
max_length = int(input('문장 최대 길이 설정: '))

# IMDB 데이터 로드 및 확인
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = max_features)
print(X_train[0])

# 문장 패딩
X_train = pad_sequences(X_train, maxlen=max_length)
X_test = pad_sequences(X_test, maxlen=max_length)

# 모델 파라미터
n_embed = int(input('임베딩 차원 설정: '))
n_kernels = int(input('컨볼루션 필터 수 설정: '))
s_kernels = int(input('컨볼루션 필터 사이즈 설정: '))
n_hidden = int(input('은닉 노드 수 설정: '))
EPOCHS = int(input('학습 에폭 설정: '))
BATCH = int(input('배치 사이즈 설정: '))

# 모델 네트워크 구성
X_input = Input(batch_shape=(None, max_length)) # 문장 길이 만큼 입력
X_embed = Embedding(input_dim=max_features, output_dim=n_embed)(X_input) # 어휘 집합 사이즈, 임베딩 차원
X_embed = Dropout(0.5)(X_embed)
X_conv = Conv1D(filters=n_kernels, kernel_size=s_kernels, strides=1, padding='valid', activation='relu')(X_embed) # 임베딩 입력
X_pool = GlobalMaxPooling1D()(X_conv)
X_dense = Dense(n_hidden, activation='relu')(X_pool) # 은닉층
X_dense = Dropout(0.5)(X_dense)
y_output = Dense(1, activation='sigmoid')(X_dense) # 이진분류

# 모델 구성
model = Model(X_input, y_output)
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))
print("============ 모델 전체 구조 ============")
print(model.summary())

# 학습
es = EarlyStopping(monitor='val_loss', patience=4, verbose=1)
hist = model.fit(X_train, y_train,
                 batch_size=BATCH,
                 epochs=EPOCHS,
                 validation_data=(X_test, y_test),
                 callbacks=[es])

# loss 시각화
plt.plot(hist.history['loss'], label='Train Loss')
plt.plot(hist.history['val_loss'], label='Test Loss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.title('Loss Trajectory', size=15)
plt.show()

# 예측 후 성능 확인
y_pred = model.predict(X_test)
y_pred = np.where(y_pred > 0.5, 1, 0) # 이진 분류 문제이므로
print(f"Test Accuracy: {accuracy_score(y_test, y_pred)}")

#########################################################################
# 임베딩 레이어 가중치 행렬 확인
W_embed = np.array(model.layers[1].get_weights()) # shape: (1, 6000, 60)
W_embed = W_embed.reshape(max_features, n_embed) # 2차원 행렬 구조로 reshape: 6000, 60.
print(W_embed)

# father-mother-daughter-son 거리 측정
father = W_embed[word2idx['father']]
mother = W_embed[word2idx['mother']]
daughter = W_embed[word2idx['daughter']]
son = W_embed[word2idx['son']]
print("============ 유클리드 거리 측정 ============")
print(euclidean_distances([father, mother, daughter, son]))

#########################################################################
# word index dictioanry
word2idx = imdb.get_word_index() # 기존 word index: 단어-인덱스 구조.
idx2word = dict((v, k) for k, v in word2idx.items()) # 인덱스-단어 구조 변환

# 0, 1, 2, 3 추가
idx2word = dict((idx+3, word) for idx, word in idx2word.items()) # 3씩 밀어 주기
idx2word[0] = '<PAD>'
idx2word[1] = '<START>'
idx2word[2] = '<OOV>'
idx2word[3] = '<INV>' # invalid 문자

# word : index 구조로 변환
word2idx = dict((k, v) for v, k in idx2word.items())

# 실제 단어로 변환하는 함수
def decode_sent(sent):
    x = [idx2word[s] for s in sent]
    return ' '.join(x)

# 첫 번째 문장의 임베딩 벡터 확인
print("============ 원래 문장 확인 ============")
print(decode_sent(X_train[0]))
print('')

embed_model = Model(X_input, X_embed)
embed_sent = embed_model.predict(X_train[0].reshape(1, max_length)) # 원래 문장을 2차원으로 맞춘 후
print("============ 임베딩된 문장 확인 ============")
print(embed_sent.shape)
print(embed_sent)