{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-TFIDF-logic-practice.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqlGkleH1WgL",
        "colab_type": "text"
      },
      "source": [
        "# 내 풀이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMbytQv_y70x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7HVLMCGrzVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocabulary 생성\n",
        "def build_dictionary(corpus):\n",
        "    vocab_dict = {}\n",
        "    for c in corpus:\n",
        "        vocabs = c.strip().split() # 각 문장 내 unique 어휘 찾기\n",
        "        for vocab in vocabs:\n",
        "            vocab_key = vocab.lower() # 소문자 변환\n",
        "            if vocab_key not in vocab_dict: # dictonary 내 색인 생성\n",
        "                vocab_dict[vocab_key] = 0\n",
        "    \n",
        "    return sorted(vocab_dict)\n",
        "\n",
        "# TF 계산\n",
        "def calc_TF(corpus, vocab_dict):\n",
        "    flag = len(vocab_dict)\n",
        "    TF_matrix = [[v, 0]  for _ in range(len(corpus)) for v in vocab_dict]\n",
        "    for i in range(len(corpus)):\n",
        "        sent = corpus[i].strip().lower().split()\n",
        "        vocab_cnt = 0\n",
        "        for s in sent:\n",
        "            for vocab in TF_matrix[i*flag:(i+1)*flag]:\n",
        "                if s == vocab[0]:\n",
        "                    vocab[1] += 1 # 빈도 추가\n",
        "                    vocab_cnt += 1 # 총 등장 빈도 수\n",
        "        # print(f\"{sent}에서 총 등장한 어휘 빈도 수 : {vocab_cnt}\")\n",
        "        for j in range(flag):\n",
        "            TF_matrix[i*flag:(i+1)*flag][j][1] /= vocab_cnt\n",
        "\n",
        "    return TF_matrix\n",
        "\n",
        "# IDF 계산\n",
        "def calc_IDF(corpus, vocab_dict):\n",
        "    # IDF 행렬 초기화\n",
        "    IDF_matrix = [[vocab, 0] for vocab in vocab_dict]\n",
        "\n",
        "    # IDF 계산\n",
        "    for j in range(len(IDF_matrix)):\n",
        "        for i in range(1, len(corpus)):\n",
        "            sent = corpus[i].strip().lower().split()\n",
        "            if IDF_matrix[j][0] in sent:\n",
        "                IDF_matrix[j][0]\n",
        "                IDF_matrix[j][1] += 1\n",
        "    \n",
        "    for i in range(len(IDF_matrix)):\n",
        "        IDF_matrix[i][1] = np.log((len(corpus)-1)/IDF_matrix[i][1])\n",
        "\n",
        "    return IDF_matrix * len(corpus)\n",
        "\n",
        "# TF-IDF 계산\n",
        "def calc_TFIDF(TF, IDF, corpus, vocab_dict):\n",
        "    flag = len(vocab_dict)\n",
        "    TFIDF_matrix = []\n",
        "\n",
        "    TFIDF_matrix = []\n",
        "    for i in range(len(corpus)):\n",
        "        tf = np.array([tf[1] for tf in TF[i*flag:(i+1)*flag]], dtype=np.float32)\n",
        "        idf = np.array([idf[1] for idf in IDF[i*flag:(i+1)*flag]], dtype=np.float32)\n",
        "        TFIDF_matrix.append(tf*idf)\n",
        "    return np.array(TFIDF_matrix).T\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "def calc_cos_sim(A, B):\n",
        "    # 분자\n",
        "    bunja = A * B\n",
        "\n",
        "    # 분모\n",
        "    norm_A, norm_B = 0, 0\n",
        "    for a in A:\n",
        "        norm_A += a**2\n",
        "    for b in B:\n",
        "        norm_B += b**2\n",
        "    bunmo = np.sqrt(norm_A) * np.sqrt(norm_B)\n",
        "\n",
        "    return (bunja / bunmo).T.sum()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kRoudZczCfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "f7201c37-b5d8-4afa-a39d-e9fe5522aa94"
      },
      "source": [
        "# 테스트\n",
        "sentences = [\n",
        "             'gold silver truck',\n",
        "             'Shipment of gold damaged in a fire',\n",
        "             'Delivery of silver arrived in a silver truck',\n",
        "             'Shipment of gold arrived in a truck'\n",
        "             ]\n",
        "\n",
        "dictionary = build_dictionary(sentences)\n",
        "print(f\"vocabulary 사전: \\n{dictionary}\")\n",
        "print(\"\")\n",
        "\n",
        "tf_matrix = calc_TF(sentences, dictionary)\n",
        "print(f\"Term Frequency: \\n{tf_matrix}\")\n",
        "print(\"\")\n",
        "\n",
        "idf_matrix = calc_IDF(sentences, dictionary)\n",
        "print(f\"Inverse Document Frequency: \\n{idf_matrix}\")\n",
        "print(\"\")\n",
        "\n",
        "tfidf_matrix = calc_TFIDF(tf_matrix, idf_matrix, sentences, dictionary)\n",
        "print(f\"TF-IDF: \\n{tfidf_matrix}\")\n",
        "print(\"\")\n",
        "\n",
        "cos_sim_docs = {}\n",
        "for i in range(1, len(sentences)):\n",
        "    cos_sim_docs[sentences[i]] = calc_cos_sim(tfidf_matrix.T[0], tfidf_matrix.T[i])\n",
        "print(f\"문서 간 코사인유사도: \\n{cos_sim_docs}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary 사전: \n",
            "['a', 'arrived', 'damaged', 'delivery', 'fire', 'gold', 'in', 'of', 'shipment', 'silver', 'truck']\n",
            "\n",
            "Term Frequency: \n",
            "[['a', 0.0], ['arrived', 0.0], ['damaged', 0.0], ['delivery', 0.0], ['fire', 0.0], ['gold', 0.3333333333333333], ['in', 0.0], ['of', 0.0], ['shipment', 0.0], ['silver', 0.3333333333333333], ['truck', 0.3333333333333333], ['a', 0.14285714285714285], ['arrived', 0.0], ['damaged', 0.14285714285714285], ['delivery', 0.0], ['fire', 0.14285714285714285], ['gold', 0.14285714285714285], ['in', 0.14285714285714285], ['of', 0.14285714285714285], ['shipment', 0.14285714285714285], ['silver', 0.0], ['truck', 0.0], ['a', 0.125], ['arrived', 0.125], ['damaged', 0.0], ['delivery', 0.125], ['fire', 0.0], ['gold', 0.0], ['in', 0.125], ['of', 0.125], ['shipment', 0.0], ['silver', 0.25], ['truck', 0.125], ['a', 0.14285714285714285], ['arrived', 0.14285714285714285], ['damaged', 0.0], ['delivery', 0.0], ['fire', 0.0], ['gold', 0.14285714285714285], ['in', 0.14285714285714285], ['of', 0.14285714285714285], ['shipment', 0.14285714285714285], ['silver', 0.0], ['truck', 0.14285714285714285]]\n",
            "\n",
            "Inverse Document Frequency: \n",
            "[['a', 0.0], ['arrived', 0.4054651081081644], ['damaged', 1.0986122886681098], ['delivery', 1.0986122886681098], ['fire', 1.0986122886681098], ['gold', 0.4054651081081644], ['in', 0.0], ['of', 0.0], ['shipment', 0.4054651081081644], ['silver', 1.0986122886681098], ['truck', 0.4054651081081644], ['a', 0.0], ['arrived', 0.4054651081081644], ['damaged', 1.0986122886681098], ['delivery', 1.0986122886681098], ['fire', 1.0986122886681098], ['gold', 0.4054651081081644], ['in', 0.0], ['of', 0.0], ['shipment', 0.4054651081081644], ['silver', 1.0986122886681098], ['truck', 0.4054651081081644], ['a', 0.0], ['arrived', 0.4054651081081644], ['damaged', 1.0986122886681098], ['delivery', 1.0986122886681098], ['fire', 1.0986122886681098], ['gold', 0.4054651081081644], ['in', 0.0], ['of', 0.0], ['shipment', 0.4054651081081644], ['silver', 1.0986122886681098], ['truck', 0.4054651081081644], ['a', 0.0], ['arrived', 0.4054651081081644], ['damaged', 1.0986122886681098], ['delivery', 1.0986122886681098], ['fire', 1.0986122886681098], ['gold', 0.4054651081081644], ['in', 0.0], ['of', 0.0], ['shipment', 0.4054651081081644], ['silver', 1.0986122886681098], ['truck', 0.4054651081081644]]\n",
            "\n",
            "TF-IDF: \n",
            "[[0.         0.         0.         0.        ]\n",
            " [0.         0.         0.05068314 0.05792359]\n",
            " [0.         0.15694462 0.         0.        ]\n",
            " [0.         0.         0.13732654 0.        ]\n",
            " [0.         0.15694462 0.         0.        ]\n",
            " [0.13515504 0.05792359 0.         0.05792359]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.05792359 0.         0.05792359]\n",
            " [0.3662041  0.         0.27465308 0.        ]\n",
            " [0.13515504 0.         0.05068314 0.05792359]]\n",
            "\n",
            "문서 간 코사인유사도: \n",
            "{'Shipment of gold damaged in a fire': 0.080104515, 'Delivery of silver arrived in a silver truck': 0.8247514, 'Shipment of gold arrived in a truck': 0.32718456}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPj5WICG1S9p",
        "colab_type": "text"
      },
      "source": [
        "# 다른 사람 풀이"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXpvyA971U_Y",
        "colab_type": "text"
      },
      "source": [
        "## 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8N-WDTN1aby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "eaa51d4a-6902-49b0-e0c4-14d547e64972"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "Search = input(\"\")\n",
        "\n",
        "DOC1 = str.split('shipment of gold damaged in a fire')\n",
        "DOC2 = str.split('delivery of silver arrived in a silver truck')\n",
        "DOC3 = str.split('shipment of gold arrived in a truck')\n",
        "\n",
        "Terms_raw = DOC1+DOC2+DOC3+str.split(Search)\n",
        "\n",
        "Search2 = str.split(Search)\n",
        "\n",
        "Terms = list(set(Terms_raw))\n",
        "Terms.sort()\n",
        "print(Terms)\n",
        "\n",
        "DF = []\n",
        "for Term in Terms:\n",
        "     DF.append(list(set(DOC1)).count(Term)+\n",
        "               list(set(DOC2)).count(Term)+\n",
        "               list(set(DOC3)).count(Term))\n",
        "    \n",
        "IDF = []\n",
        "for df in DF:\n",
        "    IDF.append(np.log10(3/df))\n",
        "print(IDF)\n",
        "\n",
        "TF1 = []\n",
        "TF2 = []\n",
        "TF3 = []\n",
        "TFsearch = []\n",
        "\n",
        "for Term in Terms:\n",
        "    TF1.append(DOC1.count(Term)/len(DOC1))\n",
        "    TF2.append(DOC2.count(Term)/len(DOC2))\n",
        "    TF3.append(DOC3.count(Term)/len(DOC3))\n",
        "    TFsearch.append(Search2.count(Term)/len(Search2))\n",
        "    \n",
        "TF1_IDF = []\n",
        "TF2_IDF = []\n",
        "TF3_IDF = []\n",
        "TFsearch_IDF = []\n",
        "for i in range(len(Terms)):\n",
        "     TF1_IDF.append(IDF[i]*TF1[i])\n",
        "     TF2_IDF.append(IDF[i]*TF2[i])\n",
        "     TF3_IDF.append(IDF[i]*TF3[i])\n",
        "     TFsearch_IDF.append(IDF[i]*TFsearch[i])\n",
        "print(TFsearch_IDF)\n",
        "\n",
        "\n",
        "TF1_IDF= np.array(TF1_IDF)\n",
        "TFsearch_IDF = np.array(TFsearch_IDF)\n",
        "def cos_sim(A, B):\n",
        "       return dot(A, B)/(norm(A)*norm(B))\n",
        "\n",
        "print(cos_sim(TFsearch_IDF,TF1_IDF))\n",
        "print(cos_sim(TFsearch_IDF,TF2_IDF))\n",
        "print(cos_sim(TFsearch_IDF,TF3_IDF))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gold silver truck\n",
            "['a', 'arrived', 'damaged', 'delivery', 'fire', 'gold', 'in', 'of', 'shipment', 'silver', 'truck']\n",
            "[0.0, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.0, 0.0, 0.17609125905568124, 0.47712125471966244, 0.17609125905568124]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.058697086351893746, 0.0, 0.0, 0.0, 0.15904041823988746, 0.058697086351893746]\n",
            "0.08010451753994623\n",
            "0.8247514231034944\n",
            "0.32718457421366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGt3Q6Xx1gn4",
        "colab_type": "text"
      },
      "source": [
        "## 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfh1_Wdc1hdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "41faed27-0459-4489-84c9-5d203271b5b2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log\n",
        "\n",
        "# 단어리스트 만들기\n",
        "docs= [\"gold silver truck\",\\\n",
        "    \"Shipment of gold damaged in a fire\",\\\n",
        "        \"Delivery of silver arrived in a silver truck\",\\\n",
        "            \"Shipment of gold arrived in a truck\"]\n",
        "wordset= list(set(w for doc in docs for w in doc.split()))\n",
        "wordset.sort()\n",
        "\n",
        "# 함수 정의하기\n",
        "N= len(docs)\n",
        "\n",
        "def tf(t,d):\n",
        "    if len(t) !=1:\n",
        "        return d.count(t)\n",
        "    else:\n",
        "        return int(t in d)\n",
        "\n",
        "def idf(t):\n",
        "    df = 0\n",
        "    for doc in docs:\n",
        "            df += t in doc\n",
        "    return log(N/df)\n",
        "\n",
        "def tfidf(t, d):\n",
        "    return tf(t,d)* idf(t)\n",
        "\n",
        "# DTM\n",
        "result = []\n",
        "for i in range(N):\n",
        "    result.append([])\n",
        "    d = docs[i]\n",
        "    for j in range(len(wordset)):\n",
        "        t = wordset[j]        \n",
        "        result[-1].append(tf(t, d))\n",
        "\n",
        "dtm_ = pd.DataFrame(result, columns = wordset)\n",
        "dtm_\n",
        "\n",
        "df_= dtm_.sum(axis= 1)\n",
        "\n",
        "# IDF\n",
        "result2 = []\n",
        "for j in range(len(wordset)):\n",
        "    t = wordset[j]\n",
        "    result2.append(idf(t))\n",
        "\n",
        "idf_ = pd.DataFrame(result2, index = wordset, columns = [\"IDF\"])\n",
        "idf_\n",
        "\n",
        "# TF-IDF\n",
        "result3 = []\n",
        "for i in range(N):\n",
        "    result3.append([])\n",
        "    d = docs[i]\n",
        "    for j in range(len(wordset)):\n",
        "        t = wordset[j]\n",
        "\n",
        "        result3[-1].append(tfidf(t,d))\n",
        "\n",
        "tfidf_ = pd.DataFrame(result3, columns = wordset)\n",
        "tfidf_\n",
        "\n",
        "print(\"DTM: \\n\",dtm_)\n",
        "print(\"\\n DF: \\n\",df_)\n",
        "print(\"\\n IDF: \\n\", idf_)\n",
        "print(\"\\n TFIDF: \\n\", tfidf_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DTM: \n",
            "    Delivery  Shipment  a  arrived  damaged  fire  gold  in  of  silver  truck\n",
            "0         0         0  0        0        0     0     1   0   0       1      1\n",
            "1         0         1  1        0        1     1     1   1   1       0      0\n",
            "2         1         0  1        1        0     0     0   1   1       2      1\n",
            "3         0         1  1        1        0     0     1   1   1       0      1\n",
            "\n",
            " DF: \n",
            " 0    3\n",
            "1    7\n",
            "2    8\n",
            "3    7\n",
            "dtype: int64\n",
            "\n",
            " IDF: \n",
            "                IDF\n",
            "Delivery  1.386294\n",
            "Shipment  0.693147\n",
            "a         0.287682\n",
            "arrived   0.693147\n",
            "damaged   1.386294\n",
            "fire      1.386294\n",
            "gold      0.287682\n",
            "in        0.287682\n",
            "of        0.287682\n",
            "silver    0.693147\n",
            "truck     0.287682\n",
            "\n",
            " TFIDF: \n",
            "    Delivery  Shipment         a  ...        of    silver     truck\n",
            "0  0.000000  0.000000  0.000000  ...  0.000000  0.693147  0.287682\n",
            "1  0.000000  0.693147  0.287682  ...  0.287682  0.000000  0.000000\n",
            "2  1.386294  0.000000  0.287682  ...  0.287682  1.386294  0.287682\n",
            "3  0.000000  0.693147  0.287682  ...  0.287682  0.000000  0.287682\n",
            "\n",
            "[4 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb0yv_BL2Yf6",
        "colab_type": "text"
      },
      "source": [
        "## 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUlWPATA2YSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "b5eba366-c157-454f-c055-f4a3ce4ab668"
      },
      "source": [
        "# 자연어처리 쿡북 : p.227, 텍스트 유사도 문제 (TFIDF)\n",
        "import nltk\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "docs = [\n",
        "            'gold silver truck',\n",
        "            'Shipment of gold damaged in a fire',\n",
        "            'Delivery of silver arrived in a silver truck',\n",
        "            'Shipment of gold arrived in a truck'\n",
        "        ]\n",
        "\n",
        "word_set = set()\n",
        "for doc in docs:\n",
        "    words = nltk.word_tokenize(doc.lower())\n",
        "    word_set = word_set.union(set(words))\n",
        "word_list = list(word_set)\n",
        "word_list.sort()\n",
        "word_list\n",
        "\n",
        "def TF(my_word, doc):\n",
        "    count = 0\n",
        "\n",
        "    words = nltk.word_tokenize(doc.lower())\n",
        "    for word in words:\n",
        "        if my_word == word:\n",
        "            count+=1\n",
        "    return count/len(words)\n",
        "TF('silver', docs[2])\n",
        "\n",
        "def IDF(my_word, docs):\n",
        "    count = 0\n",
        "    for doc in docs:\n",
        "        words = nltk.word_tokenize(doc.lower())\n",
        "        for word in words:\n",
        "            if my_word == word:\n",
        "                count+=1\n",
        "                break\n",
        "    return len(docs) / count\n",
        "\n",
        "word_idf_dict = {}\n",
        "for word in word_list:\n",
        "    word_idf_dict[word] = IDF(word, docs)\n",
        "word_idf_dict\n",
        "\n",
        "import math \n",
        "\n",
        "def TF_IDF(doc):\n",
        "    doc_tf_idf_list = []\n",
        "    for word in word_list:\n",
        "        tf = TF(word, doc)\n",
        "        word_tf_idf = tf*math.log(word_idf_dict[word])\n",
        "        doc_tf_idf_list.append(word_tf_idf)\n",
        "    return doc_tf_idf_list\n",
        "TF_IDF(docs[1])\n",
        "\n",
        "doc_TF_IDF_list = []\n",
        "for doc in docs:\n",
        "    doc_TF_IDF_list.append(TF_IDF(doc))\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(docs)\n",
        "cosine_similarity(doc_TF_IDF_list, doc_TF_IDF_list)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['gold silver truck', 'Shipment of gold damaged in a fire', 'Delivery of silver arrived in a silver truck', 'Shipment of gold arrived in a truck']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.0477257 , 0.60185049, 0.17564754],\n",
              "       [0.0477257 , 1.        , 0.05333544, 0.32078621],\n",
              "       [0.60185049, 0.05333544, 1.        , 0.32078621],\n",
              "       [0.17564754, 0.32078621, 0.32078621, 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiOaXYF41o30",
        "colab_type": "text"
      },
      "source": [
        "# 강사님 풀이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jkk2FDD1pMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "2eea5d0d-aa29-4fdb-a245-0a056b50dc98"
      },
      "source": [
        "# TFIDF 연습\n",
        "# 2020.07-21\n",
        "# ----------\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# 1. dictionary를 생성한다.\n",
        "def makeVocab(sentences):\n",
        "    words = [word for sentence in sentences for word in sentence.split()]\n",
        "    words = list(set(words))\n",
        "    words.sort()\n",
        "    return {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "# 2. TF를 생성한다.\n",
        "def makeTF(sentences):\n",
        "    vocab = makeVocab(sentences)\n",
        "    tf = np.zeros((len(vocab), len(sentences)))\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        freq = nltk.FreqDist(nltk.word_tokenize(sentence))\n",
        "        for key in freq.keys():\n",
        "            tf[vocab[key], i] = freq[key] / len(sentence)\n",
        "    return tf\n",
        "\n",
        "# 3. IDF를 생성한다.\n",
        "def makeIDF(sentences, tf):\n",
        "    df = tf.shape[1] - (tf == 0.0).sum(axis=1)\n",
        "    return np.log(tf.shape[1] / (0+df))\n",
        "\n",
        "# 4. TFIDF를 생성한다.\n",
        "def makeTFIDF(sentences):\n",
        "    tf = makeTF(sentences)\n",
        "    idf = makeIDF(sentences, tf)\n",
        "    return np.multiply(tf, idf.reshape(tf.shape[0], 1))\n",
        "\n",
        "sentences = ['gold silver truck', 'shipment of gold damaged in a fire', 'delivery of silver arrived in a silver truck', 'shipment of gold arrived in a truck']\n",
        "tfidf = makeTFIDF(sentences);\n",
        "print(tfidf.round(4))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.     0.0085 0.0065 0.0082]\n",
            " [0.     0.     0.0158 0.0198]\n",
            " [0.     0.0408 0.     0.    ]\n",
            " [0.     0.     0.0315 0.    ]\n",
            " [0.     0.0408 0.     0.    ]\n",
            " [0.0169 0.0085 0.     0.0082]\n",
            " [0.     0.0085 0.0065 0.0082]\n",
            " [0.     0.0085 0.0065 0.0082]\n",
            " [0.     0.0204 0.     0.0198]\n",
            " [0.0408 0.     0.0315 0.    ]\n",
            " [0.0169 0.     0.0065 0.0082]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}