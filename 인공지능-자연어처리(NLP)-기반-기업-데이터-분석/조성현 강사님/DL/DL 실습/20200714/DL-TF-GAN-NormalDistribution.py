# -*- coding: utf-8 -*-
"""DL-TF-GAN_NormalDistribution-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ocogdqdAvAs5lJ1d0UpKNqv61UUv7Jl
"""

# module import
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# 실제 데이터(가우지안 분포) 생성
real_data = np.random.normal(size=1000).astype(np.float32)
real_data = real_data.reshape(real_data.shape[0], 1) # (1000, 1)
real_data.shape

# 미니배치용 데이터 셋
real_data_batch = tf.data.Dataset.from_tensor_slices(real_data)\
                    .shuffle(buffer_size=real_data.shape[0])\
                    .batch(batch_size=300)
real_data_batch

# KL Divergence 계산 함수 정의
def calc_KL(P, Q):
    hist_P, bins_P = np.histogram(P)
    hist_Q, bins_Q = np.histogram(Q)

    pdf_P = hist_P / (np.sum(hist_P) + 1e-8)
    pdf_Q = hist_Q / (np.sum(hist_Q) + 1e-8)

    kld_PQ = np.sum(pdf_P * (np.log(pdf_P + 1e-8) - np.log(pdf_Q + 1e-8)))

    return pdf_P, pdf_Q, kld_PQ

# log 클리핑
def clip_log(x):
    return tf.math.log(x + 1e-8)

# Discriminator 네트워크
d_input = real_data.shape[1]
d_hidden = 8
d_output = 1 # 주의

d_Wh = tf.Variable(tf.random.normal(shape=[d_input, d_hidden]), name='D_hidden_weight')
d_Bh = tf.Variable(tf.random.normal(shape=[d_hidden]), name='D_hidden_bias')
d_Wo = tf.Variable(tf.random.normal(shape=[d_hidden, d_output]), name='D_output_weight')
d_Bo = tf.Variable(tf.random.normal(shape=[d_output]), name='D_output_bias')

theta_D = [d_Wh, d_Bh, d_Wo, d_Bo] # 파라미터 한 번에 묶기

# Generator 네트워크
g_input = 8
g_hidden = 4
g_output = d_input # 주의

g_Wh = tf.Variable(tf.random.normal(shape=[g_input, g_hidden]), name='G_hidden_weight')
g_Bh = tf.Variable(tf.random.normal(shape=[g_hidden]), name='G_hidden_bias')
g_Wo = tf.Variable(tf.random.normal(shape=[g_hidden, g_output]), name='G_output_weight')
g_Bo = tf.Variable(tf.random.normal(shape=[g_output]), name='G_output_bias')

theta_G = [g_Wh, g_Bh, g_Wo, g_Bo] # 파라미터 한 번에 묶기

# Discriminator: 들어온 데이터가 맞는지 아닌지 출력
def Discriminator(data):
    d_Hidden = tf.matmul(data, d_Wh) + d_Bh
    d_Hidden = tf.nn.relu(d_Hidden) # activation
    d_Out = tf.matmul(d_Hidden, d_Wo) + d_Bo 
    d_Out = tf.nn.sigmoid(d_Out)
    return d_Out

# Generator: 들어온 데이터 넣어 줌.
def Generator(data):
    g_Hidden = tf.matmul(data, g_Wh) + g_Bh
    g_Hidden = tf.nn.relu(g_Hidden)
    g_Out = tf.matmul(g_Hidden, g_Wo) + g_Bo
    return g_Out # activation 없음.

# 가짜 데이터 생성: (m, n) 크기의 랜덤한 정규분포 = 원래 real과 같다는 보장 없음.
def makeZ(m, n=g_input):
    z = np.random.normal(-1.0, 1.0, size=[m, n]).astype(np.float32)
    return z

# Discriminator Loss
def loss_Discriminator(x, z):
    Dx = Discriminator(x) # discriminator가 판별한 진짜 x
    Gz = Generator(z) # generator가 생성한 가짜 데이터
    DGz = Discriminator(Gz) # discriminator가 판별한 가짜 데이터 z

    loss = tf.reduce_mean(clip_log(Dx) + clip_log(1-DGz))
    
    return -loss # minimize로 바꿔 줌.

# Generator Loss
def loss_Generator(z):
    Gz = Generator(z)
    DGz = Discriminator(Gz)

    loss = tf.reduce_mean(clip_log(1-DGz))
    return loss

# 옵티마이저: 0.005로 했을 때 난리났다.
opt = Adam(learning_rate=0.0005)

# 학습 시 loss, KLD 기록
loss_D_history = []
loss_G_history = []
KLdivergence_history=[]

# 학습 파라미터
EPOCHS = int(input('학습 횟수 설정: '))

for epoch in range(EPOCHS): # 1번 학습할 때마다

    for X_batch in real_data_batch: # 미니배치 단위로 밀어 넣음.
        Z_batch = makeZ(m=X_batch.shape[0], n=g_input) # 가짜 데이터

        opt.minimize(lambda: loss_Discriminator(X_batch, Z_batch), var_list=theta_D) # discriminator먼저 학습한 뒤,
        opt.minimize(lambda: loss_Generator(Z_batch), var_list=theta_G) # generator 학습.

    if epoch % 10 == 0: # 10회 학습 주기별로 기록
        loss_D_history.append(loss_Discriminator(X_batch, Z_batch))
        loss_G_history.append(loss_Generator(Z_batch))
        P, Q, kld = calc_KL(X_batch, Generator(Z_batch)) # 원래 X와 generator가 만들어 낸 Z의 분포 차이 계산
        KLdivergence_history.append(kld)

        print("Epoch %d : loss-D %.4f, loss-G %.4f, KL-Divergence %.4f" %(epoch, loss_D_history[-1], loss_G_history[-1], KLdivergence_history[-1]))

# plot loss
plt.figure(figsize=(6, 4))
plt.plot(loss_D_history, label='Discriminator Loss', color='red')
plt.plot(loss_G_history, label='Generator Loss', color='blue')
plt.title('Loss History', size=18)
plt.legend()
plt.grid()
plt.show()

# plot KL divergence
plt.figure(figsize=(6, 4))
plt.plot(KLdivergence_history, label='KL Divergence', color='green')
plt.title('KL Divergence', size=18)
plt.legend()
plt.grid()
plt.show()

# 학습 완료 후 가짜 데이터 생성하고 그림 그려 보기
z = makeZ(m=real_data.shape[0], n=g_input)
fake_data = Generator(z).numpy() # 학습 완료 후 generator가 그려 낸 분포

plt.figure(figsize=(8, 5))
sns.set_style('whitegrid')
sns.kdeplot(real_data[:, 0], color='blue', bw=0.5, label='REAL data')
sns.kdeplot(fake_data[:, 0], color='red', bw=0.3, label='FAKE data')
plt.title('REAL vs. FAKE distribution')
plt.legend()
plt.show()

# 학습 완료 후 real data와 fake data 넣어 봤을 때 어떻게 달라지는가?
discriminator_real_values = Discriminator(real_data) # 0.5 근처 값이 많다.
discriminator_fake_values = Discriminator(fake_data) # 0.5 근처 값이 많다.
print(discriminator_real_values[:10])
print(discriminator_fake_values[:10])

plt.figure(figsize=(8, 5))
plt.plot(discriminator_real_values, label='Discriminated Real Data')
plt.plot(discriminator_fake_values, label='Discriminated Fake Data', color='red')
plt.title("Discriminator vs. Generator")
plt.legend()
plt.show()

