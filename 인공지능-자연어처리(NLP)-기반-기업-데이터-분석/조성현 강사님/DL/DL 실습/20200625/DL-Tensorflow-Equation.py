# -*- coding: utf-8 -*-
"""[20200625] DL-equation_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kx64PwIiS6HPc1ZavpvE46MQjQZYcqhr
"""

# module import
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# y = 2*x^2 + 3*x + 5, 데이터 생성
x = np.array(np.arange(-5, 5, 0.1))
y = 2*x*x + 3*x +5

# w1, w2, b를 찾기 위해 학습을 진행한다.
lr = 0.01 # learning_rate

# 계수 설정 : 초기값
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)

# 학습
histLoss = []
for epoch in range(1000): # 학습횟수 1000
    with tf.GradientTape() as tape:
        loss = tf.sqrt(
            tf.reduce_mean(
                tf.square(w1*x*x + w2*x + b - y)  # root mean squared error
            )
        )

        # loss에 대한 각 variable의 미분값
        dw1, dw2, db = tape.gradient(loss, [w1, w2, b])
    
    # variable update
    w1.assign_sub(lr*dw1) # a = a - lr*da
    w2.assign_sub(lr*dw2) # b = b - lr*b.numpy()
    b.assign_sub(lr*db)

    
    histLoss.append(loss) # loss 기록
    if epoch % 10 == 0:
        print("epoch = %d, loss = %.4f" %(epoch, histLoss[-1]))

# 추정 결과
print(' ===== 추정 결과 ======')
print('       w1 = %.2f' % w1.numpy()) # numpy 형식으로 확인
print('       w2 = %.2f' % w2.numpy())
print('       b = %.2f' % b.numpy())
print(' final loss = %.4f' % loss.numpy())

# plot
plt.plot(histLoss, color='red', linewidth=1)
plt.title('Loss Function')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

##################### custom early stopping 1 ####################

# module import
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# y = 2*x^2 + 3*x + 5, 데이터 생성
x = np.array(np.arange(-5, 5, 0.1))
y = 2*x*x + 3*x +5

# w1, w2, b를 찾기 위해 학습을 진행한다.
lr = 0.01 # learning_rate

# 계수 설정 : 초기값
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)

# 학습
histLoss = []
for epoch in range(10000):
    with tf.GradientTape() as tape:
        loss = tf.sqrt(
            tf.reduce_mean(
                tf.square(w1*x*x + w2*x + b - y)  # root mean squared error
            )
        )

        # loss에 대한 각 variable의 미분값
        dw1, dw2, db = tape.gradient(loss, [w1, w2, b])
    
    # Early Stopping
    if epoch > 10:
        if round(loss.numpy(), 5) in [round(histLoss[-1].numpy(), 5), round(histLoss[-2].numpy(), 5), round(histLoss[-3].numpy(), 5)]:
            print(f"{epoch+1}번째 학습. gradient : [{dw1}, {dw2}, {dw3}]")
            print("Loss값이 4번째 감소하지 않았습니다. 학습을 종료합니다.")
            break

    # variable update
    w1.assign_sub(lr*dw1) # a = a - lr*da
    w2.assign_sub(lr*dw2) # b = b - lr*b.numpy()
    b.assign_sub(lr*db)

    histLoss.append(loss) # loss 기록

    if epoch % 10 == 0:
        print("epoch = %d, loss = %.4f" %(epoch+1, histLoss[-1]))

# 추정 결과
print(' ===== 추정 결과 ======')
print('       w1 = %.2f' % w1.numpy()) # numpy 형식으로 확인
print('       w2 = %.2f' % w2.numpy())
print('       b = %.2f' % b.numpy())
print(' final loss = %.4f' % loss.numpy())

# plot
plt.plot(histLoss, color='red', linewidth=1)
plt.title('Loss Function')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()


##################### custom early stopping 2 ####################

# module import
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# y = 2*x^2 + 3*x + 5, 데이터 생성
x = np.array(np.arange(-5, 5, 0.1))
y = 2*x*x + 3*x +5

# w1, w2, b를 찾기 위해 학습을 진행한다.
lr = 0.01 # learning_rate

# 계수 설정 : 초기값
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)

# 학습
histLoss = []
for epoch in range(10000):
    with tf.GradientTape() as tape:
        loss = tf.sqrt(
            tf.reduce_mean(
                tf.square(w1*x*x + w2*x + b - y)  # root mean squared error
            )
        )

        # loss에 대한 각 variable의 미분값
        dw1, dw2, db = tape.gradient(loss, [w1, w2, b])
    
    # Early Stopping 2
    if epoch > 10:
        if abs(dw1.numpy()) < 0.001 or abs(dw2.numpy()) < 0.001 or abs(db.numpy()) < 0.001:
            print(f"{epoch+1}번째 학습. gradient : [{dw1}, {dw2}, {dw3}]")
            print("학습을 종료합니다.")
            break

    # variable update
    w1.assign_sub(lr*dw1) # a = a - lr*da
    w2.assign_sub(lr*dw2) # b = b - lr*b.numpy()
    b.assign_sub(lr*db)

    histLoss.append(loss) # loss 기록

    if epoch % 10 == 0:
        print("epoch = %d, loss = %.4f" %(epoch+1, histLoss[-1]))

# 추정 결과
print(' ===== 추정 결과 ======')
print('       w1 = %.2f' % w1.numpy()) # numpy 형식으로 확인
print('       w2 = %.2f' % w2.numpy())
print('       b = %.2f' % b.numpy())
print(' final loss = %.4f' % loss.numpy())

# plot
plt.plot(histLoss, color='red', linewidth=1)
plt.title('Loss Function')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()