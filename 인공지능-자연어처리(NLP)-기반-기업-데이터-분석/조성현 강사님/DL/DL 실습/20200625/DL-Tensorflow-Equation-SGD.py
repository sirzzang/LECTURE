# -*- coding: utf-8 -*-
"""[20200625] DL-equation-SGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AJZI5knxv0r5O5yBHgOsWIrVYLJ-wlDt
"""

# module import
import tensorflow as tf
from tensorflow.keras import optimizers
import numpy as np
import matplotlib.pyplot as plt

# x, y 데이터 생성
x = np.array(np.arange(-5, 5, 0.1))
y = 2*x*x + 3*x + 5

# 그래프 생성
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)
var_list = [w1, w2, b]

# SGD optimizer, momentum 방법
opt = optimizers.SGD(learning_rate=0.01, momentum=0.1, nesterov=False) 

histLoss = []
for epoch in range(300):
    with tf.GradientTape() as tape:
        # root mean squared error
        loss = tf.sqrt(tf.reduce_mean(tf.square(w1*x*x + w2*x + b -y)))
    
    # loss에 대한 각 variable 미분값
    grads = tape.gradient(loss, var_list)

    # update variable, gradient descent
    opt.apply_gradients(zip(grads, var_list))
    
    if epoch % 10 == 0:
        histLoss.append(loss.numpy())
        print("epoch = %d, loss = %.4f" %(epoch, histLoss[-1]))

print('======= 추정 결과 ========')
print('        W1 : %.2f' % w1.numpy())
print('        W2 : %.2f' % w2.numpy())
print('        b : %.2f' % b.numpy())
print(' 최종 loss : %.4f' % loss.numpy())

plt.plot(histLoss, color='blue', linewidth=1)
plt.title("Loss function")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()