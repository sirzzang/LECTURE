# -*- coding: utf-8 -*-
"""[20200625] DL-equation-Adam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y7l5v-T1-YKFq7ym6jeulZrNVu-aQYrn
"""

# module import
import tensorflow as tf
from tensorflow.keras import optimizers
import numpy as np
import matplotlib.pyplot as plt

# x, y 데이터 집합 생성
x = np.array(np.arange(-5, 5, 0.1))
y = 2*x*x + 3*x + 5

# 그래프 생성
w1 = tf.Variable(1.0)
w2 = tf.Variable(1.0)
b = tf.Variable(1.0)

# loss : rmse
def loss():
    return tf.sqrt(tf.reduce_mean(tf.square(w1*x*x + w2*x + b - y)))

# AdamOptimizer
opt = optimizers.Adam(learning_rate = 0.05)

# train
histLoss = []
for epoch in range(300):
    opt.minimize(loss, var_list=[w1, w2, b])

    histLoss.append(loss()) # loss 함수
    if epoch % 10 == 0:
        print("epoch = %d, loss = %.4f" %(epoch, histLoss[-1]))

# result
print('====== 추정 결과 ======')
print('       W1 : %.2f' % w1.numpy())
print('       W2 : %.2f' % w2.numpy())
print('       b : %.2f' % b.numpy())
print(' 최종 loss : %.4f' % histLoss[-1].numpy())

# plot
plt.plot(histLoss, color='orange', linewidth=1)
plt.title('Loss Function')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()