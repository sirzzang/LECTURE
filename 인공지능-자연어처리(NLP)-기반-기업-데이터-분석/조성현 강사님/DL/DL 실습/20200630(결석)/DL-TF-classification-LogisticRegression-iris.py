# -*- coding: utf-8 -*-
"""DL-TF-classification-LogisticRegression-iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V-imq-xQi7EnnqgeA76sdDm_oXjWtnBL
"""

# module import
from sklearn.datasets import load_iris
import numpy as np
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# load data
iris = load_iris()

# preprocessing
X_data = iris.data.astype(np.float32)
y_data = to_categorical(iris.target, num_classes=len(iris.target_names)).astype(np.float32)

# standardize
scaler = StandardScaler()
X_data = scaler.fit_transform(X_data)

# split
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)
print("Train: {0}, {1} / Test: {2}, {3}".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))

# layer
n_input = X_train.shape[1]
n_hidden = int(input('은닉 노드의 수를 설정하세요: '))
n_output = int(input('출력 노드의 수를 설정하세요: '))

# data for mini-batch
BATCH = int(input('배치 사이즈를 설정하세요: '))
train_batch = tf.data.Dataset.from_tensor_slices((X_train, y_train))\
                .shuffle(buffer_size=X_train.shape[0])\
                .batch(BATCH)
print(train_batch)

# graph: weight, bias
Wh = tf.Variable(tf.random.normal([n_input, n_hidden]), dtype=tf.float32, name='weight1')
Bh = tf.Variable(tf.zeros([n_hidden]), dtype=tf.float32, name='bias1')
Wo = tf.Variable(tf.random.normal([n_hidden, n_output]), dtype=tf.float32, name='weight2')
Bo = tf.Variable(tf.zeros([n_output]), dtype=tf.float32, name='bias2')
print(Wh.shape, Bh.shape, Wo.shape, Bo.shape)

# define functions
def predict(X):
    Hh = tf.nn.relu(tf.add(tf.matmul(X, Wh), Bh)) # 은닉층 활성화 함수: relu
    Ho = tf.nn.softmax(tf.add(tf.matmul(Hh, Wo), Bo)) # 출력층 활성화 함수: softmax
    return Ho

def loss_CCE(X, y, C):
    y_pred = predict(X)
    y_clip = tf.clip_by_value(y_pred, 0.000001, 0.999999)

    # categorical crossentropy
    cce = -tf.reduce_mean(tf.reduce_sum(y * tf.math.log(y_clip), axis=1))

    # regularization term
    cce += C * tf.reduce_mean(tf.square(Wh)) +\
           C * tf.reduce_mean(tf.square(Bh)) +\
           C * tf.reduce_mean(tf.square(Wo)) +\
           C * tf.reduce_mean(tf.square(Bo))
    
    return cce

# optimizer
adam = Adam(lr=0.01)

# train
epochs = int(input('학습 epoch 수를 설정하세요: '))
C = float(input('규제항 크기를 설정하세요: '))

train_loss, test_loss = [], []
for epoch in range(epochs):
    # batch update
    for batch_X, batch_y in train_batch:
        adam.minimize(lambda: loss_CCE(batch_X, batch_y, C), var_list=[Wh, Bh, Wo, Bo])
    
    # loss 기록
    train_loss.append(loss_CCE(X_train, y_train, C))
    test_loss.append(loss_CCE(X_test, y_test, C))

    # check
    print("Epoch : %d, Train Loss : %.4f, Test Loss : %.4f" %(epoch, train_loss[-1], test_loss[-1]))

# 테스트셋 정확도 확인
y_hat = predict(X_test)
y_hat_pred = np.argmax(y_hat, axis=1)
y_test_label = np.argmax(y_test, axis=1)
accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat_pred, y_test_label), dtype=tf.float32))
print("Test Set Accuracy: %.4f" % accuracy)

# plot
plt.figure(figsize=(10, 4))
plt.plot(train_loss, label='Train Loss')
plt.plot(test_loss, label='Test Loss')
plt.title('Loss Function_Categorical CrossEntropy')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()