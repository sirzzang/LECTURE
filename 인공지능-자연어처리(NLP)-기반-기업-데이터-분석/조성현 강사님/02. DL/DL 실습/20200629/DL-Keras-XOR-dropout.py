# -*- coding: utf-8 -*-
"""20200629-DL-Keras-XOR_dropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1kcmUwcqSBJakILyAx7KIMwyjS5GZVB
"""

# module import
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

# set data : XOR
X_data = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]], dtype=np.float32)
y_data = np.array([[1],
                   [0],
                   [0],
                   [1]], dtype=np.float32)

# layer 설정
n_input = 2 # input layer neuron 개수: X_data.shape[1]
n_hidden = 4 # hidden layer neuron 개수
n_output = 1 # output layer neuron 개수

# 그래프 생성
X_Input = Input(batch_shape=(None, 2))
X_Hidden = Dense(n_hidden, activation='sigmoid')(X_Input)
y_Output = Dense(1, activation='sigmoid')(X_Hidden)

# 모델 생성
model = Model(X_Input, y_Output)
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01))
print(model.summary())

# 학습
hist = model.fit(X_data, y_data,
                 epochs=100)

# 예측
y_hat = model.predict(X_data)
y_hat_pred = np.where(y_hat > 0.5, 1, 0)
print(y_hat_pred)

# 결과 확인
parameters = model.layers[1].get_weights()
print("===== 추정 결과 =====")
Wh = parameters[0]
Bh = parameters[1]
print('     Hidden Layer Weights = ', Wh)
print('     Hidden Layer Bias = ', Bh)

# plot
plt.plot(hist.history['loss'], label='Train Loss')
plt.title('Loss Function: Binary Crossentropy')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

"""## 강사님 코드"""

# module import
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers
from tensorflow.keras import regularizers
import numpy as np

# data
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y = np.array([[0], [1], [1], [0]], dtype=np.float32)

# model
X_input = Input(batch_shape=(None, 2))
X_hidden = Dense(4, activation='sigmoid',
                 kernel_regularizer=regularizers.l2(0.0001))(X_input) # 데이터가 적어서 오히려 regularize하면 안 좋다. 그래서 매우 작은 수를 넣는다.
X_hidden = Dropout(rate=0.1)(X_hidden) # fit에만 적용, predict에서는 알아서 적용 안 됨.
y_output = Dense(1, activation='sigmoid')(X_hidden)

model = Model(X_input, y_output)
model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.05))
print(model.summary())

# 학습
hist = model.fit(X, y,
                 epochs=500,
                 batch_size=4)

# 예측
y_hat = model.predict(X)
y_hat_pred = np.where(y_hat > 0.5, 1, 0)
print(y_hat)
print(y_hat_pred)