# -*- coding: utf-8 -*-
"""20200629-DL-TF-classification-LogisticRegression-credit-practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mq8ykH_O3MvoQXBsB2vLx3uSG_K-nEW8
"""

# Commented out IPython magic to ensure Python compatibility.
# module import
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# dataset folder
data_path = '/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/dataset'

# load data
data = pd.read_csv(f'{data_path}/3-4.credit_data.csv')

# prepare data: ok!
data = np.array(data, dtype=np.float32)
X_data = data[:, :-1]
y_data = data[:, -1]
y_data = y_data.reshape(len(y_data), 1) # 2차원 배열

# split data
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)

# mini batch data
train_batch = tf.data.Dataset.from_tensor_slices((X_train, y_train))\
    .shuffle(buffer_size=X_train.shape[0])\
    .batch(batch_size=50)

# 그래프 생성
def ANN_model(n_input, n_hidden, n_output):
    '''
    :param n_input: input layer neuron 수
    :param n_hidden: hidden layer neuron 수
    :param n_output: output layer neuron 수
    :return: 각 layer의 weight, bias
    '''
    W_h = tf.Variable(tf.random.normal([n_input, n_hidden]), dtype=tf.float32)
    B_h = tf.Variable(tf.zeros([n_hidden]), dtype=tf.float32)
    W_o = tf.Variable(tf.random.normal([n_hidden, n_output]), dtype=tf.float32)
    B_o = tf.Variable(tf.zeros([n_output]), dtype=tf.float32)
    return W_h, B_h, W_o, B_o

Wh, Bh, Wo, Bo = ANN_model(X_train.shape[1], 8, 1) # hidden layer 8, output 1.

# 예측
def predict(X):
    Hh = tf.nn.relu(tf.matmul(X, Wh) + Bh)
    Ho = tf.sigmoid(tf.matmul(Hh, Wo) + Bo)
    return Ho

# cost
def loss_CE(X, y, c):
    y_pred = predict(X)
    y_clip = tf.clip_by_value(y_pred, 0.000001, 0.999999)
    cost = -tf.reduce_mean(y * tf.math.log(y_clip) + (1-y) * tf.math.log(1-y_clip)) +\
                           c * tf.reduce_mean(tf.square(Wh)) +\
                           c * tf.reduce_mean(tf.square(Bh)) +\
                           c * tf.reduce_mean(tf.square(Wo)) +\
                           c * tf.reduce_mean(tf.square(Bo))
    return cost

# 옵티마이저
adam = Adam(learning_rate=0.01)

# 학습: mini-batch 방식
train_loss, test_loss = [], []
c = 0.05 # regularization constant
epochs = 50

for epoch in range(epochs):

    # mini-batch
    for batch_X, batch_y in train_batch:
        adam.minimize(lambda: loss_CE(batch_X, batch_y, c), var_list=[Wh, Bh, Wo, Bo])

    # 학습 세트 손실
    train_loss.append(loss_CE(X_train, y_train, c))

    # 테스트 세트 손실
    test_loss.append(loss_CE(X_test, y_test, c))

    print('Epoch: %d, train_loss: %.4f, test_loss: %.4f'
#           %(epoch, train_loss[-1], test_loss[-1]))

# 정확도 측정
y_hat = predict(X_test).numpy()
y_hat_pred = np.where(y_hat > 0.5, 1 ,0)
test_accuracy = tf.reduce_mean(tf.cast(tf.equal(y_test, y_hat_pred), dtype=tf.float32))
print(f'Final Test Accuracy: {test_accuracy}')

# 결과 확인
fig = plt.figure(figsize=(10, 4))
p1 = fig.add_subplot(1, 2, 1)
p2 = fig.add_subplot(1, 2, 2)

p1.plot(train_loss, label='Train Loss')
p1.plot(test_loss, label='Test Loss')
p1.legend()
p1.set_title('Loss Function')
p1.set_xlabel('Epoch')
p1.set_ylabel('Loss')

n, bins, patches = p2.hist(y_hat, 50, facecolor='blue', alpha=0.5)
p2.set_title('y_hat Distribution')
plt.show()