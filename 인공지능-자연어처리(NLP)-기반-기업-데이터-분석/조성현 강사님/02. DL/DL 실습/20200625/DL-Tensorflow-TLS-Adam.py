# -*- coding: utf-8 -*-
"""[20200625] DL-Tensorflow-TLS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LdHDhfvFCxD48xqE-bhyXk0u5bY_DlzK
"""

# module import
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np


# create samples : y = ax + b + e
def createData(a, b, n):
    result_X, result_y = [], []
    for i in range(n):
        x = np.random.normal(0.0, 0.5)
        y = a*x + b + np.random.normal(0.0, 0.05)
        result_X.append(x)
        result_y.append(y)
    return result_X, result_y

# 입력 데이터 관계식: y = 0.1*x + 0.3 + e
x, y = createData(0.1, 0.3, 1000)

# 선형 회귀식 weight, bias
W = tf.Variable(tf.random.normal([1], -1, 1), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')

# optimizer
adam = Adam(learning_rate = 0.05)

# loss 수직 거리 정의
def loss(x):
    bunja = tf.abs(tf.subtract(tf.add(tf.multiply(W, x), b), y)) # 분자
    bunmo = tf.sqrt(tf.add(tf.square(W), 1)) # 분모
    return tf.reduce_mean(tf.sqrt(tf.divide(bunja, bunmo)))

# 학습
train_loss = []
for i in range(int(input('학습 횟수를 입력하세요: '))):
    adam.minimize(lambda: loss(x), var_list=[W, b])
    train_loss.append(loss(x))

    if i % 10 == 0:
        print("%d번째 epoch: loss %.4f" % (i, train_loss[-1]))

# 결과 확인
print("")
y_hat = W.numpy() * x + b.numpy()
for estimate, real in zip(y_hat[:10], y[:10]):
    print(f"추정치: {estimate}, 실제: {real}")
print("")
print("========= 회귀 직선의 방정식(TLS) =========")
print("y = %.4f*x + %.4f" % (W.numpy(), b.numpy()))

# plot
fig = plt.figure(figsize=(10, 4))
p1 = fig.add_subplot(1, 2, 1)
p2 = fig.add_subplot(1, 2, 2)

p1.plot(x, y, 'ro', markersize=1.5)
p1.plot(x, y_hat)

p2.plot(train_loss)
p2.set_title('Loss Function')
p2.set_xlabel('epoch')
p2.set_ylabel('loss')
plt.show()