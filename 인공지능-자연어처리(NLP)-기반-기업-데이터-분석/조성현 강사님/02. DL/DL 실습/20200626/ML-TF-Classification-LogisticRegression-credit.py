# -*- coding: utf-8 -*-
"""20200626-ML-Tensorflow-classification-credit-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTxM9fmnrvYed5otYoPOnrZhLDqR2ikm
"""

# module import
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# dataset folder
data_path = '/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님/dataset'

# load data
data = pd.read_csv(f'{data_path}/3-4.credit_data.csv')

# shuffle rows: 필요 없다!
# data = data.sample(frac=1).reset_index(drop=True)

# to numpy array
data = np.array(data, dtype=np.float32)

# split data
train_num = int(len(data) * 0.8)
X_train, X_test = data[:train_num, :-1], data[train_num:, :-1]
y_train, y_test = data[:train_num, -1], data[train_num:, -1]

# reshape labels: 원래 label 1차원이었음.
y_train = y_train.reshape(len(y_train), 1)
y_test = y_test.reshape(len(y_test), 1)

# mini_batch_dataset
BATCH = 30
X_train_batch = tf.data.Dataset.from_tensor_slices((X_train, y_train))\
                               .shuffle(buffer_size=X_train.shape[0])\
                               .batch(batch_size=BATCH)

# 모델 그래프 생성: W, b
W = tf.Variable(tf.random.normal([6, 1]), name='weight')
# W = tf.Variable(tf.random.uniform([6, 1], -1.0, 1.0), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')

# predict 함수: sigmoid 적용한 logit값 반환
def predict(X):
    y_tmp = tf.add(tf.matmul(X, W), b) # sigmoid 적용 전
    y_pred = tf.math.divide(1.0, 1.0 + tf.math.exp(-y_tmp))
    return y_pred

# loss 함수: binary Cross Entropy
def loss_BCE(X, y):
    y_pred = predict(X)
    # y 예측값의 범위 설정: 이건 log 안에만 들어가야 한다.
    y_clip = tf.clip_by_value(y_pred, 0.000001, 0.999999)
    # cross entropy 함수: y 자체는 그대로. 들어오는 y는 batch_y 값이 그대로 들어가야.
    BCE = -tf.reduce_mean(y*tf.math.log(y_clip) + (1-y)*tf.math.log(1-y_clip))
    return BCE

# optimizer
adam = Adam(learning_rate=0.05)

# 학습
train_epoch = int(input('학습 횟수를 설정하세요: '))
train_loss = []
for epoch in range(train_epoch):
    # mini batch
    for batch_X, batch_y in X_train_batch:
        adam.minimize(lambda: loss_BCE(batch_X, batch_y), var_list = [W, b])    
    # train loss
    train_loss.append(loss_BCE(X_train, y_train))
    
    if epoch % 10 == 0:
        print("epoch : %d, loss: %.4f" % (epoch, train_loss[-1]))

# 예측
y_hat = predict(X_test).numpy() # y 추정치: 0~1 사이의 값으로 나와야 한다. # 계속 예측이 이상하게 나와서 보니까, y_clip값을...
y_hat_pred = np.where(y_hat > 0.5, 1.0, 0.0)

# 정확도 측정
accuracy_1 = (y_hat_pred == y_test).sum() / len(y_hat_pred)
accuracy_2 = tf.reduce_mean(tf.cast(tf.equal(y_hat_pred, y_test), dtype=tf.float32)) # 문쌤 방식 : 아니 왜 텐서로 나온담?
print(f'accuracy: {np.round(accuracy_1, 3)}')

# 결과 확인
W = W.numpy().T[0]
b = b.numpy()
print(f'weight: {np.round(W, 3)}, bias: {np.round(b, 3)}')
print('')
print('class별 결과 확인')
print(f'   [ TEST DATA ] actual 0 : {len(np.where(y_test[:, -1] == 0.0)[0])} vs pred 0 : {len(np.where(y_hat_pred[:, -1] == 0.0)[0])}')
print(f'   [ TEST DATA ] actual 1 : {len(np.where(y_test[:, -1] == 1.0)[0])} vs pred 1 : {len(np.where(y_hat_pred[:, -1] == 1.0)[0])}')

# plot
fig = plt.figure(figsize=(10, 4))
p1 = fig.add_subplot(1, 2, 1)
p2 = fig.add_subplot(1, 2, 2)

p1.plot(train_loss)
p1.set_title('Loss Function')
p1.set_xlabel('Epoch')
p1.set_ylabel('Train Loss')

n, bins, patches = p2.hist(y_hat, 50, facecolor='blue', alpha=0.5)
p2.set_title('y_hat Distribution')
plt.show()