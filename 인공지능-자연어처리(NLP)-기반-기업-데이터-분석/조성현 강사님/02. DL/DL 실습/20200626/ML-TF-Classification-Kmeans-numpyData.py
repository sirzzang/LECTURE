# -*- coding: utf-8 -*-
"""20200626-ML-Tensorflow-Kmeans-NumpyData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fhIx72h8IX1WUDkIr3zeZOPcTOX2jhKL

* 강사님 참고 자료 출처 : Jordi Torres, First contact with tensorflow (page 27)
"""

# module import
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib.pyplot import cm

# create data
def createData(n):
    xy = []
    for i in range(n):
        if np.random.random() > 0.5:
            x = np.random.normal(0.0, 0.9)
            y = np.random.normal(0.0, 0.9)
        else:
            x = np.random.normal(3.0, 0.6)
            y = np.random.normal(1.0, 0.6)
        xy.append([x, y])
    
    coordinates = np.array(xy, dtype=np.float32)
    return coordinates

n = int(input('생성할 좌표의 개수를 설정하세요.: '))
input_data = tf.constant(createData(n))
print(f"입력 데이터(좌표) 형태: {input_data.shape}")

final_error = np.inf # 최종 에러
trc_error = []
k = int(input('클러스터의 개수를 설정하세요.: ')) # 초기 중점의 개수
train_epoch = int(input('초기 중점 설정 횟수를 설정하세요.: '))

for epoch in range(train_epoch):
    prev_error = 0 # 첫 에러

    # 초기 중심좌표 설정
    tmp_data = tf.random.shuffle(input_data) # input_data 셔플
    tmp_cent = tf.slice(tmp_data, [0, 0], [k, -1]) # 셔플 후 앞의 k개를 초기 중점으로 설정
    centroid = tf.Variable(tmp_cent) # 중점 좌표 변할 것이므로 Variable 선언

    for j in range(200): 
        # 데이터 중심좌표 거리 계산을 위해 dimension 확장: 교재 참고.
        exp_data = tf.expand_dims(input_data, axis=0) # D0축 확장.
        exp_cent = tf.expand_dims(centroid, axis=1) # D1축 확장.

        # 데이터와 중심좌표 사이의 거리 계산.
        tmp_dist = tf.square(tf.subtract(exp_data, exp_cent))
        dist = tf.sqrt(tf.reduce_sum(tmp_dist, axis=2)) # D2 축을 합쳐야 함.
        error = tf.reduce_sum(dist) # 거리의 총합 error

        # 각 데이터를 거리가 작은 중점으로 assign
        assignment = tf.argmin(dist)

        # 중점 좌표 업데이트
        for c in range(k):
            correct = tf.equal(assignment, c) # 올바르게 할당되었는지 체크
            update = tf.where(correct) # True가 있는 곳으로
            flat = tf.reshape(update, [-1])
            gather = tf.gather(input_data, flat) # True 있는 좌표들 모음

            new_mean = tf.reduce_mean(gather, axis=0) # row 방향으로 모음

            # k개의 평균 좌표를 모은다
            if c == 0:
                concat_mean = tf.concat(new_mean, axis=0)
            else:
                concat_mean = tf.concat([concat_mean, new_mean], axis=0)

        mean_data = tf.reshape(concat_mean, [k, 2]) # 새로운 좌표로

        # 새로운 중점 assign
        centroid.assign(mean_data)

        # 그만할 조건 설정
        if np.abs(error-prev_error) < 0.0001:
            break
        prev_error = error # 에러 업데이트

    # 최소 에러 저장
    if error < final_error:
        final_cent = centroid.numpy().copy()
        final_cluster = assignment.numpy().copy()
        final_error = error
    
    trc_error.append(error)
    print("%d Done" % epoch)

# 분류 결과 표시
data = input_data.numpy()
color = cm.rainbow(np.linspace(0, 1, k))

# plot
plt.figure(figsize=(8, 6))

# 전체 클러스터 별로 분류
for i, c in zip(range(k), color):
    plt.scatter(data[final_cluster == i, 0], data[final_cluster == i, 1], \
                s=20, color=c, marker='o', alpha=0.5, \
                label=f'cluster-{i}')

# 중점 좌표
plt.scatter(final_cent[:, 0], final_cent[:, 1], \
            s=250, marker='^', color='black', \
            label='centroids')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

plt.plot(trc_error)
plt.title("ERROR_total distance")
plt.show()

# 결과 표시
print("Minimum Error = %.2f" % final_error)

