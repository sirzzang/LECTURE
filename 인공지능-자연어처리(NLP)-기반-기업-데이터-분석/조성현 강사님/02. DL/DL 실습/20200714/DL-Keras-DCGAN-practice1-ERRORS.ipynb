{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Keras-DCGAN-practice1-ERRORS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txq6ZJfdgmUa",
        "colab_type": "text"
      },
      "source": [
        "# 일단 돌기는 돈다.\n",
        "- accuracy 왜 계속 1.000이니? : trainable True로 바꿔 주고 다시 해 보기\n",
        "- 일단 모델 레이어 층만 보고."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcEfyuFzKUMq",
        "colab_type": "text"
      },
      "source": [
        "# 모르겠는 거?!\n",
        "- leakyrelu가 activation function이 아니라 층인가?\n",
        "- activation 층?\n",
        "- batchnormalization 정확히 모르겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K50RM3jJw-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# module import\n",
        "from tensorflow.keras.layers import Input, Dense, Activation\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape\n",
        "from tensorflow.keras.layers import LeakyReLU ## 이거가 층인가?\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_4NJj9bP7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 경로 설정\n",
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/dataset\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK5cXgzyKxmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 로드: X_train 데이터만 사용\n",
        "(X_train, _), (_, _) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEWDgM3ULI8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "694d8b8a-76f9-406e-a633-038812948940"
      },
      "source": [
        "# CNN 모델 입력 형태로 데이터 바꾸기\n",
        "print(f\"<before> Train Data: {X_train.shape}\")\n",
        "image_size = X_train.shape[1]\n",
        "n_channel = 1 # 흑백 이미지\n",
        "X_train = np.reshape(X_train, [-1, image_size, image_size, n_channel])\n",
        "print(f\"<after> Train Data: {X_train.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<before> Train Data: (60000, 28, 28)\n",
            "<after> Train Data: (60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAVJLAwtLI6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8444d9e-7661-4425-a724-dbcfaa008204"
      },
      "source": [
        "# 이미지 데이터 표준화: 출력 지움.\n",
        "print(\"<sample>\")\n",
        "print(X_train[0])\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "print(X_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sample>\n",
            "[[[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  3]\n",
            "  [ 18]\n",
            "  [ 18]\n",
            "  [ 18]\n",
            "  [126]\n",
            "  [136]\n",
            "  [175]\n",
            "  [ 26]\n",
            "  [166]\n",
            "  [255]\n",
            "  [247]\n",
            "  [127]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 30]\n",
            "  [ 36]\n",
            "  [ 94]\n",
            "  [154]\n",
            "  [170]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [225]\n",
            "  [172]\n",
            "  [253]\n",
            "  [242]\n",
            "  [195]\n",
            "  [ 64]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 49]\n",
            "  [238]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [251]\n",
            "  [ 93]\n",
            "  [ 82]\n",
            "  [ 82]\n",
            "  [ 56]\n",
            "  [ 39]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 18]\n",
            "  [219]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [198]\n",
            "  [182]\n",
            "  [247]\n",
            "  [241]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 80]\n",
            "  [156]\n",
            "  [107]\n",
            "  [253]\n",
            "  [253]\n",
            "  [205]\n",
            "  [ 11]\n",
            "  [  0]\n",
            "  [ 43]\n",
            "  [154]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 14]\n",
            "  [  1]\n",
            "  [154]\n",
            "  [253]\n",
            "  [ 90]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [139]\n",
            "  [253]\n",
            "  [190]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 11]\n",
            "  [190]\n",
            "  [253]\n",
            "  [ 70]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 35]\n",
            "  [241]\n",
            "  [225]\n",
            "  [160]\n",
            "  [108]\n",
            "  [  1]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 81]\n",
            "  [240]\n",
            "  [253]\n",
            "  [253]\n",
            "  [119]\n",
            "  [ 25]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 45]\n",
            "  [186]\n",
            "  [253]\n",
            "  [253]\n",
            "  [150]\n",
            "  [ 27]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 16]\n",
            "  [ 93]\n",
            "  [252]\n",
            "  [253]\n",
            "  [187]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [249]\n",
            "  [253]\n",
            "  [249]\n",
            "  [ 64]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 46]\n",
            "  [130]\n",
            "  [183]\n",
            "  [253]\n",
            "  [253]\n",
            "  [207]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 39]\n",
            "  [148]\n",
            "  [229]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [250]\n",
            "  [182]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 24]\n",
            "  [114]\n",
            "  [221]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [201]\n",
            "  [ 78]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 23]\n",
            "  [ 66]\n",
            "  [213]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [198]\n",
            "  [ 81]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 18]\n",
            "  [171]\n",
            "  [219]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [195]\n",
            "  [ 80]\n",
            "  [  9]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 55]\n",
            "  [172]\n",
            "  [226]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [244]\n",
            "  [133]\n",
            "  [ 11]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [136]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [212]\n",
            "  [135]\n",
            "  [132]\n",
            "  [ 16]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]]\n",
            "[[[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.01176471]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.49411765]\n",
            "  [0.53333336]\n",
            "  [0.6862745 ]\n",
            "  [0.10196079]\n",
            "  [0.6509804 ]\n",
            "  [1.        ]\n",
            "  [0.96862745]\n",
            "  [0.49803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.11764706]\n",
            "  [0.14117648]\n",
            "  [0.36862746]\n",
            "  [0.6039216 ]\n",
            "  [0.6666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.88235295]\n",
            "  [0.6745098 ]\n",
            "  [0.99215686]\n",
            "  [0.9490196 ]\n",
            "  [0.7647059 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.19215687]\n",
            "  [0.93333334]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.9843137 ]\n",
            "  [0.3647059 ]\n",
            "  [0.32156864]\n",
            "  [0.32156864]\n",
            "  [0.21960784]\n",
            "  [0.15294118]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.7137255 ]\n",
            "  [0.96862745]\n",
            "  [0.94509804]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.3137255 ]\n",
            "  [0.6117647 ]\n",
            "  [0.41960785]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8039216 ]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.16862746]\n",
            "  [0.6039216 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.05490196]\n",
            "  [0.00392157]\n",
            "  [0.6039216 ]\n",
            "  [0.99215686]\n",
            "  [0.3529412 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.54509807]\n",
            "  [0.99215686]\n",
            "  [0.74509805]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.04313726]\n",
            "  [0.74509805]\n",
            "  [0.99215686]\n",
            "  [0.27450982]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.13725491]\n",
            "  [0.94509804]\n",
            "  [0.88235295]\n",
            "  [0.627451  ]\n",
            "  [0.42352942]\n",
            "  [0.00392157]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.31764707]\n",
            "  [0.9411765 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.46666667]\n",
            "  [0.09803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.1764706 ]\n",
            "  [0.7294118 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.5882353 ]\n",
            "  [0.10588235]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.0627451 ]\n",
            "  [0.3647059 ]\n",
            "  [0.9882353 ]\n",
            "  [0.99215686]\n",
            "  [0.73333335]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.9764706 ]\n",
            "  [0.99215686]\n",
            "  [0.9764706 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.18039216]\n",
            "  [0.50980395]\n",
            "  [0.7176471 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8117647 ]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.15294118]\n",
            "  [0.5803922 ]\n",
            "  [0.8980392 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.98039216]\n",
            "  [0.7137255 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09411765]\n",
            "  [0.44705883]\n",
            "  [0.8666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7882353 ]\n",
            "  [0.30588236]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09019608]\n",
            "  [0.25882354]\n",
            "  [0.8352941 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.31764707]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.67058825]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7647059 ]\n",
            "  [0.3137255 ]\n",
            "  [0.03529412]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.21568628]\n",
            "  [0.6745098 ]\n",
            "  [0.8862745 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.95686275]\n",
            "  [0.52156866]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.53333336]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.83137256]\n",
            "  [0.5294118 ]\n",
            "  [0.5176471 ]\n",
            "  [0.0627451 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvP-5aUYLI4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc8719c3-9b58-4684-80bc-2c65b24d4958"
      },
      "source": [
        "# latent vector size 설정\n",
        "# latent_size = int(input('latent size 설정: '))\n",
        "latent_size = 100"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "latent size 설정: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60iX-AkMTlT",
        "colab_type": "text"
      },
      "source": [
        "# Generator\n",
        "\n",
        "- 초기 레이어 설정: 7 x 7 x 128\n",
        "    - 평활화한 뒤\n",
        "    - reshape을 통해 3차원 입력 형태로 바꿔 준다\n",
        "\n",
        "- 반복문을 통해 layer 쌓는다. strides 설정 다르게 준다. 앞의 두 필터 사이즈(128, 64)에는 strides를 2로, 뒤의 두 필터 사이즈에는 strides를 1로!\n",
        "- 쌓으면서 내내 똑같아야 하니까 'same' 옵션 주는 듯?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPT-TCqKxkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(inputs, image_size):\n",
        "    image_resize = image_size // 4\n",
        "\n",
        "    # generator 네트워크 파라미터\n",
        "    kernel_size = 5\n",
        "    layer_filters = [128, 64, 32, 1]\n",
        "\n",
        "    # 초기 입력층 설정\n",
        "    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n",
        "    print(f\"평활화 초기 층: {x.shape}\")\n",
        "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
        "    print(f\"reshape 후 초기 층: {x.shape}\")\n",
        "\n",
        "    # 은닉층\n",
        "    for filters in layer_filters:\n",
        "        if filters > layer_filters[-2]:\n",
        "            strides = 2\n",
        "        else:\n",
        "            strides = 1\n",
        "\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2DTranspose(filters=filters,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=strides,\n",
        "                            padding='same')(x)\n",
        "        \n",
        "        print(f\"이번 층: {x.shape}\")\n",
        "    \n",
        "    # 마지막 activation 층\n",
        "    x = Activation('sigmoid')(x)\n",
        "    g_model = Model(inputs, x, name='generator')\n",
        "\n",
        "    print(\"====== Generator 모델 전체 구조 ======\")\n",
        "    print(g_model.summary())\n",
        "\n",
        "    return g_model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxlSchpjUvfI",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator\n",
        "* 초기 레이어 설정: 그냥 input\n",
        "* 필터 사이즈는 점점 늘어 난다.\n",
        "* 첫 3개 레이어 필터에는 stride 2 적용하고, 마지막 필터 레이어만 1로 잡아 준다.\n",
        "* `LeakyReLU` : https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU\n",
        "* dense(1) 왜 노드 1?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSx_oAQ7Kxid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(inputs):\n",
        "    kernel_size = 5\n",
        "    layer_filters = [32, 64, 128, 256]\n",
        "\n",
        "    # 입력\n",
        "    x = inputs\n",
        "    print(f\"초기 입력: {x.shape}\")\n",
        "\n",
        "    # 은닉층 쌓기\n",
        "    for filters in layer_filters:\n",
        "        if filters == layer_filters[-1]:\n",
        "            strides = 1\n",
        "        else:\n",
        "            strides = 2\n",
        "\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = Conv2D(filters=filters,\n",
        "                   kernel_size=kernel_size,\n",
        "                   strides=strides,\n",
        "                   padding='same')(x)\n",
        "        \n",
        "        print(f\"이번 층: {x.shape}\")\n",
        "    \n",
        "    # activation 층\n",
        "    # 평활화 후 Dense로 뽑아 내기\n",
        "    x = Flatten()(x)\n",
        "    print(f\"평활화 후: {x.shape}\")\n",
        "    x = Dense(1)(x)\n",
        "    print(f\"Dense 거친 후: {x.shape}\")\n",
        "    x = Activation('sigmoid')(x)\n",
        "    print(f\"마지막 활성화 후: {x.shape}\")\n",
        "\n",
        "    # 모델 구성\n",
        "    d_model = Model(inputs, x, name='discriminator')\n",
        "    print(\"====== discriminator 모델 전체 구조 ======\")\n",
        "    print(d_model.summary())\n",
        "\n",
        "    return d_model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzYRdMhWkMj",
        "colab_type": "text"
      },
      "source": [
        "# 훈련\n",
        "\n",
        "* discriminator 학습\n",
        "    - 1.0, 0.0 라벨 하나씩 생성해서 지도학습 방식으로 훈련.\n",
        "    - 데이터셋에서 실제 이미지 랜덤하게 추출\n",
        "    - 가짜 이미지 생성.\n",
        "    - 실제 이미지와 가짜 이미지를 묶어 하나의 훈련 배치로 생성.\n",
        "    - 라벨 만드는 것은 이전과 동일한 방식.\n",
        "\n",
        "* generator(GAN= adversarial) 학습\n",
        "    - fake image 1배치. 라벨 1 나오도록 학습.\n",
        "    - discriminator 웨이트는 업데이트하지 않도록 설정.\n",
        "    - \n",
        "\n",
        "* log 모아서 한 번에 출력!\n",
        "\n",
        "* 모델 만드는 건 나중에 진행한다.\n",
        "* 논문은 Adam 사용했으나, RMSprop이 discriminator에서는 더 잘 작동하는듯하다.\n",
        "\n",
        "* discriminator compile metrics accuracy.\n",
        "\n",
        "\n",
        "* 여기서의 adversarial model = 앞에서의 GAN model인 듯.\n",
        "\n",
        "    - 왜 lr 반으로 주는가?\n",
        "    - 모델 자체를 넣는다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEUz5m5YWj63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(models, X_train, params):\n",
        "\n",
        "    # GAN 모델 구성 요소\n",
        "    generator, discriminator, adversarial = models\n",
        "\n",
        "    # 네트워크 파라미터\n",
        "    batch_size, latent_size, train_steps, model_names = params\n",
        "\n",
        "    # 훈련 데이터 개수\n",
        "    train_size = X_train.shape[0]\n",
        "\n",
        "    # 훈련\n",
        "    for i in range(train_steps):\n",
        "        \n",
        "        # 랜덤하게 이미지 고르기\n",
        "        rand_indices = np.random.randint(0, train_size, size=batch_size)\n",
        "        real_images = X_train[rand_indices]\n",
        "\n",
        "        # fake image 생성: generator\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # 훈련 데이터 1배치 생성\n",
        "        X = np.concatenate((real_images, fake_images))\n",
        "\n",
        "        # 실제 이미지 라벨 생성한 후, 반만 가짜 이미지로 바꾸기. 배치 뒤쪽이 가짜 이미지.\n",
        "        y = np.ones([2*batch_size, 1])\n",
        "        y[batch_size:, :] = 0.0 # 뒤쪽 라벨은 가짜 이미지이므로 0.0\n",
        "\n",
        "        # discriminator 학습\n",
        "        loss, acc = discriminator.train_on_batch(X, y)\n",
        "        log = \"Epoch %d: [D-loss: %.4f, acc: %.4f]\" % (i, loss, acc)\n",
        "\n",
        "        # generator: noise에 대해서 이미지 생성하고 1로 학습\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        y = np.ones([batch_size, 1])\n",
        "        loss, acc = adversarial.train_on_batch(noise, y)\n",
        "        \n",
        "        log = \"%s [G-loss: %.4f, acc: %.4f]\" % (log, loss, acc) # 로그 한 번에 출력\n",
        "        print(log)\n",
        "    \n",
        "    # generator 훈련 뒤 모델 저장\n",
        "    generator.save(model_name + \".h5\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aiwbm-yyeLf1",
        "colab_type": "text"
      },
      "source": [
        "# 모델 빌드 및 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjZK4BheLSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_and_train_models(load_W=False, train_steps=100):\n",
        "    model_name = \"dcgan_mnist\"\n",
        "\n",
        "   # 네트워크 파라미터 설정\n",
        "    batch_size = 64\n",
        "    lr = 2e-4\n",
        "    decay = 6e-8\n",
        "    input_shape = (image_size, image_size, 1)\n",
        "\n",
        "\n",
        "    ### Discriminator Model\n",
        "    inputs = Input(shape=input_shape, name='Discriminator_Input')\n",
        "    D = build_discriminator(inputs)\n",
        "    \n",
        "    # discriminator: RMSprop optimizer\n",
        "    print(\"====== Discriminator ======\")\n",
        "    D_opt = RMSprop(lr=lr, decay=decay)\n",
        "    D.compile(loss='binary_crossentropy',\n",
        "              optimizer=D_opt,\n",
        "              metrics=['accuracy'])\n",
        "    # print(D.summary())\n",
        "\n",
        "    # 저장된 모델 있으면 불러 오기\n",
        "    if load_W:\n",
        "        D.load_weights(f\"{data_path}/dcgan_D.h5\")\n",
        "\n",
        "    ### Generator Model: 학습하지 않음.\n",
        "    print(\"====== Discriminator ======\")\n",
        "    input_shape = (latent_size, )\n",
        "    inputs = Input(shape=input_shape, name='Z_Input')\n",
        "    G = build_generator(inputs, image_size)\n",
        "    # print(D.summary())\n",
        "\n",
        "    # 저장된 모델 있으면 불러 오기\n",
        "    if load_W:\n",
        "        G.load_weights(f\"{data_path}/dcgan_D.h5\")\n",
        "    \n",
        "    ### Adversarial Model: generator 모델 학습 + discriminator까지 같이 feed.\n",
        "    print(\"====== GAN 네트워크 ======\")\n",
        "    g_opt = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
        "    # D.trainable = False # GAN 네트워크 도중 discriminator 업데이트 해제\n",
        "    GAN = Model(inputs, D(G(inputs)), name=model_name)\n",
        "    GAN.compile(loss='binary_crossentropy', optimizer=g_opt, metrics=['accuracy'])\n",
        "    print(GAN.summary())\n",
        "\n",
        "    # D, GAN 네트워크 학습\n",
        "    models = (G, D, GAN)\n",
        "    params = (batch_size, latent_size, train_steps, model_name)\n",
        "    train(models, X_train, params) ##################################################\n",
        "\n",
        "    # 모델 저장\n",
        "    D.save_weights(f\"{data_path}/dcgan_D.h5\")\n",
        "    G.save_weights(f\"{data_path}/dcgan_G.h5\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiiNbYDsdGlP",
        "colab_type": "text"
      },
      "source": [
        "# 그림 그리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh-JtToEWj48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images():\n",
        "    inputs = Input(shape=(latent_size, ), name='Z_input')\n",
        "    G = build_generator(inputs, image_size)\n",
        "    G.load_weights(f\"{data_path}/dcgan_G.h5\")\n",
        "\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "    images = G.predict(noise_input) # 가짜 이미지 생성\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    num_images = images.shape[0]\n",
        "\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "    rows = int(np.sqrt(noise_input.shape[0]))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i+1)\n",
        "        image = np.reshape(images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7mgHtxeWwe",
        "colab_type": "text"
      },
      "source": [
        "# 학습!\n",
        "- 여기서는 이미 강사님이 학습해 둔 weight를 읽어 오고, 추가로 학습한다.\n",
        "- 그런데! 에러가 난다. : load weights with 5 layers 9 layers... 대충 뭐 층 안맞는다는 뜻인듯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef1pEBa1Wj2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49a6e9d3-5e35-4b42-b8f9-2a872b051148"
      },
      "source": [
        "build_and_train_models(load_W=False, train_steps=100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "초기 입력: (None, 28, 28, 1)\n",
            "이번 층: (None, 14, 14, 32)\n",
            "이번 층: (None, 7, 7, 64)\n",
            "이번 층: (None, 4, 4, 128)\n",
            "이번 층: (None, 4, 4, 256)\n",
            "평활화 후: (None, 4096)\n",
            "Dense 거친 후: (None, 1)\n",
            "마지막 활성화 후: (None, 1)\n",
            "====== discriminator 모델 전체 구조 ======\n",
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Discriminator_Input (InputLa [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 14, 14, 32)        832       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 64)          51264     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 4097      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,080,577\n",
            "Trainable params: 1,080,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "====== Discriminator ======\n",
            "====== Discriminator ======\n",
            "평활화 초기 층: (None, 6272)\n",
            "reshape 후 초기 층: (None, 7, 7, 128)\n",
            "이번 층: (None, 14, 14, 128)\n",
            "이번 층: (None, 28, 28, 64)\n",
            "이번 층: (None, 28, 28, 32)\n",
            "이번 층: (None, 28, 28, 1)\n",
            "====== Generator 모델 전체 구조 ======\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Z_Input (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DT (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DT (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,301,505\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n",
            "None\n",
            "====== GAN 네트워크 ======\n",
            "Model: \"dcgan_mnist\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Z_Input (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "generator (Model)            (None, 28, 28, 1)         1301505   \n",
            "_________________________________________________________________\n",
            "discriminator (Model)        (None, 1)                 1080577   \n",
            "=================================================================\n",
            "Total params: 2,382,082\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 1,081,281\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 0: [D-loss: 0.6983, acc: 0.0781] [G-loss: 0.8586, acc: 0.0000]\n",
            "Epoch 1: [D-loss: 0.6164, acc: 0.6016] [G-loss: 0.9232, acc: 0.0000]\n",
            "Epoch 2: [D-loss: 0.4989, acc: 0.9219] [G-loss: 0.9213, acc: 0.0000]\n",
            "Epoch 3: [D-loss: 0.3688, acc: 0.9531] [G-loss: 0.5163, acc: 1.0000]\n",
            "Epoch 4: [D-loss: 0.2494, acc: 1.0000] [G-loss: 0.9767, acc: 0.0000]\n",
            "Epoch 5: [D-loss: 0.2094, acc: 0.9922] [G-loss: 0.1288, acc: 1.0000]\n",
            "Epoch 6: [D-loss: 0.1793, acc: 1.0000] [G-loss: 0.7839, acc: 0.0000]\n",
            "Epoch 7: [D-loss: 0.1457, acc: 0.9844] [G-loss: 0.2465, acc: 1.0000]\n",
            "Epoch 8: [D-loss: 0.0525, acc: 1.0000] [G-loss: 0.1495, acc: 1.0000]\n",
            "Epoch 9: [D-loss: 0.0395, acc: 1.0000] [G-loss: 0.1046, acc: 1.0000]\n",
            "Epoch 10: [D-loss: 0.0335, acc: 1.0000] [G-loss: 0.0667, acc: 1.0000]\n",
            "Epoch 11: [D-loss: 0.0254, acc: 1.0000] [G-loss: 0.0473, acc: 1.0000]\n",
            "Epoch 12: [D-loss: 0.0210, acc: 1.0000] [G-loss: 0.0367, acc: 1.0000]\n",
            "Epoch 13: [D-loss: 0.0174, acc: 1.0000] [G-loss: 0.0264, acc: 1.0000]\n",
            "Epoch 14: [D-loss: 0.0120, acc: 1.0000] [G-loss: 0.0232, acc: 1.0000]\n",
            "Epoch 15: [D-loss: 0.0098, acc: 1.0000] [G-loss: 0.0199, acc: 1.0000]\n",
            "Epoch 16: [D-loss: 0.0102, acc: 1.0000] [G-loss: 0.0152, acc: 1.0000]\n",
            "Epoch 17: [D-loss: 0.0084, acc: 1.0000] [G-loss: 0.0124, acc: 1.0000]\n",
            "Epoch 18: [D-loss: 0.0068, acc: 1.0000] [G-loss: 0.0102, acc: 1.0000]\n",
            "Epoch 19: [D-loss: 0.0060, acc: 1.0000] [G-loss: 0.0086, acc: 1.0000]\n",
            "Epoch 20: [D-loss: 0.0051, acc: 1.0000] [G-loss: 0.0075, acc: 1.0000]\n",
            "Epoch 21: [D-loss: 0.0053, acc: 1.0000] [G-loss: 0.0055, acc: 1.0000]\n",
            "Epoch 22: [D-loss: 0.0040, acc: 1.0000] [G-loss: 0.0051, acc: 1.0000]\n",
            "Epoch 23: [D-loss: 0.0039, acc: 1.0000] [G-loss: 0.0043, acc: 1.0000]\n",
            "Epoch 24: [D-loss: 0.0031, acc: 1.0000] [G-loss: 0.0039, acc: 1.0000]\n",
            "Epoch 25: [D-loss: 0.0031, acc: 1.0000] [G-loss: 0.0031, acc: 1.0000]\n",
            "Epoch 26: [D-loss: 0.0033, acc: 1.0000] [G-loss: 0.0024, acc: 1.0000]\n",
            "Epoch 27: [D-loss: 0.0023, acc: 1.0000] [G-loss: 0.0024, acc: 1.0000]\n",
            "Epoch 28: [D-loss: 0.0021, acc: 1.0000] [G-loss: 0.0021, acc: 1.0000]\n",
            "Epoch 29: [D-loss: 0.0022, acc: 1.0000] [G-loss: 0.0018, acc: 1.0000]\n",
            "Epoch 30: [D-loss: 0.0016, acc: 1.0000] [G-loss: 0.0018, acc: 1.0000]\n",
            "Epoch 31: [D-loss: 0.0020, acc: 1.0000] [G-loss: 0.0013, acc: 1.0000]\n",
            "Epoch 32: [D-loss: 0.0016, acc: 1.0000] [G-loss: 0.0012, acc: 1.0000]\n",
            "Epoch 33: [D-loss: 0.0013, acc: 1.0000] [G-loss: 0.0012, acc: 1.0000]\n",
            "Epoch 34: [D-loss: 0.0011, acc: 1.0000] [G-loss: 0.0011, acc: 1.0000]\n",
            "Epoch 35: [D-loss: 0.0011, acc: 1.0000] [G-loss: 0.0010, acc: 1.0000]\n",
            "Epoch 36: [D-loss: 0.0012, acc: 1.0000] [G-loss: 0.0008, acc: 1.0000]\n",
            "Epoch 37: [D-loss: 0.0011, acc: 1.0000] [G-loss: 0.0007, acc: 1.0000]\n",
            "Epoch 38: [D-loss: 0.0012, acc: 1.0000] [G-loss: 0.0006, acc: 1.0000]\n",
            "Epoch 39: [D-loss: 0.0010, acc: 1.0000] [G-loss: 0.0005, acc: 1.0000]\n",
            "Epoch 40: [D-loss: 0.0006, acc: 1.0000] [G-loss: 0.0006, acc: 1.0000]\n",
            "Epoch 41: [D-loss: 0.0007, acc: 1.0000] [G-loss: 0.0006, acc: 1.0000]\n",
            "Epoch 42: [D-loss: 0.0007, acc: 1.0000] [G-loss: 0.0005, acc: 1.0000]\n",
            "Epoch 43: [D-loss: 0.0005, acc: 1.0000] [G-loss: 0.0005, acc: 1.0000]\n",
            "Epoch 44: [D-loss: 0.0006, acc: 1.0000] [G-loss: 0.0004, acc: 1.0000]\n",
            "Epoch 45: [D-loss: 0.0008, acc: 1.0000] [G-loss: 0.0003, acc: 1.0000]\n",
            "Epoch 46: [D-loss: 0.0007, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 47: [D-loss: 0.0006, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 48: [D-loss: 0.0005, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 49: [D-loss: 0.0005, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 50: [D-loss: 0.0004, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 51: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 52: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 53: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 54: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 55: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 56: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 57: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 58: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 59: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 60: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 61: [D-loss: 0.0003, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 62: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 63: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 64: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 65: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 66: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 67: [D-loss: 0.0002, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 68: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 69: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 70: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 71: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 72: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 73: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 74: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 75: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0003, acc: 1.0000]\n",
            "Epoch 76: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0001, acc: 1.0000]\n",
            "Epoch 77: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 78: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 79: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 80: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 81: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 82: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 83: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0004, acc: 1.0000]\n",
            "Epoch 84: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0003, acc: 1.0000]\n",
            "Epoch 85: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0004, acc: 1.0000]\n",
            "Epoch 86: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0004, acc: 1.0000]\n",
            "Epoch 87: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0007, acc: 1.0000]\n",
            "Epoch 88: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0002, acc: 1.0000]\n",
            "Epoch 89: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0008, acc: 1.0000]\n",
            "Epoch 90: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0005, acc: 1.0000]\n",
            "Epoch 91: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0009, acc: 1.0000]\n",
            "Epoch 92: [D-loss: 0.0001, acc: 1.0000] [G-loss: 0.0004, acc: 1.0000]\n",
            "Epoch 93: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0014, acc: 1.0000]\n",
            "Epoch 94: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0012, acc: 1.0000]\n",
            "Epoch 95: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0009, acc: 1.0000]\n",
            "Epoch 96: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0012, acc: 1.0000]\n",
            "Epoch 97: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0016, acc: 1.0000]\n",
            "Epoch 98: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0021, acc: 1.0000]\n",
            "Epoch 99: [D-loss: 0.0000, acc: 1.0000] [G-loss: 0.0040, acc: 1.0000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5640e84f3bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_and_train_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_W\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-788c75fe6fa9>\u001b[0m in \u001b[0;36mbuild_and_train_models\u001b[0;34m(load_W, train_steps)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-320aece37685>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, X_train, params)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# generator 훈련 뒤 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrX9Tb7uWj1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이미지 표시\n",
        "plot_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tlUvrj3Wju4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkDv_dUjKxgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZOmVJBvKx7L",
        "colab_type": "text"
      },
      "source": [
        "# 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRV6WDO8KtPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(mnist)\n",
        "mnist.load_data?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdXq5NiQLI2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def build_generator(inputs, height=n_height, width=n_width):\n",
        "#     # resize image\n",
        "#     new_height = height // 4\n",
        "#     new_width = width // 4\n",
        "\n",
        "#     # generator 파라미터\n",
        "#     kernel_size = 5\n",
        "#     layer_filters = [128, 64, 32, 1]\n",
        "\n",
        "#     # 초기 층 설정\n",
        "#     x = Dense(new_height * new_width * layer_filters[0])(inputs)\n",
        "#     print(f\"초기 평활화 층: {x.shape}\")\n",
        "#     x = Reshape(target_shape=(new_height, new_width, layer_filters[0]))(x)\n",
        "#     print(f\"3차원 reshape 후: {x.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLZ6V_aPNeVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 초기 층 만든 뒤\n",
        "build_generator(X_train[0], 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY-PDHQQNxZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}