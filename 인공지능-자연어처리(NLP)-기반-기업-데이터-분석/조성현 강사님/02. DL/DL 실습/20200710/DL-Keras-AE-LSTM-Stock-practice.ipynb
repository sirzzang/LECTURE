{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Keras-AE-LSTM-Stock-practice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaScHPQAF56",
        "colab_type": "text"
      },
      "source": [
        "# _0_. MyUtils Package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGcW5LqEAMOS",
        "colab_type": "text"
      },
      "source": [
        "## ComFeatureSet.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FI21bSnBHAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supervised Learning을 위한 class를 부여한다\n",
        "# 종목마다 변동성이 다르므로 목표 수익률을 \"변동성의 몇 배\"로 정의한다.\n",
        "# ex : up = 0.5 이면 변동성의 +0.5 배.\n",
        "# up : 목표 수익률 표준편차 배수\n",
        "# dn : 손절률 표준편차 배수\n",
        "# period : holding 기간\n",
        "# return : 0 - 주가 횡보, 1 - 주가 하락, 2 - 주가 상승\n",
        "# --------------------------------------------------------------\n",
        "def getUpDnClass(data, up=1, dn=-1, period=20):\n",
        "    # 주가 수익률의 표준편차를 측정한다. 컬럼 이름은 class라고 부여해 놓는다.\n",
        "    # 수익률 표준편차 (변동성)는 목표 수익률과 손절률을 계산하기 위해 임시로 사용된다.\n",
        "    data['class'] = np.log(data['close']) - np.log(data['close'].shift(1))\n",
        "    s = np.std(data['class'])\n",
        "\n",
        "    # 목표 수익률과 손절률을 계산한다\n",
        "    uLimit = up * s * np.sqrt(period)\n",
        "    dLimit = dn * s * np.sqrt(period)\n",
        "    \n",
        "    # 가상 Trading을 통해 미래 주가 방향에 대한 Class를 결정한다. class에는 원래 수익률이 기록되어 있었으나 NaN을 기록해 둔다\n",
        "    data['class'] = np.nan\n",
        "    for i in range(len(data)-period):\n",
        "        buyPrc = data.iloc[i].close     # 매수 포지션을 취한다\n",
        "        y = np.nan\n",
        "            \n",
        "        # 매수 포지션 이후 청산 지점을 결정한다\n",
        "        duration = 0    # 보유 기간\n",
        "        for k in range(i+1, len(data)):\n",
        "            sellPrc = data.iloc[k].close\n",
        "            #rtn = np.log(sellPrc / buyPrc)  # 수익률을 계산한다\n",
        "            rtn = (sellPrc - buyPrc) / buyPrc\n",
        "            \n",
        "            # 목표 수익률이나 손절률에 도달하면 루프를 종료한다\n",
        "            if duration > period:\n",
        "                y = 0           # hoding 기간 동안 목표 수익률이나 손절률에 도달하지 못했음. 주가가 횡보한 것임.\n",
        "                break\n",
        "            else:\n",
        "                if rtn > uLimit:\n",
        "                    y = 2       # 수익\n",
        "                    break\n",
        "                elif rtn < dLimit:\n",
        "                    y = 1       # 손실\n",
        "                    break\n",
        "            duration += 1\n",
        "        data.iloc[i, 5] = y     # class 컬럼에 y를 기록한다.\n",
        "    data = data.dropna()    # 마지막 부분은 class를 결정하지 못해 NaN이 기록되어 있으므로 이를 제거한다.\n",
        "    return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thaw_GOBAE6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def getUpDnClass(data, up=1, dn=-1, period=20):\n",
        "#     data['class'] = np.log(data['close']) - np.log(data['close'].shift(1)) # close 누적 해체\n",
        "#     s = np.std(data['class']) # 변동성 : 수익률 표준편차\n",
        "\n",
        "#     u_limit = up*s*np.sqrt(period)\n",
        "#     d_limit = dn*s*np.sqrt(period)\n",
        "\n",
        "#     # 미래 주가방향에 대한 class 결정\n",
        "#     data['class'] = np.nan # NaN 기록\n",
        "#     for i in range(len(data)-period):\n",
        "#         buy_prc = data.iloc[i]['close'] # 매수 포지션\n",
        "#         y = np.nan\n",
        "\n",
        "#         # 청산지점 결정\n",
        "#         duration = 0 # 보유 기간\n",
        "#         for k in range(i+1, len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g64xhlvbBIVE",
        "colab_type": "text"
      },
      "source": [
        "## MyTimeSeries.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo9yh-OYBNs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시계열분석 관련 함수를 정의한다\n",
        "#\n",
        "# 한국생산성본부 금융 빅데이터 전문가 과정 (금융 모델링 파트) 실습용 코드\n",
        "# Written : 2018.2.5\n",
        "# 제작 : 조성현\n",
        "# -----------------------------------------------------------------\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from statsmodels.tsa.arima_process import arma_generate_sample\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ARIMA(ar, d, ma) 모형으로 n개의 데이터를 샘플링한다\n",
        "def sampleARIMA(ar, d, ma, n):\n",
        "    arparams = np.array(ar)\n",
        "    maparams = np.array(ma)\n",
        "    ar = np.r_[1.0, -arparams] # add zero-lag and negate\n",
        "    ma = np.r_[1.0, maparams]  # add zero-lag\n",
        "    \n",
        "    # ARMA 모형으로 n개의 데이터를 샘플링한다\n",
        "    y = arma_generate_sample(ar, ma, n)\n",
        "    \n",
        "    # 지정된 차분 횟수 (d) 만큼 누적한다\n",
        "    for i in np.arange(d):\n",
        "        y = np.cumsum(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "# 시계열 데이터의 정규성을 확인한다\n",
        "def checkNormality(data):\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    p1 = fig.add_subplot(2,2,1)\n",
        "    p2 = fig.add_subplot(2,2,2)\n",
        "    p3 = fig.add_subplot(2,2,3)\n",
        "    p4 = fig.add_subplot(2,2,4)\n",
        "    \n",
        "    p1.plot(data)  # 육안으로 백색 잡음 형태인지 확인한다\n",
        "    p1.set_title(\"Data\")\n",
        "    \n",
        "    # Residual의 분포를 육안으로 확인한다\n",
        "    r = np.copy(data)\n",
        "    r.sort()\n",
        "    pdf = stats.norm.pdf(r, np.mean(r), np.std(r))\n",
        "    p2.plot(r,pdf)\n",
        "    p2.hist(r, density=True, bins=100)\n",
        "    p2.set_title(\"Distribution\")\n",
        "    \n",
        "    # Q-Q plot을 그린다\n",
        "    stats.probplot(data, dist=\"norm\", plot=p3)\n",
        "    \n",
        "    # ACF plot을 확인한다. 백색 잡음은 자기상관성이 없다.\n",
        "    plot_acf(data, lags=100, ax=p4)\n",
        "    \n",
        "    # Shapiro-Wilks 검정을 수행한다\n",
        "    # (검정통계량, p-value)가 출력된다.\n",
        "    # 귀무가설 : 정규분포 이다, 대립가설 : 아니다\n",
        "    # p-value > 0.05 이면 귀무가설을 기각할 수 없다 --> \"정규분포이다\"\n",
        "    w = stats.shapiro(data)\n",
        "    print()\n",
        "    print(\"Shapiro-Wilks 검정 : 검정통계량 = %.4f, p-value = %.4f\" % (w[0], w[1]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6R1w3dkBSzQ",
        "colab_type": "text"
      },
      "source": [
        "## MyTimeSeries.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2eYZ_N6BUg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 시계열분석 관련 함수를 정의한다\n",
        "#\n",
        "# 한국생산성본부 금융 빅데이터 전문가 과정 (금융 모델링 파트) 실습용 코드\n",
        "# Written : 2018.2.5\n",
        "# 제작 : 조성현\n",
        "# -----------------------------------------------------------------\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from statsmodels.tsa.arima_process import arma_generate_sample\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ARIMA(ar, d, ma) 모형으로 n개의 데이터를 샘플링한다\n",
        "def sampleARIMA(ar, d, ma, n):\n",
        "    arparams = np.array(ar)\n",
        "    maparams = np.array(ma)\n",
        "    ar = np.r_[1.0, -arparams] # add zero-lag and negate\n",
        "    ma = np.r_[1.0, maparams]  # add zero-lag\n",
        "    \n",
        "    # ARMA 모형으로 n개의 데이터를 샘플링한다\n",
        "    y = arma_generate_sample(ar, ma, n)\n",
        "    \n",
        "    # 지정된 차분 횟수 (d) 만큼 누적한다\n",
        "    for i in np.arange(d):\n",
        "        y = np.cumsum(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "# 시계열 데이터의 정규성을 확인한다\n",
        "def checkNormality(data):\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    p1 = fig.add_subplot(2,2,1)\n",
        "    p2 = fig.add_subplot(2,2,2)\n",
        "    p3 = fig.add_subplot(2,2,3)\n",
        "    p4 = fig.add_subplot(2,2,4)\n",
        "    \n",
        "    p1.plot(data)  # 육안으로 백색 잡음 형태인지 확인한다\n",
        "    p1.set_title(\"Data\")\n",
        "    \n",
        "    # Residual의 분포를 육안으로 확인한다\n",
        "    r = np.copy(data)\n",
        "    r.sort()\n",
        "    pdf = stats.norm.pdf(r, np.mean(r), np.std(r))\n",
        "    p2.plot(r,pdf)\n",
        "    p2.hist(r, density=True, bins=100)\n",
        "    p2.set_title(\"Distribution\")\n",
        "    \n",
        "    # Q-Q plot을 그린다\n",
        "    stats.probplot(data, dist=\"norm\", plot=p3)\n",
        "    \n",
        "    # ACF plot을 확인한다. 백색 잡음은 자기상관성이 없다.\n",
        "    plot_acf(data, lags=100, ax=p4)\n",
        "    \n",
        "    # Shapiro-Wilks 검정을 수행한다\n",
        "    # (검정통계량, p-value)가 출력된다.\n",
        "    # 귀무가설 : 정규분포 이다, 대립가설 : 아니다\n",
        "    # p-value > 0.05 이면 귀무가설을 기각할 수 없다 --> \"정규분포이다\"\n",
        "    w = stats.shapiro(data)\n",
        "    print()\n",
        "    print(\"Shapiro-Wilks 검정 : 검정통계량 = %.4f, p-value = %.4f\" % (w[0], w[1]))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrkUowD0BPht",
        "colab_type": "text"
      },
      "source": [
        "## PatternFeatureSet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSbAPeTBOOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OHLCV 데이터에서 기술적 분석 지표들의 FeatureSet을 추출한다\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from MyUtil.ComFeatureSet import getUpDnClass\n",
        "\n",
        "# 과거 n-day 동안의 주가 패턴으로 Feature Set을 구성한다\n",
        "def getPatternFeatureSet(data, u, d, nPast=20, nHop=3, nFuture=20, binary=False):\n",
        "    # OHLCV 데이터에 class를 부여한다.\n",
        "    df = getUpDnClass(data, up=u, dn=d, period=nFuture)\n",
        "    \n",
        "    # 학습 데이터를 구성한다.\n",
        "    ds = getclosePatternWithClass(df, nPast, nHop = nHop)\n",
        "    \n",
        "    # Class는 0, 1, 2로 (multi-class) 측정되었는데, binary-classification을\n",
        "    # 위해서는 주가 횡보인 class=0을 1로 대치하고, class = 1을 0으로, 2를 1로 변환한다.\n",
        "    if binary:\n",
        "#        ds.loc[ds['class'] == 0.0, 'class'] = 1.0\n",
        "        ds = ds[ds['class'] != 0.0]\n",
        "        ds['class'] -= 1.0\n",
        "    return ds\n",
        "    \n",
        "# OHLCV 데이터에서 종가 (close)를 기준으로 과거 n-기간 동안의 Pattern을 구성한다\n",
        "# nHop = 3 : 3기간씩 건너 뛰면서 pattern을 구성한다.\n",
        "def getclosePattern(data, n, nHop = 3, normalize=True):\n",
        "    loc = tuple(range(0, len(data) - n, nHop))\n",
        "    \n",
        "    # n개의 column을 가진 데이터프레임을 생성한다\n",
        "    column = [str(e) for e in range(1, (n+1))]\n",
        "    df = pd.DataFrame(columns=column)\n",
        "    \n",
        "    for i in loc:\n",
        "        pt = data['close'].iloc[i:(i+n)].values\n",
        "        \n",
        "        if normalize:\n",
        "            pt = (pt - pt.mean()) / pt.std()\n",
        "        df = df.append(pd.DataFrame([pt],columns=column, index=[data.index[i+n]]), ignore_index=False)\n",
        "        \n",
        "    return df\n",
        "\n",
        "# OHLCV + class 데이터에서 종가 (close)를 기준으로 과거 n-기간 동안의 Pattern을 구성한다\n",
        "# nHop = 3 : 3기간씩 건너 뛰면서 pattern을 구성한다.\n",
        "# 리턴 값 :\n",
        "#           1         2         3  ...          20       vol  class\n",
        "# 0  0.056859 -0.492078 -1.041017  ...    1.586034  0.187116    0.0\n",
        "# 1  0.056859 -0.492078 -1.041017  ...    1.586034  0.187116    2.0\n",
        "# 2  0.056859 -0.492078 -1.041017  ...    1.586034  0.187116    1.0\n",
        "# ...\n",
        "def getclosePatternWithClass(data, n, nHop = 3, normalize=True):\n",
        "    # 패턴의 시작 지점을 확인해 둔다\n",
        "    loc = tuple(range(0, len(data) - n, nHop))\n",
        "    \n",
        "    # 1~n의 column과 vol, class을 가진 데이터프레임을 생성한다\n",
        "    column = np.array([str(e) for e in range(1, (n+1))])\n",
        "    column = np.append(column, ['vol', 'class'])\n",
        "    df = pd.DataFrame(columns=column)\n",
        "    \n",
        "    # 패턴 시작 지점부터 n-기간 동안의 종가, 변동성, class를 column으로 갖는 dataframe을 생성한다\n",
        "    for i in loc:       \n",
        "        # n-기간 동안의 종가 패턴\n",
        "        closePat = np.array(data['close'].iloc[i:(i+n)])\n",
        "        \n",
        "        # n-기간의 마지막 데이터의 class\n",
        "        classY = data['class'].iloc[i+n-1]\n",
        "        \n",
        "        # closePat의 표준편차를 계산한다.\n",
        "        # 주가 수익률의 표준편차로 변동성을 측정하는 것이 일반적이나, 여기서는\n",
        "        # 주가의 표준편차 / 평균 주가로 측정한다.\n",
        "        vol = np.sqrt(n) * np.std(closePat) / np.mean(closePat)\n",
        "        \n",
        "        if normalize:\n",
        "            closePat = (closePat - np.mean(closePat)) / np.std(closePat)\n",
        "        \n",
        "        # n-기간 동안의 종가, 변동성, class를 colume으로 dataframe을 생성한다. (1-row)\n",
        "        closePat = np.append(closePat, [vol, classY])\n",
        "        tmpdf = pd.DataFrame([closePat], columns=column)\n",
        "        \n",
        "        # 결과 dataframe인 df에 계속 추가한다 (row bind)\n",
        "        df = df.append(tmpdf)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dCxDcyvBRqQ",
        "colab_type": "text"
      },
      "source": [
        "## StockLabel.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0URUl4FBXta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Labeling for supervised learning\n",
        "# \n",
        "# 1. returnLabel() - 주어진 period 동안의 수익률이 upper bound를 touch 하면 \"BUY\", lower bound를 touch하면 \"SELL\", 아니면 \"HOLD\"\n",
        "# 2. barrierLabel() - 주어진 period 동안 upper bound를 touch 하면 \"BUY\", lower bound를 touch하면 \"SELL\", 아니면 \"HOLD\"\n",
        "# 3. tradeLabeling() - 주어진 window 크기에 대해 시계열 데이터의 저점에서 매수 (Buy)하고, 고점에서 매도 (Sell)하는 action을 찾는다\n",
        "# 3-1. calculateRtn() - tradeLabeling() 함수가 결정한 action sequence를 따를 때의 누적 수익률을 계산한다\n",
        "# 3-2. optimizeLabel() - 누적 수익률이 최대가 되는 window size를 결정한다. optimal tradeLabeling을 결정함.\n",
        "# 3-2. neighborAction() - optimal labelel에 대해 분할 매수, 분할 매도를 적용한다.\n",
        "#\n",
        "# 2018.12.03, 아마추어퀀트 (조성현)\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "HOLD = 0\n",
        "BUY = 1\n",
        "SELL = 2\n",
        "sAction = ['HOLD', 'BUY', 'SELL']\n",
        "\n",
        "# Algorithm #1 : Fixed period return Labeling for supervised learning.\n",
        "def returnLabel(df, upper=1, lower=-1, period=5):\n",
        "    # 현재 주가와 period 이후 시점의 주가와의 수익률을 측정한다. \n",
        "    df['rtn'] = np.log(df['close'].shift(-period)) - np.log(df['close'])\n",
        "    df = df.dropna()\n",
        "\n",
        "    # 목표 수익률과 손절률을 계산한다\n",
        "    s = df['rtn'].std() # n-기간 변동성\n",
        "    uLimit = df['rtn'].mean() + upper * s\n",
        "    dLimit = df['rtn'].mean() + lower * s\n",
        "    \n",
        "    # label을 부여한다\n",
        "    rtn = np.array(df['rtn'])\n",
        "    \n",
        "    df2 = df.copy()  # 자꾸 SettingWithCopyWarning: 이 발생해서 copy해서 사용\n",
        "    df2['label'] = HOLD\n",
        "    df2.loc[np.where(rtn > uLimit)[0], 'label'] = BUY\n",
        "    df2.loc[np.where(rtn < dLimit)[0], 'label'] = SELL\n",
        "    return df2\n",
        "    \n",
        "# Algorithm #2 : Barrier Labeling for supervised learning.\n",
        "#\n",
        "# 종목마다 변동성이 다르므로 목표 수익률을 \"변동성의 몇 배\"로 정의한다.\n",
        "# ex : up = 0.5 이면 변동성의 +0.5 배.\n",
        "# up : 목표 수익률 표준편차 배수\n",
        "# dn : 손절률 표준편차 배수\n",
        "# period : holding 기간\n",
        "# return : 0 - 주가 횡보, 1 - 주가 하락, 2 - 주가 상승\n",
        "# --------------------------------------------------------------\n",
        "def barrierLabel(df, upper=1, lower=-1, period=20):\n",
        "    # 주가 수익률의 표준편차를 측정한다. 컬럼 이름은 label 이라고 임시로 부여해 놓는다.\n",
        "    # 수익률 표준편차 (변동성)는 목표 수익률과 손절률을 계산하기 위해 임시로 사용된다.\n",
        "    df['label'] = np.log(df['close']) - np.log(df['close'].shift(1))\n",
        "    s = np.std(df['label'])\n",
        "\n",
        "    # 목표 수익률과 손절률을 계산한다\n",
        "    uLimit = upper * s * np.sqrt(period)\n",
        "    dLimit = lower * s * np.sqrt(period)\n",
        "    \n",
        "    # 가상 Trading을 통해 미래 주가 방향에 대한 Class를 결정한다. class에는 원래 수익률이 기록되어 있었으나 NaN을 기록해 둔다\n",
        "    df['label'] = np.nan\n",
        "    for i in range(len(df)-period):\n",
        "        buyPrc = df.iloc[i].close     # 매수 포지션을 취한다\n",
        "        y = np.nan\n",
        "            \n",
        "        # 매수 포지션 이후 청산 지점을 결정한다\n",
        "        duration = 0    # 보유 기간\n",
        "        for k in range(i+1, len(df)):\n",
        "            sellPrc = df.iloc[k].close\n",
        "            rtn = np.log(sellPrc / buyPrc)  # 수익률을 계산한다\n",
        "#            rtn = (sellPrc - buyPrc) / buyPrc\n",
        "            \n",
        "            # 목표 수익률이나 손절률에 도달하면 루프를 종료한다\n",
        "            if duration > period:\n",
        "                y = HOLD           # hoding 기간 동안 목표 수익률이나 손절률에 도달하지 못했음. 주가가 횡보한 것임.\n",
        "                break\n",
        "            else:\n",
        "                if rtn > uLimit:\n",
        "                    y = BUY       # 수익\n",
        "                    break\n",
        "                elif rtn < dLimit:\n",
        "                    y = SELL       # 손실\n",
        "                    break\n",
        "            duration += 1\n",
        "        df.loc[i, 'label'] = y     # label 컬럼에 y를 기록한다.\n",
        "    df = df.dropna()    # 마지막 부분은 label을 결정하지 못해 NaN이 기록되어 있으므로 이를 제거한다.\n",
        "    return df\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------\n",
        "# Algorithm #3 : Action Labeling for supervised learning.\n",
        "# \n",
        "# 1. tradeLabeling() - 주어진 window 크기에 대해 시계열 데이터의 저점에서 매수 (Buy)하고, 고점에서 매도 (Sell)하는 action을 찾는다\n",
        "# 2. calculateRtn() - tradeLabeling() 함수가 결정한 action sequence를 따를 때의 누적 수익률을 계산한다\n",
        "# 3. optimizeLabel() - 누적 수익률이 최대가 되는 window size를 결정한다. optimal tradeLabeling을 결정함.\n",
        "# 4. neighborAction() - optimal labelel에 대해 분할 매수, 분할 매도를 적용한다.\n",
        "# --------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 주어진 window 크기에 대해 시계열 데이터의 저점에서 매수 (Buy)하고, 고점에서 매도 (Sell)하는 action sequence를 찾는다\n",
        "def tradeLabeling(data, nWindow):\n",
        "    nPtr = 0\n",
        "    nDays = data.shape[0]\n",
        "    action = np.repeat(HOLD, nDays)\n",
        "    while nPtr < nDays:\n",
        "        if nPtr > nWindow:\n",
        "            idxBegin = nPtr - nWindow\n",
        "            idxEnd = idxBegin + nWindow - 1\n",
        "            midIndex = int((idxBegin + idxEnd) / 2)\n",
        "            \n",
        "            prcMin = 99999999999\n",
        "            prcMax = 0\n",
        "            for i in np.arange(idxBegin, (idxEnd + 1)):\n",
        "                price = data[i]\n",
        "                if price < prcMin:\n",
        "                    prcMin = price\n",
        "                    minIdx = i\n",
        "                if price > prcMax:\n",
        "                    prcMax = price\n",
        "                    maxIdx = i\n",
        "            if maxIdx == midIndex:\n",
        "                action[maxIdx] = SELL\n",
        "            if minIdx == midIndex:\n",
        "                action[minIdx] = BUY\n",
        "        nPtr += 1\n",
        "    \n",
        "    # 중첩된 action을 제거한다. [BUY - BUY] --> [HOLD - BUY] or [BUY - HOLD]\n",
        "    n = 0\n",
        "    for i in np.where(action != HOLD)[0]:\n",
        "        n += 1\n",
        "        if n == 1:\n",
        "            prevIdx = i\n",
        "            prevAct = action[i]\n",
        "            prevPrc = data[i]\n",
        "            continue\n",
        "        \n",
        "        if prevAct == BUY and action[i] == BUY:\n",
        "            # 둘 중 높은 가격 지점은 HOLD로 바꿔준다\n",
        "            if prevPrc > data[i]:\n",
        "                action[prevIdx] = HOLD\n",
        "            else:\n",
        "                action[i] = HOLD\n",
        "            \n",
        "        if prevAct == SELL and action[i] == SELL:\n",
        "            # 둘 중 낮은 가격 지점은 HOLD로 바꿔준다\n",
        "            if prevPrc < data[i]:\n",
        "                action[prevIdx] = HOLD\n",
        "            else:\n",
        "                action[i] = HOLD\n",
        "\n",
        "        if action[i] != HOLD:\n",
        "            prevIdx = i\n",
        "            prevAct = action[i]\n",
        "            prevPrc = data[i]\n",
        "    return action\n",
        "\n",
        "# tradeLabeling() 함수가 결정한 action sequence를 따를 때의 누적 수익률을 계산한다\n",
        "def calculateRtn(data, action):\n",
        "    profit = 0.0\n",
        "    nTrade = 0\n",
        "    n = 0\n",
        "    for i in np.where(action != HOLD)[0]:\n",
        "        n += 1\n",
        "        if n == 1:\n",
        "            prevAct = action[i]\n",
        "            prevPrc = data[i]\n",
        "            continue\n",
        "        \n",
        "        if prevAct == BUY and action[i] == SELL:\n",
        "            profit += 100 * np.log(data[i] / prevPrc) - 5.0\n",
        "            nTrade += 1\n",
        "            n = 0\n",
        "        if prevAct == SELL and action[i] == BUY:\n",
        "            profit += 100 * np.log(prevPrc / data[i]) - 5.0\n",
        "            nTrade += 1\n",
        "            n = 0\n",
        "            \n",
        "    return profit, nTrade\n",
        "\n",
        "# 누적 수익률이 최대가 되는 window size를 결정한다. optimal tradeLabeling을 결정한다.\n",
        "def optimizeLabel(data, verbose=False):\n",
        "    trajProfit = []\n",
        "    maxProfit = 0\n",
        "    maxWindow = 0\n",
        "    maxAction = []\n",
        "    maxTrade = 0\n",
        "    for i in np.arange(10, int(len(data) / 60)):\n",
        "        action = tradeLabeling(data, i)\n",
        "        profit, nTrade = calculateRtn(data, action)\n",
        "        if profit > maxProfit:\n",
        "            maxProfit = profit\n",
        "            maxWindow = i\n",
        "            maxAction = np.copy(action)\n",
        "            maxTrade = nTrade\n",
        "        trajProfit.append(profit)\n",
        "        if verbose == True:\n",
        "            print(\"nWindow = %d, profit = %.4f (%s)\" % (i, profit, '%'))\n",
        "    return maxWindow, maxProfit, maxTrade, maxAction, trajProfit\n",
        "\n",
        "# optimal labelel에 대해 분할 매수, 분할 매도를 적용한다.\n",
        "def neighborAction(action, neighbor=1):\n",
        "    # action 전,후를 동일하게 만든다. [HOLD - BUY - HOLD] --> [BUY - BUY - BUY]\n",
        "    # 특정 지점이 아닌 부근에서 매수하거나 매도하는 것으로..\n",
        "    for i in np.arange(0, neighbor):\n",
        "        loc = np.where(action == BUY)[0]\n",
        "        action[loc - 1] = BUY\n",
        "        action[loc + 1] = BUY\n",
        "        \n",
        "        loc = np.where(action == SELL)[0]\n",
        "        action[loc - 1] = SELL\n",
        "        action[loc + 1] = SELL\n",
        "    return action\n",
        "\n",
        "def tradeLabel(df, window=20, optimize=True, neighbor=1, verbose=False):\n",
        "    data = np.array(df['close'])\n",
        "    if optimize == True:\n",
        "        nWindow, profit, trade, action, trajProfit = optimizeLabel(data, verbose=verbose)\n",
        "        if neighbor > 0:\n",
        "            action = neighborAction(action, neighbor)\n",
        "    else:\n",
        "        action = tradeLabeling(data, window)\n",
        "        if neighbor > 0:\n",
        "            action = neighborAction(action, neighbor)\n",
        "    \n",
        "    df['label'] = pd.DataFrame(action)\n",
        "    return df, trajProfit"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK9JVPxzBY45",
        "colab_type": "text"
      },
      "source": [
        "## TaFeatureSet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K31XVhC-BbMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OHLCV 데이터에서 기술적 분석 지표들의 FeatureSet을 추출한다\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "from scipy import ndimage\n",
        "# from MyUtil.ComFeatureSet import getUpDnClass\n",
        "\n",
        "# OHLCV 데이터로부터 기술적분석 (TA) Feature Set을 구성한다\n",
        "def getTaFeatureSet(data, u, d, period, binary=False):\n",
        "    # OHLCV 데이터에 class를 부여한다.\n",
        "    df = getUpDnClass(data, up=u, dn=d, period=period)\n",
        "    \n",
        "    # Feature value를 계산한 후 Z-score Normalization 한다\n",
        "    fmacd = scale(MACD(data, 12, 26, 9))\n",
        "    frsi = scale(RSI(data, 40))\n",
        "    fobv = scale(OBV(data, ext=True))\n",
        "    fliquidity = scale(Liquidity(data))\n",
        "    fparkinson = scale(ParkinsonVol(data, 10))\n",
        "    fvol = scale(closeVol(data, 10))\n",
        "    \n",
        "    ft = pd.DataFrame()\n",
        "    ft['macd'] = fmacd\n",
        "    ft['rsi'] = frsi\n",
        "    ft['obv'] = fobv\n",
        "    ft['liquidity'] = fliquidity\n",
        "    ft['parkinson'] = fparkinson\n",
        "    ft['volatility'] = fvol\n",
        "    ft['class'] = df['class']\n",
        "    ft = ft.dropna()\n",
        "    \n",
        "    # Class는 0, 1, 2로 (multi-class) 측정되었는데, binary-classification을\n",
        "    # 위해서는 주가 횡보인 class=0을 제거하고, class = 1을 0으로, 2를 1로 변환한다.\n",
        "    if binary:\n",
        "        ft = ft[ft['class'] != 0.0]\n",
        "        ft['class'] -= 1.0\n",
        "        \n",
        "    # Feature들의 value (수준) 보다는 방향 (up, down)을 분석하는 것이 의미가 있어 보임.\n",
        "    # 방향을 어떻게 검출할 지는 향후 연구 과제로 한다\n",
        "\n",
        "    return ft\n",
        "\n",
        "# MACD 지표를 계산한다\n",
        "# MACD Line : 12-day EMA - 26-day EMA\n",
        "# Signal Line : 9-day EMA of MACD line\n",
        "# MACD oscilator : MACD Line - Signal Line\n",
        "# ----------------------------------------\n",
        "def MACD(ohlc, nFast=12, nSlow=26, nSig=9, percent=True):\n",
        "    ema1 = EMA(ohlc.close, nFast)\n",
        "    ema2 = EMA(ohlc.close, nSlow)\n",
        "    \n",
        "    if percent:\n",
        "        macdLine =  100 * (ema1 - ema2) / ema2\n",
        "    else:\n",
        "        macdLine =  ema1 - ema2\n",
        "    signalLine = EMA(macdLine, nSig)\n",
        "    \n",
        "    return pd.DataFrame(macdLine - signalLine, index=ohlc.index)\n",
        "\n",
        "# 지수이동평균을 계산한다\n",
        "# data : Series\n",
        "def EMA(data, n):\n",
        "    ma = []\n",
        "    \n",
        "    # data 첫 부분에 na 가 있으면 skip한다\n",
        "    x = 0\n",
        "    while True:\n",
        "        if math.isnan(data[x]):\n",
        "            ma.append(data[x])\n",
        "        else:\n",
        "            break;\n",
        "        x += 1\n",
        "        \n",
        "    # x ~ n - 1 기간까지는 na를 assign 한다\n",
        "    for i in range(x, x + n - 1):\n",
        "        ma.append(np.nan)\n",
        "    \n",
        "    # x + n - 1 기간은 x ~ x + n - 1 까지의 평균을 적용한다\n",
        "    sma = np.mean(data[x:(x + n)])\n",
        "    ma.append(sma)\n",
        "    \n",
        "    # x + n 기간 부터는 EMA를 적용한다\n",
        "    k = 2 / (n + 1)\n",
        "    \n",
        "    for i in range(x + n, len(data)):\n",
        "        #print(i, data[i])\n",
        "        ma.append(ma[-1] + k * (data[i] - ma[-1]))\n",
        "    \n",
        "    return pd.Series(ma, index=data.index)\n",
        "\n",
        "# RSI 지표를 계산한다. (Momentum indicator)\n",
        "# U : Gain, D : Loss, AU : Average Gain, AD : Average Loss\n",
        "# smoothed RS는 고려하지 않았음.\n",
        "# --------------------------------------------------------\n",
        "def RSI(ohlc, n=14):\n",
        "    closePrice = pd.DataFrame(ohlc.close)\n",
        "    U = np.where(closePrice.diff(1) > 0, closePrice.diff(1), 0)\n",
        "    D = np.where(closePrice.diff(1) < 0, closePrice.diff(1) * (-1), 0)\n",
        "    \n",
        "    U = pd.DataFrame(U, index=ohlc.index)\n",
        "    D = pd.DataFrame(D, index=ohlc.index)\n",
        "    \n",
        "    AU = U.rolling(window=n).mean()\n",
        "    AD = D.rolling(window=n).mean()\n",
        "\n",
        "    return 100 * AU / (AU + AD)\n",
        "    \n",
        "# On Balance volume (OBV) : buying and selling pressure\n",
        "# ext = False : 기존의 OBV\n",
        "# ext = True  : Extended OBV. 가격 변화를 이용하여 거래량을 매수수량, 매도수량으로 분해하여 매집량 누적\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "def OBV(ohlcv, ext=True):\n",
        "    obv = [0]\n",
        "    \n",
        "    # 기존의 OBV\n",
        "    if ext == False:\n",
        "        # 기술적 지표인 OBV를 계산한다\n",
        "        for curr, prev in zip(ohlcv.itertuples(), ohlcv.shift(1).itertuples()):\n",
        "            if math.isnan(prev.volume):\n",
        "                continue\n",
        "            \n",
        "            if curr.close > prev.close:\n",
        "                obv.append(obv[-1] + curr.volume)\n",
        "            if curr.close < prev.close:\n",
        "                obv.append(obv[-1] - curr.volume)\n",
        "            if curr.close == prev.close:\n",
        "                obv.append(obv[-1])\n",
        "    # Extendedd OBV\n",
        "    else:\n",
        "        # 가격 변화를 측정한다. 가격 변화 = 금일 종가 - 전일 종가\n",
        "        deltaclose = ohlcv['close'].diff(1)\n",
        "        deltaclose = deltaclose.dropna(axis = 0)\n",
        "        \n",
        "        # 가격 변화의 표준편차를 측정한다\n",
        "        stdev = np.std(deltaclose)\n",
        "        \n",
        "        for curr, prev in zip(ohlcv.itertuples(), ohlcv.shift(1).itertuples()):\n",
        "            if math.isnan(prev.close):\n",
        "                continue\n",
        "            \n",
        "            buy = curr.volume * norm.cdf((curr.close - prev.close) / stdev)\n",
        "            sell = curr.volume - buy\n",
        "            bs = abs(buy - sell)\n",
        "            \n",
        "            if curr.close > prev.close:\n",
        "                obv.append(obv[-1] + bs)\n",
        "            if curr.close < prev.close:\n",
        "                obv.append(obv[-1] - bs)\n",
        "            if curr.close == prev.close:\n",
        "                obv.append(obv[-1])\n",
        "        \n",
        "    return pd.DataFrame(obv, index=ohlcv.index)\n",
        "\n",
        "# 유동성 척도를 계산한다\n",
        "def Liquidity(ohlcv):\n",
        "    k = []\n",
        "    \n",
        "    i = 0\n",
        "    for curr in ohlcv.itertuples():\n",
        "        dp = abs(curr.high - curr.low)\n",
        "        if dp == 0:\n",
        "            if i == 0:\n",
        "                k = [np.nan]\n",
        "            else:\n",
        "                # dp = 0 이면 유동성은 매우 큰 것이지만, 계산이 불가하므로 이전의 유동성을 유지한다\n",
        "                k.append(k[-1])\n",
        "        else:\n",
        "            k.append(np.log(curr.volume) / dp)\n",
        "        i += 1\n",
        "        \n",
        "    return pd.DataFrame(k, index=ohlcv.index)\n",
        "\n",
        "# 전일 close price와 금일 close price를 이용하여 변동성을 계산한다\n",
        "def closeVol(ohlc, n):\n",
        "    rtn = pd.DataFrame(ohlc['close']).apply(lambda x: np.log(x) - np.log(x.shift(1)))\n",
        "    vol = pd.DataFrame(rtn).rolling(window=n).std()\n",
        "\n",
        "    return pd.DataFrame(vol, index=ohlc.index)\n",
        "    \n",
        "# 당일의 high price와 low price를 이용하여 Parkinson 변동성 (장 중 변동성)을 계산한다.\n",
        "def ParkinsonVol(ohlc, n):\n",
        "    vol = []\n",
        "    for i in range(n-1):\n",
        "        vol.append(np.nan)\n",
        "        \n",
        "    for i in range(n-1, len(ohlc)):\n",
        "        sigma = 0\n",
        "        for k in range(0, n):\n",
        "            sigma += np.log(ohlc.iloc[i-k].high / ohlc.iloc[i-k].low) ** 2\n",
        "        vol.append(np.sqrt(sigma / (n * 4 * np.log(2))))\n",
        "        \n",
        "    return pd.DataFrame(vol, index=ohlc.index)\n",
        "\n",
        "# Z-score normalization\n",
        "def scale(data):\n",
        "    col = data.columns[0]\n",
        "    return (data[col] - data[col].mean()) / data[col].std()\n",
        "\n",
        "# 시계열을 평활화한다\n",
        "def smooth(data, s=5):\n",
        "    y = data[data.columns[0]].values\n",
        "    w = np.isnan(y)\n",
        "    y[w] = 0.\n",
        "    sm = ndimage.gaussian_filter1d(y, s)\n",
        "    return pd.DataFrame(sm)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xku9H5f_Bb9f",
        "colab_type": "text"
      },
      "source": [
        "## UnrolledGAN.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57llH8loBefB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2b87b58-8e5e-4d78-d500-c8f606ad3241"
      },
      "source": [
        "# Unrolled GAN library - Generator가 사용할 Surrogate loss function (fk)을 생성한다.\n",
        "#\n",
        "# 원 논문 : Luke Metz, et, al., 2016, Unrolled Generative Adversarial Network\n",
        "# 아래 코드는 논문의 저자인 Google Brain 팀의 Luke Metz등이 작성한 코드의 일부임.\n",
        "# 코드 URL : https://github.com/poolio/unrolled_gan\n",
        "#\n",
        "# 이 코드를 MyUtil/UnrolledGAN에 넣고 library 처럼 활용하기로 함.\n",
        "#\n",
        "# 2018.9.15, 아마추어퀀트 (조성현)\n",
        "# ---------------------------------------------------------------------------------\n",
        "from collections import OrderedDict\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "def graph_replace(*args, **kwargs):\n",
        "    graph = tf.get_default_graph()\n",
        "    for op in graph.get_operations():\n",
        "        op._original_op = None\n",
        "    return tf.contrib.graph_editor.graph_replace(*args, **kwargs)\n",
        "\n",
        "def extract_update_dict(update_ops):\n",
        "    name_to_var = {v.name: v for v in tf.global_variables()}\n",
        "    updates = OrderedDict()\n",
        "    for update in update_ops:\n",
        "        var_name = update.op.inputs[0].name\n",
        "        var = name_to_var[var_name]\n",
        "        value = update.op.inputs[1]\n",
        "        if update.op.type == 'Assign':\n",
        "            updates[var.value()] = value\n",
        "        elif update.op.type == 'AssignAdd':\n",
        "            updates[var.value()] = var + value\n",
        "        else:\n",
        "            raise ValueError(\"Update op type (%s) must be of type Assign or AssignAdd\" % update.op.type)\n",
        "    return updates\n",
        "\n",
        "def SurrogateLoss(theta_d, loss_d, unrolled_k):\n",
        "    D_update = Adam(lr=0.0001).get_updates(theta_d, [], loss_d)\n",
        "    train_d = tf.group(*D_update, name=\"train_d\")\n",
        "    \n",
        "    if unrolled_k > 0:\n",
        "        # Get dictionary mapping from variables to their update value after one optimization step\n",
        "        update_dict = extract_update_dict(D_update)\n",
        "        cur_update_dict = update_dict\n",
        "        \n",
        "        for i in range(unrolled_k - 1):\n",
        "            # Compute variable updates given the previous iteration's updated variable\n",
        "            cur_update_dict = graph_replace(update_dict, cur_update_dict)\n",
        "            \n",
        "        # Final unrolled loss uses the parameters at the last time step\n",
        "        loss_g = graph_replace(loss_d, cur_update_dict)\n",
        "    else:\n",
        "        loss_g = loss_d\n",
        "        \n",
        "    return train_d, loss_g"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gsN2jQ5BgTr",
        "colab_type": "text"
      },
      "source": [
        "## YahooData.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nMrhiX2Bh8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 주가 데이터 수집 관련 함수를 정의한다\n",
        "#\n",
        "# 한국생산성본부 금융 빅데이터 전문가 과정 (금융 모델링 파트) 실습용 코드\n",
        "# Written : 2018.2.5\n",
        "# 제작 : 조성현\n",
        "# -----------------------------------------------------------------\n",
        "import pandas as pd\n",
        "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
        "import pandas_datareader.data as web\n",
        "import datetime as dt\n",
        "\n",
        "# Yahoo site로 부터 대형주 종목 데이터를 수집하여 파일에 저장한다.\n",
        "# Yahoo site로 부터 주가 데이터를 수집한다. 가끔 안들어올 때가 있어서 10번 시도한다.\n",
        "# 수정 주가로 환산하여 읽어온다\n",
        "def getStockDataYahoo(stockCode, start='', end=''):\n",
        "    # 수집 기간\n",
        "    if start == '':\n",
        "        start = dt.datetime(2007, 1, 1)\n",
        "    else:\n",
        "        start = dt.datetime.strptime(start, '%Y-%m-%d')\n",
        "    \n",
        "    if end == '':\n",
        "        end = dt.date.today()\n",
        "    else:\n",
        "        end = dt.datetime.strptime(end, '%Y-%m-%d')\n",
        "    \n",
        "    stock = pd.DataFrame()\n",
        "    for i in range(0, 10):\n",
        "        try:\n",
        "            stock = web.YahooDailyReader(stockCode, start, end, adjust_price=True).read()\n",
        "        except:\n",
        "            print(\"%s not collected (%d)\" % (stockCode, i + 1))\n",
        "            \n",
        "        if not stock.empty:\n",
        "            break\n",
        "        \n",
        "    if stock.empty:\n",
        "        print(\"%s not collected\" % stockCode)\n",
        "    \n",
        "    # 수정주가 비율은 이미 적용되었으므로 제거한다\n",
        "    stock = stock.drop('Adj_Ratio', 1)\n",
        "    \n",
        "    # Volume이 0 인 경우가 있으므로, 이를 제거한다 \n",
        "    stock = stock.drop(stock[stock.Volume < 10].index)\n",
        "    \n",
        "    # 데이터에 NA 값이 있으면 제거한다\n",
        "    stock = stock.dropna()\n",
        "    \n",
        "    # 수집한 데이터를 파일에 저장한다.\n",
        "    stock.to_csv('StockData/' + stockCode[0:6] + '.csv', date_format='%Y-%m-%d')\n",
        "    print (\"%s 데이터를 수집하였습니다. (rows = %d)\" % (stockCode, len(stock)))\n",
        "    return stock\n",
        "\n",
        "def getStockDataList(stockList, start='', end=''):\n",
        "    for code in stockList.keys():\n",
        "        getStockDataYahoo(code + '.KS', start=start, end=end)\n",
        "        \n",
        "# 일일 데이터를 주간 (Weekly), 혹은 월간 (Monthly)으로 변환한다\n",
        "def myAgg(x):\n",
        "    names = {\n",
        "            'Open' : x['Open'].head(1),\n",
        "            'High' : x['High'].max(),\n",
        "            'Low' : x['Low'].min(),\n",
        "            'Close' : x['Close'].tail(1),\n",
        "            'Volume' : x['Volume'].mean()}\n",
        "    return pd.Series(names, index=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "def getWeekMonthOHLC(x, type='Week'):\n",
        "    if type == 'Week':\n",
        "        rtn = x.resample('W-Fri').apply(myAgg)\n",
        "    elif type == 'Month':\n",
        "        rtn = x.resample('M').apply(myAgg)\n",
        "    else:\n",
        "        print(\"invalid type in getWeekMonthOHLC()\")\n",
        "        return\n",
        "    rtn = rtn.dropna()\n",
        "    rtn = rtn.apply(pd.to_numeric)\n",
        "    return rtn"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEMYkbP4BkP0",
        "colab_type": "text"
      },
      "source": [
        "# _1_. 단순차원축소\n",
        "\n",
        "feature의 개수를 autoencoder로 줄여 데이터로 저장하는 용도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L39X_PamB6mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# module import\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM\n",
        "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# from MyUtil import TaFeatureSet\n",
        "import pickle"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNBdAvFPEo8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = \"/content/drive/My Drive/멀티캠퍼스/[혁신성장] 인공지능 자연어처리 기반/[강의]/조성현 강사님\"\n",
        "data_path = f\"{root_path}/StockData\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g8kvNHcB8qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "889caaf9-f2f6-4325-d7cb-cfd81e82106b"
      },
      "source": [
        "# 파라미터 설정\n",
        "# n_input = int(input())\n",
        "n_step = int(input('시계열 recurrent timestep 설정: '))\n",
        "n_hidden = int(input('축소할 차원 수 설정: '))\n",
        "MASHORT = int(input('종가 단기 이동평균: '))\n",
        "MALONG = int(input('종가 장기 이동평균: '))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "시계열 recurrent timestep 설정: 20\n",
            "축소할 차원 수 설정: 3\n",
            "종가 단기 이동평균: 10\n",
            "종가 장기 이동평균: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCJMfRBzChrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM 데이터 생성\n",
        "def createData(X_data, step=n_step):\n",
        "    n_feature = X_data.shape[1] # feature 수\n",
        "    m = np.arange(X_data.shape[0] + 1) # 시계열 수\n",
        "\n",
        "    # 시계열 데이터 x\n",
        "    x = []\n",
        "    for i in m[0:(-step-5)]:\n",
        "        a = X_data[i:(i+step), :] # 20일씩 끊어서 데이터 생성\n",
        "        x.append(a)\n",
        "    X = np.reshape(np.array(x), (len(m[0:(-step-5)]), step, n_feature)) # 3차원 구조 변환\n",
        "\n",
        "    # 시계열 데이터 x에 대한 5일 후 이동평균\n",
        "    y = []\n",
        "    for i in m[0:(-step-5)]:\n",
        "        b = X_data[i+step+5-1, 4] # 5일 후 이동평균 feature 열: SHORTMA(index 4)\n",
        "        y.append(b)\n",
        "    Y = np.reshape(np.array(y), (len(m[0:(-step-5)]), 1))\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xygJ0aDPD5XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OHLC 정규화\n",
        "def normalizeOHLC(ohlc):\n",
        "    m = np.mean(ohlc.mean())\n",
        "    scale = np.mean(ohlc.std())\n",
        "    \n",
        "    rdf = pd.DataFrame((ohlc['open']-m) / scale) # rdf 열\n",
        "    rdf['high'] = (ohlc['high']-m) / scale\n",
        "    rdf['low'] = (ohlc['low']-m) / scale\n",
        "    rdf['close'] = (ohlc['close']-m) / scale\n",
        "\n",
        "    return rdf"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrEQ0OfBEYBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "95b74349-434a-4efc-8f18-0a009ce4a1d9"
      },
      "source": [
        "# 주가 데이터 로드\n",
        "df = pd.read_csv(f\"{data_path}/069500.csv\", index_col=0, parse_dates=True)[::-1] # index: DatetimeIndex\n",
        "df = df.drop(columns=['volume'], axis=1)\n",
        "df"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-02-04</th>\n",
              "      <td>18527</td>\n",
              "      <td>18527</td>\n",
              "      <td>18377</td>\n",
              "      <td>18467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-05</th>\n",
              "      <td>18009</td>\n",
              "      <td>18155</td>\n",
              "      <td>17855</td>\n",
              "      <td>17940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-08</th>\n",
              "      <td>17983</td>\n",
              "      <td>17983</td>\n",
              "      <td>17761</td>\n",
              "      <td>17790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-09</th>\n",
              "      <td>17731</td>\n",
              "      <td>17970</td>\n",
              "      <td>17731</td>\n",
              "      <td>17949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-10</th>\n",
              "      <td>18073</td>\n",
              "      <td>18090</td>\n",
              "      <td>17902</td>\n",
              "      <td>17958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>29849</td>\n",
              "      <td>30004</td>\n",
              "      <td>29485</td>\n",
              "      <td>29635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>29764</td>\n",
              "      <td>29889</td>\n",
              "      <td>29585</td>\n",
              "      <td>29734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>29695</td>\n",
              "      <td>29765</td>\n",
              "      <td>29095</td>\n",
              "      <td>29175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>29370</td>\n",
              "      <td>29485</td>\n",
              "      <td>28865</td>\n",
              "      <td>28865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>28540</td>\n",
              "      <td>28995</td>\n",
              "      <td>28400</td>\n",
              "      <td>28895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2464 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             open   high    low  close\n",
              "date                                  \n",
              "2010-02-04  18527  18527  18377  18467\n",
              "2010-02-05  18009  18155  17855  17940\n",
              "2010-02-08  17983  17983  17761  17790\n",
              "2010-02-09  17731  17970  17731  17949\n",
              "2010-02-10  18073  18090  17902  17958\n",
              "...           ...    ...    ...    ...\n",
              "2020-01-28  29849  30004  29485  29635\n",
              "2020-01-29  29764  29889  29585  29734\n",
              "2020-01-30  29695  29765  29095  29175\n",
              "2020-01-31  29370  29485  28865  28865\n",
              "2020-02-03  28540  28995  28400  28895\n",
              "\n",
              "[2464 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLtqiizeG_Nv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "82f526b9-239d-4b9a-964c-5037583be6b3"
      },
      "source": [
        "# OHLC 정규화\n",
        "ndf = normalizeOHLC(df)\n",
        "ndf"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-02-04</th>\n",
              "      <td>-1.886842</td>\n",
              "      <td>-1.886842</td>\n",
              "      <td>-1.934354</td>\n",
              "      <td>-1.905847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-05</th>\n",
              "      <td>-2.050919</td>\n",
              "      <td>-2.004673</td>\n",
              "      <td>-2.099698</td>\n",
              "      <td>-2.072774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-08</th>\n",
              "      <td>-2.059154</td>\n",
              "      <td>-2.059154</td>\n",
              "      <td>-2.129473</td>\n",
              "      <td>-2.120287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-09</th>\n",
              "      <td>-2.138975</td>\n",
              "      <td>-2.063272</td>\n",
              "      <td>-2.138975</td>\n",
              "      <td>-2.069924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-02-10</th>\n",
              "      <td>-2.030647</td>\n",
              "      <td>-2.025262</td>\n",
              "      <td>-2.084811</td>\n",
              "      <td>-2.067073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>1.699414</td>\n",
              "      <td>1.748510</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.631629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>1.672490</td>\n",
              "      <td>1.712084</td>\n",
              "      <td>1.615792</td>\n",
              "      <td>1.662987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>1.650634</td>\n",
              "      <td>1.672807</td>\n",
              "      <td>1.460584</td>\n",
              "      <td>1.485924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>1.547690</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.387731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>1.284787</td>\n",
              "      <td>1.428908</td>\n",
              "      <td>1.240442</td>\n",
              "      <td>1.397233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2464 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                open      high       low     close\n",
              "date                                              \n",
              "2010-02-04 -1.886842 -1.886842 -1.934354 -1.905847\n",
              "2010-02-05 -2.050919 -2.004673 -2.099698 -2.072774\n",
              "2010-02-08 -2.059154 -2.059154 -2.129473 -2.120287\n",
              "2010-02-09 -2.138975 -2.063272 -2.138975 -2.069924\n",
              "2010-02-10 -2.030647 -2.025262 -2.084811 -2.067073\n",
              "...              ...       ...       ...       ...\n",
              "2020-01-28  1.699414  1.748510  1.584116  1.631629\n",
              "2020-01-29  1.672490  1.712084  1.615792  1.662987\n",
              "2020-01-30  1.650634  1.672807  1.460584  1.485924\n",
              "2020-01-31  1.547690  1.584116  1.387731  1.387731\n",
              "2020-02-03  1.284787  1.428908  1.240442  1.397233\n",
              "\n",
              "[2464 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F435FsLFfXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "231c2f25-0a94-4ebc-a953-c529860f0771"
      },
      "source": [
        "# 이동평균 계산: dataframe으로 만든 뒤 rolling 적용\n",
        "ndf['maShort'] = pd.DataFrame(df['close']).rolling(window=MASHORT).mean()\n",
        "ndf['maLong'] = pd.DataFrame(df['close']).rolling(window=MALONG).mean()\n",
        "ndf['macd'] = MACD(df)\n",
        "ndf['rsi'] = RSI(df)\n",
        "ndf = ndf.dropna()\n",
        "ndf"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>maShort</th>\n",
              "      <th>maLong</th>\n",
              "      <th>macd</th>\n",
              "      <th>rsi</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-05-31</th>\n",
              "      <td>-1.905847</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>-1.907114</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>18395.7</td>\n",
              "      <td>19310.550</td>\n",
              "      <td>0.005796</td>\n",
              "      <td>43.648208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-01</th>\n",
              "      <td>-1.853583</td>\n",
              "      <td>-1.834261</td>\n",
              "      <td>-1.880823</td>\n",
              "      <td>-1.864352</td>\n",
              "      <td>18376.7</td>\n",
              "      <td>19282.125</td>\n",
              "      <td>0.131903</td>\n",
              "      <td>43.776544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-03</th>\n",
              "      <td>-1.820641</td>\n",
              "      <td>-1.725615</td>\n",
              "      <td>-1.835528</td>\n",
              "      <td>-1.731317</td>\n",
              "      <td>18406.2</td>\n",
              "      <td>19261.625</td>\n",
              "      <td>0.365303</td>\n",
              "      <td>51.446945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-04</th>\n",
              "      <td>-1.731317</td>\n",
              "      <td>-1.718647</td>\n",
              "      <td>-1.754440</td>\n",
              "      <td>-1.722765</td>\n",
              "      <td>18457.9</td>\n",
              "      <td>19241.600</td>\n",
              "      <td>0.518051</td>\n",
              "      <td>45.683453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-07</th>\n",
              "      <td>-1.858968</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>-1.889376</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>18507.8</td>\n",
              "      <td>19214.825</td>\n",
              "      <td>0.498324</td>\n",
              "      <td>42.256503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>1.699414</td>\n",
              "      <td>1.748510</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.631629</td>\n",
              "      <td>30503.0</td>\n",
              "      <td>29229.575</td>\n",
              "      <td>-0.201184</td>\n",
              "      <td>54.953765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>1.672490</td>\n",
              "      <td>1.712084</td>\n",
              "      <td>1.615792</td>\n",
              "      <td>1.662987</td>\n",
              "      <td>30443.1</td>\n",
              "      <td>29266.375</td>\n",
              "      <td>-0.360591</td>\n",
              "      <td>52.922227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>1.650634</td>\n",
              "      <td>1.672807</td>\n",
              "      <td>1.460584</td>\n",
              "      <td>1.485924</td>\n",
              "      <td>30312.8</td>\n",
              "      <td>29292.325</td>\n",
              "      <td>-0.574323</td>\n",
              "      <td>48.780488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>1.547690</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>30169.5</td>\n",
              "      <td>29322.625</td>\n",
              "      <td>-0.757673</td>\n",
              "      <td>39.925044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>1.284787</td>\n",
              "      <td>1.428908</td>\n",
              "      <td>1.240442</td>\n",
              "      <td>1.397233</td>\n",
              "      <td>29998.2</td>\n",
              "      <td>29351.425</td>\n",
              "      <td>-0.838323</td>\n",
              "      <td>36.935894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2386 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                open      high       low  ...     maLong      macd        rsi\n",
              "date                                      ...                                \n",
              "2010-05-31 -1.905847 -1.831410 -1.907114  ...  19310.550  0.005796  43.648208\n",
              "2010-06-01 -1.853583 -1.834261 -1.880823  ...  19282.125  0.131903  43.776544\n",
              "2010-06-03 -1.820641 -1.725615 -1.835528  ...  19261.625  0.365303  51.446945\n",
              "2010-06-04 -1.731317 -1.718647 -1.754440  ...  19241.600  0.518051  45.683453\n",
              "2010-06-07 -1.858968 -1.819057 -1.889376  ...  19214.825  0.498324  42.256503\n",
              "...              ...       ...       ...  ...        ...       ...        ...\n",
              "2020-01-28  1.699414  1.748510  1.584116  ...  29229.575 -0.201184  54.953765\n",
              "2020-01-29  1.672490  1.712084  1.615792  ...  29266.375 -0.360591  52.922227\n",
              "2020-01-30  1.650634  1.672807  1.460584  ...  29292.325 -0.574323  48.780488\n",
              "2020-01-31  1.547690  1.584116  1.387731  ...  29322.625 -0.757673  39.925044\n",
              "2020-02-03  1.284787  1.428908  1.240442  ...  29351.425 -0.838323  36.935894\n",
              "\n",
              "[2386 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azK6BzcZG6sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "outputId": "6215c3b8-3799-49c3-c310-f2cc4974f278"
      },
      "source": [
        "# 정규화\n",
        "ndf['maShort'] = (ndf['maShort'] - ndf['maShort'].mean()) / ndf['maShort'].std()\n",
        "ndf['maLong'] = (ndf['maLong'] - ndf['maLong'].mean()) / ndf['maLong'].std()\n",
        "ndf['macd'] = (ndf['macd'] - ndf['macd'].mean()) / ndf['macd'].std()\n",
        "ndf['rsi'] = (ndf['rsi'] - ndf['rsi'].mean()) / ndf['rsi'].std()\n",
        "ndf"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>maShort</th>\n",
              "      <th>maLong</th>\n",
              "      <th>macd</th>\n",
              "      <th>rsi</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-05-31</th>\n",
              "      <td>-1.905847</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>-1.907114</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>-2.058362</td>\n",
              "      <td>-1.758238</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>-0.544542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-01</th>\n",
              "      <td>-1.853583</td>\n",
              "      <td>-1.834261</td>\n",
              "      <td>-1.880823</td>\n",
              "      <td>-1.864352</td>\n",
              "      <td>-2.064626</td>\n",
              "      <td>-1.767735</td>\n",
              "      <td>0.389674</td>\n",
              "      <td>-0.537252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-03</th>\n",
              "      <td>-1.820641</td>\n",
              "      <td>-1.725615</td>\n",
              "      <td>-1.835528</td>\n",
              "      <td>-1.731317</td>\n",
              "      <td>-2.054900</td>\n",
              "      <td>-1.774585</td>\n",
              "      <td>1.105865</td>\n",
              "      <td>-0.101499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-04</th>\n",
              "      <td>-1.731317</td>\n",
              "      <td>-1.718647</td>\n",
              "      <td>-1.754440</td>\n",
              "      <td>-1.722765</td>\n",
              "      <td>-2.037856</td>\n",
              "      <td>-1.781276</td>\n",
              "      <td>1.574574</td>\n",
              "      <td>-0.428921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-07</th>\n",
              "      <td>-1.858968</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>-1.889376</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>-2.021405</td>\n",
              "      <td>-1.790222</td>\n",
              "      <td>1.514041</td>\n",
              "      <td>-0.623605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>1.699414</td>\n",
              "      <td>1.748510</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.631629</td>\n",
              "      <td>1.933135</td>\n",
              "      <td>1.555955</td>\n",
              "      <td>-0.632407</td>\n",
              "      <td>0.097722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>1.672490</td>\n",
              "      <td>1.712084</td>\n",
              "      <td>1.615792</td>\n",
              "      <td>1.662987</td>\n",
              "      <td>1.913387</td>\n",
              "      <td>1.568251</td>\n",
              "      <td>-1.121550</td>\n",
              "      <td>-0.017689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>1.650634</td>\n",
              "      <td>1.672807</td>\n",
              "      <td>1.460584</td>\n",
              "      <td>1.485924</td>\n",
              "      <td>1.870431</td>\n",
              "      <td>1.576921</td>\n",
              "      <td>-1.777387</td>\n",
              "      <td>-0.252980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>1.547690</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.823188</td>\n",
              "      <td>1.587045</td>\n",
              "      <td>-2.340000</td>\n",
              "      <td>-0.756054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>1.284787</td>\n",
              "      <td>1.428908</td>\n",
              "      <td>1.240442</td>\n",
              "      <td>1.397233</td>\n",
              "      <td>1.766714</td>\n",
              "      <td>1.596668</td>\n",
              "      <td>-2.587475</td>\n",
              "      <td>-0.925866</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2386 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                open      high       low  ...    maLong      macd       rsi\n",
              "date                                      ...                              \n",
              "2010-05-31 -1.905847 -1.831410 -1.907114  ... -1.758238  0.002714 -0.544542\n",
              "2010-06-01 -1.853583 -1.834261 -1.880823  ... -1.767735  0.389674 -0.537252\n",
              "2010-06-03 -1.820641 -1.725615 -1.835528  ... -1.774585  1.105865 -0.101499\n",
              "2010-06-04 -1.731317 -1.718647 -1.754440  ... -1.781276  1.574574 -0.428921\n",
              "2010-06-07 -1.858968 -1.819057 -1.889376  ... -1.790222  1.514041 -0.623605\n",
              "...              ...       ...       ...  ...       ...       ...       ...\n",
              "2020-01-28  1.699414  1.748510  1.584116  ...  1.555955 -0.632407  0.097722\n",
              "2020-01-29  1.672490  1.712084  1.615792  ...  1.568251 -1.121550 -0.017689\n",
              "2020-01-30  1.650634  1.672807  1.460584  ...  1.576921 -1.777387 -0.252980\n",
              "2020-01-31  1.547690  1.584116  1.387731  ...  1.587045 -2.340000 -0.756054\n",
              "2020-02-03  1.284787  1.428908  1.240442  ...  1.596668 -2.587475 -0.925866\n",
              "\n",
              "[2386 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsif9eIjHak5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c62a46a2-73b2-45ae-dd85-6076041994d4"
      },
      "source": [
        "# 학습 데이터 생성\n",
        "data = np.array(ndf)\n",
        "print(data.shape)\n",
        "X_train, y_train = createData(data, n_step)\n",
        "print(f\"X: {X_train.shape}, y: {y_train.shape}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2386, 8)\n",
            "X: (2362, 20, 8), y: (2362, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUd3k8mcHdzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffbe7d9d-6eac-4dd3-c56e-47d0bb1e66e3"
      },
      "source": [
        "# LSTM 오토인코더 모델 구성\n",
        "\n",
        "X_input = Input(batch_shape=(None, X_train.shape[1], X_train.shape[2])) # Input\n",
        "X_encoder = Bidirectional(LSTM(n_hidden, return_sequences=True), merge_mode='sum')(X_input) # 인코더\n",
        "y_decoder = Bidirectional(LSTM(n_hidden, return_sequences=True), merge_mode='sum')(X_encoder) # 디코더\n",
        "y_output = TimeDistributed(Dense(X_train.shape[2]))(y_decoder) # 출력 : 출력 feature 뽑아내기\n",
        "\n",
        "# 모델\n",
        "model = Model(X_input, y_output)\n",
        "print(\"=========== 모델 전체 구조 ===========\")\n",
        "print(model.summary())\n",
        "print()\n",
        "model_encoder = Model(X_input, X_encoder)\n",
        "print(\"=========== 인코더 모델 구조 ===========\")\n",
        "print(model_encoder.summary())\n",
        "print()\n",
        "\n",
        "# 모델 학습\n",
        "EPOCHS = int(input('학습 에폭 수 설정: '))\n",
        "BATCH = int(input('배치 사이즈 설정: '))\n",
        "model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
        "hist = model.fit(X_train, X_train,\n",
        "                 epochs=EPOCHS,\n",
        "                 batch_size=BATCH)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== 모델 전체 구조 ===========\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20, 8)]           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 3)             288       \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 20, 3)             168       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 20, 8)             32        \n",
            "=================================================================\n",
            "Total params: 488\n",
            "Trainable params: 488\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "=========== 인코더 모델 구조 ===========\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20, 8)]           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 3)             288       \n",
            "=================================================================\n",
            "Total params: 288\n",
            "Trainable params: 288\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "학습 에폭 수 설정: 300\n",
            "배치 사이즈 설정: 300\n",
            "Epoch 1/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.9610\n",
            "Epoch 2/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.9499\n",
            "Epoch 3/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.9421\n",
            "Epoch 4/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.9350\n",
            "Epoch 5/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.9273\n",
            "Epoch 6/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.9178\n",
            "Epoch 7/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.9060\n",
            "Epoch 8/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.8920\n",
            "Epoch 9/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.8754\n",
            "Epoch 10/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.8564\n",
            "Epoch 11/300\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.8348\n",
            "Epoch 12/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.8115\n",
            "Epoch 13/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.7866\n",
            "Epoch 14/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.7600\n",
            "Epoch 15/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.7333\n",
            "Epoch 16/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.7061\n",
            "Epoch 17/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.6798\n",
            "Epoch 18/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.6549\n",
            "Epoch 19/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.6307\n",
            "Epoch 20/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.6084\n",
            "Epoch 21/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.5874\n",
            "Epoch 22/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.5672\n",
            "Epoch 23/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.5483\n",
            "Epoch 24/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.5302\n",
            "Epoch 25/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.5132\n",
            "Epoch 26/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4972\n",
            "Epoch 27/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4816\n",
            "Epoch 28/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4673\n",
            "Epoch 29/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4537\n",
            "Epoch 30/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4410\n",
            "Epoch 31/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4291\n",
            "Epoch 32/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4178\n",
            "Epoch 33/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4070\n",
            "Epoch 34/300\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3972\n",
            "Epoch 35/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3877\n",
            "Epoch 36/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3788\n",
            "Epoch 37/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3704\n",
            "Epoch 38/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3625\n",
            "Epoch 39/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3551\n",
            "Epoch 40/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3481\n",
            "Epoch 41/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3415\n",
            "Epoch 42/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3354\n",
            "Epoch 43/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3295\n",
            "Epoch 44/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3242\n",
            "Epoch 45/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3191\n",
            "Epoch 46/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3143\n",
            "Epoch 47/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3098\n",
            "Epoch 48/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3055\n",
            "Epoch 49/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3015\n",
            "Epoch 50/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2976\n",
            "Epoch 51/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2938\n",
            "Epoch 52/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2903\n",
            "Epoch 53/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2869\n",
            "Epoch 54/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2835\n",
            "Epoch 55/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2802\n",
            "Epoch 56/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2770\n",
            "Epoch 57/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2737\n",
            "Epoch 58/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2705\n",
            "Epoch 59/300\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2672\n",
            "Epoch 60/300\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2639\n",
            "Epoch 61/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2606\n",
            "Epoch 62/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2572\n",
            "Epoch 63/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2537\n",
            "Epoch 64/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2502\n",
            "Epoch 65/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2466\n",
            "Epoch 66/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2429\n",
            "Epoch 67/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2393\n",
            "Epoch 68/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2356\n",
            "Epoch 69/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2319\n",
            "Epoch 70/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2282\n",
            "Epoch 71/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2246\n",
            "Epoch 72/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2210\n",
            "Epoch 73/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2174\n",
            "Epoch 74/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2139\n",
            "Epoch 75/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2103\n",
            "Epoch 76/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2069\n",
            "Epoch 77/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2034\n",
            "Epoch 78/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2000\n",
            "Epoch 79/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1966\n",
            "Epoch 80/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1932\n",
            "Epoch 81/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1898\n",
            "Epoch 82/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1865\n",
            "Epoch 83/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1832\n",
            "Epoch 84/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1798\n",
            "Epoch 85/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1765\n",
            "Epoch 86/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1733\n",
            "Epoch 87/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1701\n",
            "Epoch 88/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1668\n",
            "Epoch 89/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1637\n",
            "Epoch 90/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1605\n",
            "Epoch 91/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1574\n",
            "Epoch 92/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1543\n",
            "Epoch 93/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1513\n",
            "Epoch 94/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1482\n",
            "Epoch 95/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1453\n",
            "Epoch 96/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1423\n",
            "Epoch 97/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1394\n",
            "Epoch 98/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1366\n",
            "Epoch 99/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1338\n",
            "Epoch 100/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1311\n",
            "Epoch 101/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1284\n",
            "Epoch 102/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1257\n",
            "Epoch 103/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1231\n",
            "Epoch 104/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.1206\n",
            "Epoch 105/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.1181\n",
            "Epoch 106/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.1157\n",
            "Epoch 107/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1134\n",
            "Epoch 108/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.1110\n",
            "Epoch 109/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1088\n",
            "Epoch 110/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1066\n",
            "Epoch 111/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.1045\n",
            "Epoch 112/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1024\n",
            "Epoch 113/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1004\n",
            "Epoch 114/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0985\n",
            "Epoch 115/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0966\n",
            "Epoch 116/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0948\n",
            "Epoch 117/300\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.0930\n",
            "Epoch 118/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0912\n",
            "Epoch 119/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0896\n",
            "Epoch 120/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0879\n",
            "Epoch 121/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0864\n",
            "Epoch 122/300\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0848\n",
            "Epoch 123/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0833\n",
            "Epoch 124/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0819\n",
            "Epoch 125/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0805\n",
            "Epoch 126/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0792\n",
            "Epoch 127/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0778\n",
            "Epoch 128/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0766\n",
            "Epoch 129/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0753\n",
            "Epoch 130/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0741\n",
            "Epoch 131/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0729\n",
            "Epoch 132/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0718\n",
            "Epoch 133/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0707\n",
            "Epoch 134/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0696\n",
            "Epoch 135/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0686\n",
            "Epoch 136/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0676\n",
            "Epoch 137/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0666\n",
            "Epoch 138/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0656\n",
            "Epoch 139/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0647\n",
            "Epoch 140/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0638\n",
            "Epoch 141/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0629\n",
            "Epoch 142/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0621\n",
            "Epoch 143/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0612\n",
            "Epoch 144/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0604\n",
            "Epoch 145/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0597\n",
            "Epoch 146/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0589\n",
            "Epoch 147/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0582\n",
            "Epoch 148/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0574\n",
            "Epoch 149/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0568\n",
            "Epoch 150/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0561\n",
            "Epoch 151/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0554\n",
            "Epoch 152/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0547\n",
            "Epoch 153/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0541\n",
            "Epoch 154/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0535\n",
            "Epoch 155/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0529\n",
            "Epoch 156/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0523\n",
            "Epoch 157/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0517\n",
            "Epoch 158/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0511\n",
            "Epoch 159/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0506\n",
            "Epoch 160/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0500\n",
            "Epoch 161/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0495\n",
            "Epoch 162/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0490\n",
            "Epoch 163/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0485\n",
            "Epoch 164/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0479\n",
            "Epoch 165/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0474\n",
            "Epoch 166/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0470\n",
            "Epoch 167/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0465\n",
            "Epoch 168/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0460\n",
            "Epoch 169/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0455\n",
            "Epoch 170/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0450\n",
            "Epoch 171/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0446\n",
            "Epoch 172/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0441\n",
            "Epoch 173/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0437\n",
            "Epoch 174/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0433\n",
            "Epoch 175/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0428\n",
            "Epoch 176/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0424\n",
            "Epoch 177/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0420\n",
            "Epoch 178/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0415\n",
            "Epoch 179/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0411\n",
            "Epoch 180/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0407\n",
            "Epoch 181/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0403\n",
            "Epoch 182/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0399\n",
            "Epoch 183/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0395\n",
            "Epoch 184/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0391\n",
            "Epoch 185/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0387\n",
            "Epoch 186/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0383\n",
            "Epoch 187/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0379\n",
            "Epoch 188/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0376\n",
            "Epoch 189/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0372\n",
            "Epoch 190/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0368\n",
            "Epoch 191/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0364\n",
            "Epoch 192/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0361\n",
            "Epoch 193/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0357\n",
            "Epoch 194/300\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0353\n",
            "Epoch 195/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0350\n",
            "Epoch 196/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0346\n",
            "Epoch 197/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0343\n",
            "Epoch 198/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0339\n",
            "Epoch 199/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0336\n",
            "Epoch 200/300\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 0.0332\n",
            "Epoch 201/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0329\n",
            "Epoch 202/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0326\n",
            "Epoch 203/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0322\n",
            "Epoch 204/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0319\n",
            "Epoch 205/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0316\n",
            "Epoch 206/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0313\n",
            "Epoch 207/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0309\n",
            "Epoch 208/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0306\n",
            "Epoch 209/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0303\n",
            "Epoch 210/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0300\n",
            "Epoch 211/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0297\n",
            "Epoch 212/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0294\n",
            "Epoch 213/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0291\n",
            "Epoch 214/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0288\n",
            "Epoch 215/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0285\n",
            "Epoch 216/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0282\n",
            "Epoch 217/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0280\n",
            "Epoch 218/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0277\n",
            "Epoch 219/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0274\n",
            "Epoch 220/300\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0271\n",
            "Epoch 221/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0269\n",
            "Epoch 222/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0266\n",
            "Epoch 223/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0263\n",
            "Epoch 224/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0261\n",
            "Epoch 225/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0258\n",
            "Epoch 226/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0256\n",
            "Epoch 227/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0253\n",
            "Epoch 228/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0251\n",
            "Epoch 229/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0248\n",
            "Epoch 230/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0246\n",
            "Epoch 231/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0244\n",
            "Epoch 232/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0241\n",
            "Epoch 233/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0239\n",
            "Epoch 234/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0237\n",
            "Epoch 235/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0235\n",
            "Epoch 236/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0233\n",
            "Epoch 237/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0230\n",
            "Epoch 238/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0228\n",
            "Epoch 239/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0226\n",
            "Epoch 240/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0224\n",
            "Epoch 241/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0222\n",
            "Epoch 242/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0220\n",
            "Epoch 243/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0218\n",
            "Epoch 244/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0216\n",
            "Epoch 245/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0214\n",
            "Epoch 246/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0212\n",
            "Epoch 247/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0210\n",
            "Epoch 248/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0209\n",
            "Epoch 249/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0207\n",
            "Epoch 250/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0205\n",
            "Epoch 251/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0203\n",
            "Epoch 252/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0201\n",
            "Epoch 253/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0200\n",
            "Epoch 254/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0198\n",
            "Epoch 255/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0196\n",
            "Epoch 256/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0195\n",
            "Epoch 257/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0193\n",
            "Epoch 258/300\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.0192\n",
            "Epoch 259/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0190\n",
            "Epoch 260/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0188\n",
            "Epoch 261/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0187\n",
            "Epoch 262/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0185\n",
            "Epoch 263/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0184\n",
            "Epoch 264/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0182\n",
            "Epoch 265/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0181\n",
            "Epoch 266/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0180\n",
            "Epoch 267/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0178\n",
            "Epoch 268/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0177\n",
            "Epoch 269/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0175\n",
            "Epoch 270/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0174\n",
            "Epoch 271/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0173\n",
            "Epoch 272/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0171\n",
            "Epoch 273/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0170\n",
            "Epoch 274/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0169\n",
            "Epoch 275/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0168\n",
            "Epoch 276/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0166\n",
            "Epoch 277/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0165\n",
            "Epoch 278/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0164\n",
            "Epoch 279/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0163\n",
            "Epoch 280/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0161\n",
            "Epoch 281/300\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0160\n",
            "Epoch 282/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0159\n",
            "Epoch 283/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0158\n",
            "Epoch 284/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0157\n",
            "Epoch 285/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0156\n",
            "Epoch 286/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0155\n",
            "Epoch 287/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0153\n",
            "Epoch 288/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0152\n",
            "Epoch 289/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0151\n",
            "Epoch 290/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0150\n",
            "Epoch 291/300\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0149\n",
            "Epoch 292/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0148\n",
            "Epoch 293/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0147\n",
            "Epoch 294/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0146\n",
            "Epoch 295/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0145\n",
            "Epoch 296/300\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0144\n",
            "Epoch 297/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0143\n",
            "Epoch 298/300\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.0142\n",
            "Epoch 299/300\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0141\n",
            "Epoch 300/300\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq7u9JpdJEaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8133890f-4761-434e-9ca8-abcbf9606525"
      },
      "source": [
        "# plot loss\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.title('Loss Trajectory')\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwgYCQBXJMpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7dd1bef4-57cf-4bf4-deb9-9cdc334657dc"
      },
      "source": [
        "# 학습 데이터 차원을 줄이고 확인한다\n",
        "X_train_encoded = model_encoder.predict(X_train)\n",
        "print(X_train_encoded.shape) # shape 축소되었다.\n",
        "for i in range(5):\n",
        "    print(X_train[i], X_train_encoded[i])\n",
        "    print()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2362, 20, 3)\n",
            "[[-1.90584668 -1.83141018 -1.90711368 -1.83141018 -2.05836195 -1.75823767\n",
            "   0.00271388 -0.54454236]\n",
            " [-1.85358276 -1.83426094 -1.88082335 -1.86435229 -2.06462581 -1.76773517\n",
            "   0.38967395 -0.5372517 ]\n",
            " [-1.82064065 -1.72561533 -1.83552795 -1.73131685 -2.05490034 -1.77458473\n",
            "   1.1058655  -0.10149927]\n",
            " [-1.73131685 -1.71864681 -1.75443968 -1.72276457 -2.03785604 -1.78127558\n",
            "   1.57457442 -0.42892093]\n",
            " [-1.85896752 -1.81905689 -1.88937563 -1.81905689 -2.02140517 -1.79022177\n",
            "   1.514041   -0.62360457]\n",
            " [-1.82064065 -1.75855744 -1.85611676 -1.77249449 -2.00409713 -1.79833265\n",
            "   1.59319651 -0.08863674]\n",
            " [-1.77091073 -1.76964373 -1.83014318 -1.8067036  -1.97419543 -1.80659389\n",
            "   1.48566798 -0.13028251]\n",
            " [-1.79023255 -1.75443968 -1.81905689 -1.78199702 -1.9451509  -1.81288379\n",
            "   1.45751764  0.13797445]\n",
            " [-1.70914428 -1.68317069 -1.74493714 -1.69140622 -1.91683165 -1.81649234\n",
            "   1.69369119  0.69554289]\n",
            " [-1.66669963 -1.5951139  -1.6720844  -1.63344077 -1.88877615 -1.82104479\n",
            "   1.97049143  0.736402  ]\n",
            " [-1.63502453 -1.60461643 -1.65846411 -1.62520525 -1.86731419 -1.82594807\n",
            "   2.09004454  1.48983144]\n",
            " [-1.57579208 -1.56343879 -1.60873419 -1.57325807 -1.83701688 -1.82844565\n",
            "   2.24199636  1.51705419]\n",
            " [-1.57452508 -1.56058803 -1.5951139  -1.57864284 -1.82112645 -1.82826188\n",
            "   2.21427707  1.32931348]\n",
            " [-1.57864284 -1.56058803 -1.5951139  -1.56343879 -1.8045437  -1.82867954\n",
            "   2.13728377  1.24052595]\n",
            " [-1.51529263 -1.44940841 -1.51941039 -1.47949976 -1.76920234 -1.82948979\n",
            "   2.24908946  1.26589752]\n",
            " [-1.52099415 -1.48646828 -1.52352816 -1.49058605 -1.7398611  -1.8300578\n",
            "   2.1616766   1.40632224]\n",
            " [-1.53999921 -1.49755457 -1.53999921 -1.52922968 -1.7109814  -1.83143607\n",
            "   1.86221089  0.81753024]\n",
            " [-1.52922968 -1.46587947 -1.54284997 -1.48646828 -1.68022255 -1.83343246\n",
            "   1.70152203  0.92109593]\n",
            " [-1.54158297 -1.48805204 -1.56217179 -1.54570073 -1.66505741 -1.83684889\n",
            "   1.30790725  1.17739644]\n",
            " [-1.53461444 -1.50452309 -1.54570073 -1.52099415 -1.65335388 -1.83743361\n",
            "   1.05329261  1.11846025]] [[-0.3521402  -0.12966114 -1.5845938 ]\n",
            " [-0.4922347   0.07156649 -1.7783151 ]\n",
            " [-0.57715786  0.3372574  -1.8452299 ]\n",
            " [-0.83569205  0.35480547 -1.8887198 ]\n",
            " [-0.932207    0.29086986 -1.8908541 ]\n",
            " [-0.7681508   0.4179287  -1.874083  ]\n",
            " [-0.7690208   0.40564752 -1.871138  ]\n",
            " [-0.68018323  0.4614892  -1.8540821 ]\n",
            " [-0.54442424  0.6131968  -1.8389133 ]\n",
            " [-0.5900659   0.67510176 -1.8524272 ]\n",
            " [-0.42495155  0.7806265  -1.7992055 ]\n",
            " [-0.4353094   0.81122684 -1.8054175 ]\n",
            " [-0.49316406  0.7914352  -1.8022525 ]\n",
            " [-0.51114845  0.7704005  -1.7855282 ]\n",
            " [-0.53968996  0.7880661  -1.7744008 ]\n",
            " [-0.4991159   0.7909829  -1.738506  ]\n",
            " [-0.6160876   0.67463946 -1.7443626 ]\n",
            " [-0.56305873  0.6517943  -1.6941524 ]\n",
            " [-0.4137404   0.61917925 -1.5703897 ]\n",
            " [-0.31040585  0.6011925  -1.3695598 ]]\n",
            "\n",
            "[[-1.85358276 -1.83426094 -1.88082335 -1.86435229 -2.06462581 -1.76773517\n",
            "   0.38967395 -0.5372517 ]\n",
            " [-1.82064065 -1.72561533 -1.83552795 -1.73131685 -2.05490034 -1.77458473\n",
            "   1.1058655  -0.10149927]\n",
            " [-1.73131685 -1.71864681 -1.75443968 -1.72276457 -2.03785604 -1.78127558\n",
            "   1.57457442 -0.42892093]\n",
            " [-1.85896752 -1.81905689 -1.88937563 -1.81905689 -2.02140517 -1.79022177\n",
            "   1.514041   -0.62360457]\n",
            " [-1.82064065 -1.75855744 -1.85611676 -1.77249449 -2.00409713 -1.79833265\n",
            "   1.59319651 -0.08863674]\n",
            " [-1.77091073 -1.76964373 -1.83014318 -1.8067036  -1.97419543 -1.80659389\n",
            "   1.48566798 -0.13028251]\n",
            " [-1.79023255 -1.75443968 -1.81905689 -1.78199702 -1.9451509  -1.81288379\n",
            "   1.45751764  0.13797445]\n",
            " [-1.70914428 -1.68317069 -1.74493714 -1.69140622 -1.91683165 -1.81649234\n",
            "   1.69369119  0.69554289]\n",
            " [-1.66669963 -1.5951139  -1.6720844  -1.63344077 -1.88877615 -1.82104479\n",
            "   1.97049143  0.736402  ]\n",
            " [-1.63502453 -1.60461643 -1.65846411 -1.62520525 -1.86731419 -1.82594807\n",
            "   2.09004454  1.48983144]\n",
            " [-1.57579208 -1.56343879 -1.60873419 -1.57325807 -1.83701688 -1.82844565\n",
            "   2.24199636  1.51705419]\n",
            " [-1.57452508 -1.56058803 -1.5951139  -1.57864284 -1.82112645 -1.82826188\n",
            "   2.21427707  1.32931348]\n",
            " [-1.57864284 -1.56058803 -1.5951139  -1.56343879 -1.8045437  -1.82867954\n",
            "   2.13728377  1.24052595]\n",
            " [-1.51529263 -1.44940841 -1.51941039 -1.47949976 -1.76920234 -1.82948979\n",
            "   2.24908946  1.26589752]\n",
            " [-1.52099415 -1.48646828 -1.52352816 -1.49058605 -1.7398611  -1.8300578\n",
            "   2.1616766   1.40632224]\n",
            " [-1.53999921 -1.49755457 -1.53999921 -1.52922968 -1.7109814  -1.83143607\n",
            "   1.86221089  0.81753024]\n",
            " [-1.52922968 -1.46587947 -1.54284997 -1.48646828 -1.68022255 -1.83343246\n",
            "   1.70152203  0.92109593]\n",
            " [-1.54158297 -1.48805204 -1.56217179 -1.54570073 -1.66505741 -1.83684889\n",
            "   1.30790725  1.17739644]\n",
            " [-1.53461444 -1.50452309 -1.54570073 -1.52099415 -1.65335388 -1.83743361\n",
            "   1.05329261  1.11846025]\n",
            " [-1.52099415 -1.47949976 -1.60208242 -1.5963809  -1.65035382 -1.8400732\n",
            "   0.57930836  0.82923192]] [[-0.37989542 -0.05257228 -1.6125695 ]\n",
            " [-0.4727217   0.30898902 -1.8114021 ]\n",
            " [-0.7912197   0.35378864 -1.8788582 ]\n",
            " [-0.9190444   0.2885219  -1.8855529 ]\n",
            " [-0.76285857  0.4141433  -1.8705223 ]\n",
            " [-0.765723    0.4028652  -1.8688905 ]\n",
            " [-0.6781974   0.4595923  -1.8527106 ]\n",
            " [-0.5432484   0.6118332  -1.8382342 ]\n",
            " [-0.5893185   0.67419183 -1.8523765 ]\n",
            " [-0.42456207  0.7800671  -1.7999167 ]\n",
            " [-0.43518293  0.8109127  -1.8070586 ]\n",
            " [-0.4933722   0.791294   -1.8049436 ]\n",
            " [-0.5118618   0.7703921  -1.7898002 ]\n",
            " [-0.5411693   0.78834784 -1.7814382 ]\n",
            " [-0.5012942   0.79170036 -1.7492424 ]\n",
            " [-0.622133    0.6758566  -1.7594812 ]\n",
            " [-0.5756999   0.6546512  -1.7200141 ]\n",
            " [-0.4419523   0.6192768  -1.6266863 ]\n",
            " [-0.37222546  0.5696545  -1.5560517 ]\n",
            " [-0.31053     0.47754326 -1.341145  ]]\n",
            "\n",
            "[[-1.82064065 -1.72561533 -1.83552795 -1.73131685 -2.05490034 -1.77458473\n",
            "   1.1058655  -0.10149927]\n",
            " [-1.73131685 -1.71864681 -1.75443968 -1.72276457 -2.03785604 -1.78127558\n",
            "   1.57457442 -0.42892093]\n",
            " [-1.85896752 -1.81905689 -1.88937563 -1.81905689 -2.02140517 -1.79022177\n",
            "   1.514041   -0.62360457]\n",
            " [-1.82064065 -1.75855744 -1.85611676 -1.77249449 -2.00409713 -1.79833265\n",
            "   1.59319651 -0.08863674]\n",
            " [-1.77091073 -1.76964373 -1.83014318 -1.8067036  -1.97419543 -1.80659389\n",
            "   1.48566798 -0.13028251]\n",
            " [-1.79023255 -1.75443968 -1.81905689 -1.78199702 -1.9451509  -1.81288379\n",
            "   1.45751764  0.13797445]\n",
            " [-1.70914428 -1.68317069 -1.74493714 -1.69140622 -1.91683165 -1.81649234\n",
            "   1.69369119  0.69554289]\n",
            " [-1.66669963 -1.5951139  -1.6720844  -1.63344077 -1.88877615 -1.82104479\n",
            "   1.97049143  0.736402  ]\n",
            " [-1.63502453 -1.60461643 -1.65846411 -1.62520525 -1.86731419 -1.82594807\n",
            "   2.09004454  1.48983144]\n",
            " [-1.57579208 -1.56343879 -1.60873419 -1.57325807 -1.83701688 -1.82844565\n",
            "   2.24199636  1.51705419]\n",
            " [-1.57452508 -1.56058803 -1.5951139  -1.57864284 -1.82112645 -1.82826188\n",
            "   2.21427707  1.32931348]\n",
            " [-1.57864284 -1.56058803 -1.5951139  -1.56343879 -1.8045437  -1.82867954\n",
            "   2.13728377  1.24052595]\n",
            " [-1.51529263 -1.44940841 -1.51941039 -1.47949976 -1.76920234 -1.82948979\n",
            "   2.24908946  1.26589752]\n",
            " [-1.52099415 -1.48646828 -1.52352816 -1.49058605 -1.7398611  -1.8300578\n",
            "   2.1616766   1.40632224]\n",
            " [-1.53999921 -1.49755457 -1.53999921 -1.52922968 -1.7109814  -1.83143607\n",
            "   1.86221089  0.81753024]\n",
            " [-1.52922968 -1.46587947 -1.54284997 -1.48646828 -1.68022255 -1.83343246\n",
            "   1.70152203  0.92109593]\n",
            " [-1.54158297 -1.48805204 -1.56217179 -1.54570073 -1.66505741 -1.83684889\n",
            "   1.30790725  1.17739644]\n",
            " [-1.53461444 -1.50452309 -1.54570073 -1.52099415 -1.65335388 -1.83743361\n",
            "   1.05329261  1.11846025]\n",
            " [-1.52099415 -1.47949976 -1.60208242 -1.5963809  -1.65035382 -1.8400732\n",
            "   0.57930836  0.82923192]\n",
            " [-1.69394022 -1.64737782 -1.70787727 -1.65149558 -1.65849684 -1.84547766\n",
            "   0.0557749   0.41399068]] [[-0.33325857  0.15580444 -1.641325  ]\n",
            " [-0.65332437  0.3274297  -1.8427453 ]\n",
            " [-0.87467885  0.2921357  -1.8772907 ]\n",
            " [-0.7527293   0.41036773 -1.8646543 ]\n",
            " [-0.76029533  0.39811105 -1.8645889 ]\n",
            " [-0.6745557   0.45650777 -1.850142  ]\n",
            " [-0.5411569   0.60977477 -1.8367045 ]\n",
            " [-0.58796734  0.6728275  -1.851574  ]\n",
            " [-0.42378536  0.7791909  -1.7998149 ]\n",
            " [-0.43472528  0.8103685  -1.8076909 ]\n",
            " [-0.49320126  0.79096943 -1.8063519 ]\n",
            " [-0.5120532   0.77020943 -1.7922595 ]\n",
            " [-0.54180616  0.78836876 -1.7856022 ]\n",
            " [-0.50226855  0.79195356 -1.7554942 ]\n",
            " [-0.62485296  0.6762142  -1.7677839 ]\n",
            " [-0.5809592   0.6557406  -1.7328439 ]\n",
            " [-0.45168868  0.62219524 -1.6490569 ]\n",
            " [-0.4005711   0.5703465  -1.6075156 ]\n",
            " [-0.39263988  0.42494348 -1.527745  ]\n",
            " [-0.29053485  0.32478517 -1.3463085 ]]\n",
            "\n",
            "[[-1.73131685 -1.71864681 -1.75443968 -1.72276457 -2.03785604 -1.78127558\n",
            "   1.57457442 -0.42892093]\n",
            " [-1.85896752 -1.81905689 -1.88937563 -1.81905689 -2.02140517 -1.79022177\n",
            "   1.514041   -0.62360457]\n",
            " [-1.82064065 -1.75855744 -1.85611676 -1.77249449 -2.00409713 -1.79833265\n",
            "   1.59319651 -0.08863674]\n",
            " [-1.77091073 -1.76964373 -1.83014318 -1.8067036  -1.97419543 -1.80659389\n",
            "   1.48566798 -0.13028251]\n",
            " [-1.79023255 -1.75443968 -1.81905689 -1.78199702 -1.9451509  -1.81288379\n",
            "   1.45751764  0.13797445]\n",
            " [-1.70914428 -1.68317069 -1.74493714 -1.69140622 -1.91683165 -1.81649234\n",
            "   1.69369119  0.69554289]\n",
            " [-1.66669963 -1.5951139  -1.6720844  -1.63344077 -1.88877615 -1.82104479\n",
            "   1.97049143  0.736402  ]\n",
            " [-1.63502453 -1.60461643 -1.65846411 -1.62520525 -1.86731419 -1.82594807\n",
            "   2.09004454  1.48983144]\n",
            " [-1.57579208 -1.56343879 -1.60873419 -1.57325807 -1.83701688 -1.82844565\n",
            "   2.24199636  1.51705419]\n",
            " [-1.57452508 -1.56058803 -1.5951139  -1.57864284 -1.82112645 -1.82826188\n",
            "   2.21427707  1.32931348]\n",
            " [-1.57864284 -1.56058803 -1.5951139  -1.56343879 -1.8045437  -1.82867954\n",
            "   2.13728377  1.24052595]\n",
            " [-1.51529263 -1.44940841 -1.51941039 -1.47949976 -1.76920234 -1.82948979\n",
            "   2.24908946  1.26589752]\n",
            " [-1.52099415 -1.48646828 -1.52352816 -1.49058605 -1.7398611  -1.8300578\n",
            "   2.1616766   1.40632224]\n",
            " [-1.53999921 -1.49755457 -1.53999921 -1.52922968 -1.7109814  -1.83143607\n",
            "   1.86221089  0.81753024]\n",
            " [-1.52922968 -1.46587947 -1.54284997 -1.48646828 -1.68022255 -1.83343246\n",
            "   1.70152203  0.92109593]\n",
            " [-1.54158297 -1.48805204 -1.56217179 -1.54570073 -1.66505741 -1.83684889\n",
            "   1.30790725  1.17739644]\n",
            " [-1.53461444 -1.50452309 -1.54570073 -1.52099415 -1.65335388 -1.83743361\n",
            "   1.05329261  1.11846025]\n",
            " [-1.52099415 -1.47949976 -1.60208242 -1.5963809  -1.65035382 -1.8400732\n",
            "   0.57930836  0.82923192]\n",
            " [-1.69394022 -1.64737782 -1.70787727 -1.65149558 -1.65849684 -1.84547766\n",
            "   0.0557749   0.41399068]\n",
            " [-1.69140622 -1.68031993 -1.79023255 -1.71864681 -1.67306856 -1.85003847\n",
            "  -0.51050687 -0.31336062]] [[-0.42683062  0.16458182 -1.664577  ]\n",
            " [-0.72371566  0.27785715 -1.8464434 ]\n",
            " [-0.7145664   0.4139491  -1.8586366 ]\n",
            " [-0.75139743  0.39306235 -1.858118  ]\n",
            " [-0.6688529   0.451299   -1.8457146 ]\n",
            " [-0.5375051   0.60657436 -1.8342108 ]\n",
            " [-0.58573204  0.6708593  -1.8500888 ]\n",
            " [-0.42255154  0.7779112  -1.7990735 ]\n",
            " [-0.43394032  0.80954856 -1.8075907 ]\n",
            " [-0.4927197   0.790448   -1.806891  ]\n",
            " [-0.511893    0.7698658  -1.7935581 ]\n",
            " [-0.54195637  0.7882047  -1.7880268 ]\n",
            " [-0.5026216   0.7919521  -1.7592013 ]\n",
            " [-0.62605184  0.67623115 -1.7725859 ]\n",
            " [-0.58325255  0.6559807  -1.7399352 ]\n",
            " [-0.4556177   0.6230127  -1.6604131 ]\n",
            " [-0.4092484   0.5739143  -1.6280444 ]\n",
            " [-0.42518952  0.42347303 -1.5744765 ]\n",
            " [-0.3914954   0.24294826 -1.5269071 ]\n",
            " [-0.34790894  0.12990713 -1.3631792 ]]\n",
            "\n",
            "[[-1.85896752 -1.81905689 -1.88937563 -1.81905689 -2.02140517 -1.79022177\n",
            "   1.514041   -0.62360457]\n",
            " [-1.82064065 -1.75855744 -1.85611676 -1.77249449 -2.00409713 -1.79833265\n",
            "   1.59319651 -0.08863674]\n",
            " [-1.77091073 -1.76964373 -1.83014318 -1.8067036  -1.97419543 -1.80659389\n",
            "   1.48566798 -0.13028251]\n",
            " [-1.79023255 -1.75443968 -1.81905689 -1.78199702 -1.9451509  -1.81288379\n",
            "   1.45751764  0.13797445]\n",
            " [-1.70914428 -1.68317069 -1.74493714 -1.69140622 -1.91683165 -1.81649234\n",
            "   1.69369119  0.69554289]\n",
            " [-1.66669963 -1.5951139  -1.6720844  -1.63344077 -1.88877615 -1.82104479\n",
            "   1.97049143  0.736402  ]\n",
            " [-1.63502453 -1.60461643 -1.65846411 -1.62520525 -1.86731419 -1.82594807\n",
            "   2.09004454  1.48983144]\n",
            " [-1.57579208 -1.56343879 -1.60873419 -1.57325807 -1.83701688 -1.82844565\n",
            "   2.24199636  1.51705419]\n",
            " [-1.57452508 -1.56058803 -1.5951139  -1.57864284 -1.82112645 -1.82826188\n",
            "   2.21427707  1.32931348]\n",
            " [-1.57864284 -1.56058803 -1.5951139  -1.56343879 -1.8045437  -1.82867954\n",
            "   2.13728377  1.24052595]\n",
            " [-1.51529263 -1.44940841 -1.51941039 -1.47949976 -1.76920234 -1.82948979\n",
            "   2.24908946  1.26589752]\n",
            " [-1.52099415 -1.48646828 -1.52352816 -1.49058605 -1.7398611  -1.8300578\n",
            "   2.1616766   1.40632224]\n",
            " [-1.53999921 -1.49755457 -1.53999921 -1.52922968 -1.7109814  -1.83143607\n",
            "   1.86221089  0.81753024]\n",
            " [-1.52922968 -1.46587947 -1.54284997 -1.48646828 -1.68022255 -1.83343246\n",
            "   1.70152203  0.92109593]\n",
            " [-1.54158297 -1.48805204 -1.56217179 -1.54570073 -1.66505741 -1.83684889\n",
            "   1.30790725  1.17739644]\n",
            " [-1.53461444 -1.50452309 -1.54570073 -1.52099415 -1.65335388 -1.83743361\n",
            "   1.05329261  1.11846025]\n",
            " [-1.52099415 -1.47949976 -1.60208242 -1.5963809  -1.65035382 -1.8400732\n",
            "   0.57930836  0.82923192]\n",
            " [-1.69394022 -1.64737782 -1.70787727 -1.65149558 -1.65849684 -1.84547766\n",
            "   0.0557749   0.41399068]\n",
            " [-1.69140622 -1.68031993 -1.79023255 -1.71864681 -1.67306856 -1.85003847\n",
            "  -0.51050687 -0.31336062]\n",
            " [-1.71864681 -1.69140622 -1.76837672 -1.75602343 -1.69311291 -1.85558494\n",
            "  -0.97901066 -0.78802383]] [[-0.46366438  0.12373534 -1.6703589 ]\n",
            " [-0.5701101   0.40422526 -1.834777  ]\n",
            " [-0.71479     0.39658892 -1.8521488 ]\n",
            " [-0.65969276  0.44623286 -1.8394408 ]\n",
            " [-0.5318958   0.60156256 -1.8299844 ]\n",
            " [-0.5818642   0.667994   -1.8476752 ]\n",
            " [-0.420567    0.77616835 -1.7976555 ]\n",
            " [-0.4327305   0.8084162  -1.8067434 ]\n",
            " [-0.49188414  0.7897088  -1.8065932 ]\n",
            " [-0.5113911   0.76935744 -1.7938032 ]\n",
            " [-0.5417075   0.7878739  -1.7889564 ]\n",
            " [-0.50253326  0.7917548  -1.7608483 ]\n",
            " [-0.6263362   0.6760542  -1.7747867 ]\n",
            " [-0.58394545  0.65588206 -1.7432083 ]\n",
            " [-0.45685533  0.623018   -1.665531  ]\n",
            " [-0.41166753  0.57445467 -1.6366589 ]\n",
            " [-0.4313059   0.42850187 -1.5887175 ]\n",
            " [-0.41971642  0.23948939 -1.5585477 ]\n",
            " [-0.486017   -0.00950468 -1.5362384 ]\n",
            " [-0.3815223  -0.00731489 -1.3538308 ]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3bS2BMRJfst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 결과 저장\n",
        "with open(f'{data_path}/trainAE.pickle', 'wb') as f:\n",
        "    pickle.dump([ndf, X_train_encoded, y_train], f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QliTZdO2Jv39",
        "colab_type": "text"
      },
      "source": [
        "# _2_. 평균 종가 예측\n",
        "\n",
        "feature의 개수를 autoencoder로 줄일 뿐만 아니라 학습 및 예측까지 진행한다. 결과가 어떤지 비교한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cZO2BSQJua_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 로드\n",
        "with open(f'{data_path}/trainAE.pickle', 'rb') as f:\n",
        "    df, X_train, y_train = pickle.load(f)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVq5AzwFKAHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "62e24c3b-d368-47c6-c4e1-63c5c9a64a0d"
      },
      "source": [
        "df # 원래 데이터"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>maShort</th>\n",
              "      <th>maLong</th>\n",
              "      <th>macd</th>\n",
              "      <th>rsi</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-05-31</th>\n",
              "      <td>-1.905847</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>-1.907114</td>\n",
              "      <td>-1.831410</td>\n",
              "      <td>-2.058362</td>\n",
              "      <td>-1.758238</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>-0.544542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-01</th>\n",
              "      <td>-1.853583</td>\n",
              "      <td>-1.834261</td>\n",
              "      <td>-1.880823</td>\n",
              "      <td>-1.864352</td>\n",
              "      <td>-2.064626</td>\n",
              "      <td>-1.767735</td>\n",
              "      <td>0.389674</td>\n",
              "      <td>-0.537252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-03</th>\n",
              "      <td>-1.820641</td>\n",
              "      <td>-1.725615</td>\n",
              "      <td>-1.835528</td>\n",
              "      <td>-1.731317</td>\n",
              "      <td>-2.054900</td>\n",
              "      <td>-1.774585</td>\n",
              "      <td>1.105865</td>\n",
              "      <td>-0.101499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-04</th>\n",
              "      <td>-1.731317</td>\n",
              "      <td>-1.718647</td>\n",
              "      <td>-1.754440</td>\n",
              "      <td>-1.722765</td>\n",
              "      <td>-2.037856</td>\n",
              "      <td>-1.781276</td>\n",
              "      <td>1.574574</td>\n",
              "      <td>-0.428921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-06-07</th>\n",
              "      <td>-1.858968</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>-1.889376</td>\n",
              "      <td>-1.819057</td>\n",
              "      <td>-2.021405</td>\n",
              "      <td>-1.790222</td>\n",
              "      <td>1.514041</td>\n",
              "      <td>-0.623605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-28</th>\n",
              "      <td>1.699414</td>\n",
              "      <td>1.748510</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.631629</td>\n",
              "      <td>1.933135</td>\n",
              "      <td>1.555955</td>\n",
              "      <td>-0.632407</td>\n",
              "      <td>0.097722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-29</th>\n",
              "      <td>1.672490</td>\n",
              "      <td>1.712084</td>\n",
              "      <td>1.615792</td>\n",
              "      <td>1.662987</td>\n",
              "      <td>1.913387</td>\n",
              "      <td>1.568251</td>\n",
              "      <td>-1.121550</td>\n",
              "      <td>-0.017689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-30</th>\n",
              "      <td>1.650634</td>\n",
              "      <td>1.672807</td>\n",
              "      <td>1.460584</td>\n",
              "      <td>1.485924</td>\n",
              "      <td>1.870431</td>\n",
              "      <td>1.576921</td>\n",
              "      <td>-1.777387</td>\n",
              "      <td>-0.252980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-31</th>\n",
              "      <td>1.547690</td>\n",
              "      <td>1.584116</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.387731</td>\n",
              "      <td>1.823188</td>\n",
              "      <td>1.587045</td>\n",
              "      <td>-2.340000</td>\n",
              "      <td>-0.756054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-02-03</th>\n",
              "      <td>1.284787</td>\n",
              "      <td>1.428908</td>\n",
              "      <td>1.240442</td>\n",
              "      <td>1.397233</td>\n",
              "      <td>1.766714</td>\n",
              "      <td>1.596668</td>\n",
              "      <td>-2.587475</td>\n",
              "      <td>-0.925866</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2386 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                open      high       low  ...    maLong      macd       rsi\n",
              "date                                      ...                              \n",
              "2010-05-31 -1.905847 -1.831410 -1.907114  ... -1.758238  0.002714 -0.544542\n",
              "2010-06-01 -1.853583 -1.834261 -1.880823  ... -1.767735  0.389674 -0.537252\n",
              "2010-06-03 -1.820641 -1.725615 -1.835528  ... -1.774585  1.105865 -0.101499\n",
              "2010-06-04 -1.731317 -1.718647 -1.754440  ... -1.781276  1.574574 -0.428921\n",
              "2010-06-07 -1.858968 -1.819057 -1.889376  ... -1.790222  1.514041 -0.623605\n",
              "...              ...       ...       ...  ...       ...       ...       ...\n",
              "2020-01-28  1.699414  1.748510  1.584116  ...  1.555955 -0.632407  0.097722\n",
              "2020-01-29  1.672490  1.712084  1.615792  ...  1.568251 -1.121550 -0.017689\n",
              "2020-01-30  1.650634  1.672807  1.460584  ...  1.576921 -1.777387 -0.252980\n",
              "2020-01-31  1.547690  1.584116  1.387731  ...  1.587045 -2.340000 -0.756054\n",
              "2020-02-03  1.284787  1.428908  1.240442  ...  1.596668 -2.587475 -0.925866\n",
              "\n",
              "[2386 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtBbU1I-KAiD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff95e648-ce3a-4d57-916c-34fce0cb22b5"
      },
      "source": [
        "X_train.shape # 차원이 축소된 데이터"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2362, 20, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDOfmwF1KBQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62e9938f-4712-475b-aa21-8c4775465e8a"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2362, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gSwsXlNKcrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e2a9076e-9f2c-4eaf-8e86-b37fb0b577f1"
      },
      "source": [
        "# 모델 파라미터 설정\n",
        "n_input = int(input('입력 피쳐 수 설정: '))\n",
        "n_output = int(input('출력 피쳐 수 설정: '))\n",
        "n_step = int(input('recurrent timestep 설정: '))\n",
        "n_hidden = int(input('은닉 노드 수 설정: '))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 피쳐 수 설정: 3\n",
            "출력 피쳐 수 설정: 1\n",
            "recurrent timestep 설정: 20\n",
            "은닉 노드 수 설정: 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "871fxBjUKB7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d396e6c9-468f-4b60-fb37-e4f1689e851e"
      },
      "source": [
        "# LSTM 모델 생성\n",
        "X_input = Input(batch_shape = (None, n_step, n_input))\n",
        "X_lstm = Bidirectional(LSTM(n_hidden), merge_mode='concat')(X_input)\n",
        "X_lstm = Dropout(0.1)(X_lstm)\n",
        "y_output = Dense(n_hidden, activation='relu')(X_lstm)\n",
        "y_output = Dropout(0.1)(y_output)\n",
        "y_output = Dense(n_output)(y_output)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(X_input, y_output)\n",
        "print(\"=========== 모델 전체 구조 ===========\")\n",
        "print(model.summary())\n",
        "print()\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 epochs=100,\n",
        "                 batch_size=300,\n",
        "                 shuffle=True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== 모델 전체 구조 ===========\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 20, 3)]           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 128)               34816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 43,137\n",
            "Trainable params: 43,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.6215\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.1424\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0886\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0527\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0389\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0301\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 1s 64ms/step - loss: 0.0271\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0246\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 1s 65ms/step - loss: 0.0243\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0208\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0212\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0200\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 1s 71ms/step - loss: 0.0204\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0186\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0195\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0189\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0189\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0185\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0181\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0186\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0168\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0157\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0165\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0161\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0163\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0160\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0146\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0148\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0145\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0140\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0147\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0148\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0142\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0142\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0135\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0135\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0136\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0133\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0137\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0144\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0128\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0123\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0120\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0125\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0133\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0123\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0126\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0128\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0121\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0132\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0115\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0121\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0121\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0121\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0115\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0114\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0115\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0118\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0121\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0115\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0116\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0119\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0118\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0109\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0107\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0115\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0108\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0110\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0109\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0115\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0111\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0109\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0109\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0115\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0109\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 1s 72ms/step - loss: 0.0105\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0117\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0104\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.0107\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0108\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0112\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0106\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0113\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0100\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0106\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0100\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0098\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0101\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0108\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 1s 70ms/step - loss: 0.0105\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 1s 66ms/step - loss: 0.0111\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0102\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0109\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0106\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0101\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0109\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0105\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0101\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 1s 67ms/step - loss: 0.0106\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 1s 69ms/step - loss: 0.0099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH95C9gTMFtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "6acc1568-1780-4799-90c3-c601f672159e"
      },
      "source": [
        "# plot loss\n",
        "plt.plot(hist.history['loss'], label='Train Loss')\n",
        "plt.title('Loss Trajectory')\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnAMTsoUMH9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "fef5d4a2-ef2e-42f7-a359-3c56005f67fd"
      },
      "source": [
        "# 5일 후 주가 이동평균 값 예측\n",
        "X_pred = np.reshape(X_train[-1], (-1, n_step, n_input))\n",
        "X_pred"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.5319474 ,  0.42260456,  0.17807007],\n",
              "        [ 0.5165871 ,  0.35768583,  0.26544008],\n",
              "        [ 0.5647369 ,  0.38819212,  0.2997633 ],\n",
              "        [ 0.59462845,  0.3146367 ,  0.30541718],\n",
              "        [ 0.71760637,  0.18714648,  0.31670865],\n",
              "        [ 0.700021  ,  0.13324367,  0.284945  ],\n",
              "        [ 0.8155086 , -0.03858918,  0.28203326],\n",
              "        [ 0.8745609 , -0.06652907,  0.29202634],\n",
              "        [ 0.91825974, -0.15028992,  0.2891301 ],\n",
              "        [ 0.950987  , -0.1373002 ,  0.30504653],\n",
              "        [ 0.9192803 , -0.05527693,  0.33557466],\n",
              "        [ 0.8884702 ,  0.02417354,  0.38019258],\n",
              "        [ 0.8985504 ,  0.07654196,  0.42524275],\n",
              "        [ 0.92096406,  0.00542146,  0.45744905],\n",
              "        [ 0.83078027,  0.09503077,  0.4435199 ],\n",
              "        [ 0.89004135,  0.11709179,  0.43150514],\n",
              "        [ 0.93591154,  0.11677253,  0.46433017],\n",
              "        [ 1.0074693 ,  0.01767957,  0.4603017 ],\n",
              "        [ 0.88234675,  0.12486199,  0.40010095],\n",
              "        [ 0.997353  ,  0.14038427,  0.34889522]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0b94uHOMOE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52529223-6ec3-4cd5-b73c-5fcc7edf302e"
      },
      "source": [
        "y_pred = model.predict(X_pred)[0][0]\n",
        "y_pred"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9249876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hccUuyYkMRnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "8b888dca-0c26-4717-aee0-26566f866038"
      },
      "source": [
        "# 마지막 100개 데이터 plot 후 다음 값 그려 보기\n",
        "last_data = np.array(df.iloc[-200:, [3, 4, 5]]) # 종가, 단기이평, 장기이평\n",
        "\n",
        "# 원 시계열과 예측된 시계열을 그린다\n",
        "ax1 = np.arange(1, len(last_data) + 1)\n",
        "ax2 = np.arange(len(last_data), len(last_data) + 5 + 1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ax1, last_data[:, 0], 'b-o', markersize=4, color='blue', label='Time series', linewidth=1)\n",
        "plt.plot(ax1, last_data[:, 1], color='red', label='Short MA', linewidth=1)\n",
        "plt.plot(ax1, last_data[:, 2], color='black', label='Long MA', linewidth=1)\n",
        "plt.plot((200, 205), (last_data[-1:, 0], y_pred), 'b-o', markersize=8, color='red', label='Estimate')\n",
        "plt.axvline(x=ax1[-1],  linestyle='dashed', linewidth=1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}