[머신러닝]

# 회귀



선형회귀. 

모든 데이터마다 거리 d를 측정한다. 그 거리에 제곱을 해서 더해서 평균을 낸다. 최소제곱법. 최소제곱이 가장 작아지는 지점에 w와 b를 찾으려고 한다. 목적함수. 이 목적함수를 최소화하자는 것!

 목적함수 loss function, cost function. 목적 함수를 거리 제곱의 평균으로, 제곱의 평균으로 계산하는 방법을 squared error. 그래서 loss function으로 mean squared error를 사용하는 게 일반적. mean  squared error를 최소화하는 w와 b를 찾는 것이 회귀의 방법론.

 Min w, b MSE



 x축에는 변동이 없다고 가정. x축이 기준이 되고 x축에 대해 y만 오차가 발생하는 상황. 이 때 y축에 평행한 거리를 측정. ordinary least squares. OLS. x는 독립변수, y는 종속변수. 

  예를 들어 주식이라면. 종합주가지수가 x일 때 삼성전자 주식의 수익률을 y로 놓고 x가 1퍼센트 증가할 때 y가 몇 퍼센트 증가하는가 이렇게 데이터를 관측할 수 있음. 이런 직선을 찾아서 보편적으로 종합주가지수가 어느 정도 오르면 삼성전자 주식 수익률은 얼마나 오르겠구나 이런 식으로. 변수 하나에 종속되게 다른 변수의 결과를 분석하고 싶을 때.

 

 그런데 예컨대 x도 변동하고 y도 변동하는 경우는 total least squares. 직교회귀. 목적함수가 달라진다. 한 점에서 직선에 이르는 거리를 계산하면, 그 식이 목적함수가 된다. 직교회귀.



 OLS 는 x축이 독립변수, y축이 whdtrqustndlf Eo, tLS는 x, y 모두 독립변수일 때 유효.





 사이킷런에서는 OLS만 지원. regression이 무엇인지 알기 위한 것이므로 간단하게 진행합니다!



 y = ax + b + 노이즈. 사이킷런에서는 linear regression이라는 함수를 써서 모델을 생성하고, fit한 뒤 예측.



 이런 회귀 문제에서는 성과를 어떻게 측정하는 것이 좋을까?

 분류 문제에서는 실제 라벨과 얼마나 같은지를 기준으로 정확도를 측정했다. 그런데 회귀의 경우는 전부 다 실수값이기 때문에 분류 문제와 같은 방식으로 정확도를 측정할 수 없다.

 

 사이킷런에서 model.score 함수 쓰면 rsquare 값이 나온다. 통계학에서의 rsquare 점수. x축 데이터로 종속변수를 얼마나 설명할 수 있을지 나타내는 설명력. r스퀘어로 모델 정확도 측정.



1-SSR/SST

SSR - 넘파이 모듈 써서 y햇과 y의 차이 제곱해서 더한 것. SST :민스퀘어 에러 토탈.' 실제 관측값과 추정치와의 차이가 아니라, 실제 관측값과 중간을 뚝 잘라서 데이터들이 전체 평균과 얼마나 차이가 있는지. 



 분류 문제와 회귀 문제에서 model.score 함수가 하는 일이 다르다.



 학습용과 테스트용으로 나누는데. 여기서 학습 데이터만 가지고 직선을 찾는다. 직선을 그대로 옮겨서 그리면? 이랬을 때 그 직선이 과연 저 데이더들을 얼마나 잘 설명하는가.





로지스틱 회귀



 odds ratio의 범위를 조정한 게 logit. logit = log(odds)

 0과 1의 범위 안에 있음.





 선형회귀가 아니라! 시그모이드! 함수를 활용합니다



# 크로스 엔트로피



 정보량 측정. 확률이 클수록 정보 가치가 떨어진다. 정보 가치는 확률에 반비례. 정보량과 확률은 반비례한다. 



 동시 확률을 생각해 보면, A라는 사건이 발생할 가치와 A라는 사건이 발생할 확률. B라는 사건이 발생할 가치와 B라는 사건이 발생할 확률도 역수로 표현됨. A와 B가 동시에 발생할 확률을 생각해 보면, 동시 발생하려면 A와 B를 곱해야 동시 발생. 두 정보가 합쳐졌을 때 토탈 정보량은 얼마가 될까? 그건 동시 정보량의 합이 아니다.



 로그 확률로 봤을 때 두 정보량의 합은 자연스럽게 로그로 나타내면 곱으로 나타낼 수 있게 된다.

 그러면, 정보량은 로그 확률에 반비례하는 것으로 정의하면 깔끔하게 정의된다.

그래서 인포메이션을 



 일어날 수도 있고 안 일어날 수도 있는데, 그 평균 확률을 계산해 보면, 정보량의 기댓값은 얼마일까. 보통 기댓값 계산할 때 E(I)..



 기대량을 저런 식으로 정의하면 잘 맞겠다. 평균 I = -시그마(p곱하기 로그p)

 이거 보니까 이미 엔트로피 공식! 물리학의 무질서도를 측정하는! 

 그래서 정보량이라고 하지 말고, 엔트로피라고 부르자. 의미는? 어떤 사건이 발생할 확률이 이러이러할 때 그런 사건이 가지는 정보의 가치, 정보량의 크기가 얼마나되는가를 이야기함. 이것을 엔트로피라고 합니다!