{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 설치\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install tensorflow-gpu==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트웨어측면 - 인공지능 (AI)\n",
    "\n",
    "# CS에서 궁극의 목표 중 하나\n",
    "# 문제도 많다!\n",
    "\n",
    "# 빅뱅의 시작을 1년으로 잡으면\n",
    "# 인류탄생은 2일전\n",
    "# 산업혁명은 2초전\n",
    "# 기술의 발전속도는 기하급수적으로 증가하고 있다.\n",
    "# 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 \n",
    "# 순간이 올거라고 예측!\n",
    "# 이 시점을 특이점 (singlurarity)\n",
    "# 조만간 특이점이 올거라고 많은 사람들이 생각하고 있다.\n",
    "# 그 시점을 사람들이 예측해보건대..\n",
    "# 2045년도에 탄생할 것\n",
    "\n",
    "# AI가 개발이 되면 인공지능은 전자회로의 속도로 학습을 하고\n",
    "# 사람은 생화학적 회로로 학습한다.\n",
    "# 전자회로의 속도가 약 100만배 정도 빠르다.\n",
    "# MIT의 AI개발자들이 만약 인공지능을 만들어내면\n",
    "# 인공지능이 1주일 동안 할 수 있는 양은 MIT AI개발자들이 2만년이 걸리는 양이다.\n",
    "\n",
    "# 현 시점에 가장 빠른 슈퍼컴퓨터가 미국 IBM이 만든 summit\n",
    "# 농구코트 2배정도 되는 크기에 캐비넷 깔아놓고 컴퓨터를 채워놓은 형태\n",
    "# 기후예측, 우주개발 시뮬레이션 등등\n",
    "\n",
    "# 2019년 10월 23일 구글이 네이처 지에 양자컴퓨터 개발에 대한 내용을 발표.\n",
    "# summit이 1000년동안 해야 하는 일을 3.5초만에 해결했다! 고 발표.\n",
    "# 실제로는 2.5일 정도되는 일을 3.5초 해결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1960년대부터 꾸준히 연구,개발\n",
    "\n",
    "# 인간이 뇌를 연구하기 시작\n",
    "# 1958년에 perceptron을 모델링한 기계를 실제로 구현\n",
    "# 뉴욕타임즈 기사 게재\n",
    "# \"조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만들수 있다!\"\n",
    "4\n",
    "\n",
    "# AND / OR 에 대한 logistic regression => perceptron\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit,\n",
    "                                                                 labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습해보기\n",
    "\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X : x_data,\n",
    "                                                       Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(f\"Cost값은 : {cost_val}\")\n",
    "        \n",
    "# predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "# sess.run(predict, feed_dict = {[0,0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict = {X:[[1,0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron(logistic)으로 AND / OR는 구현이 가능\n",
    "# XOR는 perceptron으로 구현이 안된다\n",
    "# 많은 사람들이 XOR를 어떻게 perceptron으로 구현할 수 있을까?\n",
    "\n",
    "# 1969에 마빈 민스키라는 사람이 논문을 발표\n",
    "# MIT AI Lab 창시자\n",
    "# XOR는 한개의 perceptron으로 학습이 불가능\n",
    "# MLP (Multi Layer Perceptron) 으로는 가능\n",
    "# MLP는 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수 없다.\n",
    "# AI가 망했다.\n",
    "\n",
    "# 1974년도 Paul이라는 박사과정 학생이 Backpropagation방법을 고안\n",
    "# 그러나 이미 배는 떠났다.\n",
    "# 1982년도에 다시 한번 논문을 발표!\n",
    "# 1986년도에 Hinton교수님이 논문을 발표! (살을 붙이고 추가해서)\n",
    "# 다시 활황\n",
    "\n",
    "# 1995년 쯤에 Backpropagation방식이 안되는건 아닌데..\n",
    "# 복잡한 문제는 역시나 해결이 불가능!\n",
    "# 이 시기에 다른 여러가지 알고리즘이 나타나기 시작\n",
    "# SVM, 나이브 베이지안, Decision Tree..\n",
    "# LeCun => 다른 알고리즘이 더 우수하다는 것을 증명\n",
    "# 다시 침체기\n",
    "\n",
    "# 캐나다가 국책연구기관을 설립\n",
    "# Canadian Institute For Advanced Research (CIFAR)\n",
    "# 1987년에 캐나다로 건너가서 AI연구를 지속\n",
    "# 2006~7년도에 2개의 논문을 발표\n",
    "# 망했던 이유를 찾았다!\n",
    "# W, b의 초기값을 random으로 주면 안되는 이유를 규명 <= 2006년도\n",
    "# 2007년도 => 초기값에 대한 증명에 대한 논문, \n",
    "# layer를 더 많이 사용할 수록 복잡한 문제를 해결할 수 있다!\n",
    "\n",
    "# 사람들이 반응이 싸늘하다.\n",
    "# 신분세탁 -> deep learning 으로!\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 1.262202262878418\n",
      "Cost값은 : 0.001196566503494978\n",
      "Cost값은 : 0.0006161608034744859\n",
      "Cost값은 : 0.0004337542341090739\n",
      "Cost값은 : 0.00033550671651028097\n",
      "Cost값은 : 0.00027329649310559034\n",
      "Cost값은 : 0.0002302827633684501\n",
      "Cost값은 : 0.00019878038438037038\n",
      "Cost값은 : 0.00017470816965214908\n",
      "Cost값은 : 0.00015574952703900635\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32)\n",
    "\n",
    "# Weight & bias 1\n",
    "W1 = tf.Variable(tf.random_normal([2,100]),name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([100]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# Weight & bias 2\n",
    "W2 = tf.Variable(tf.random_normal([100,256]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "# Weight & bias 3\n",
    "W3 = tf.Variable(tf.random_normal([256,1]),name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([1]),name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit,\n",
    "                                                                 labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습해보기\n",
    "\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X : x_data,\n",
    "                                                       Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(f\"Cost값은 : {cost_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict = {X:[[1,1]]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
