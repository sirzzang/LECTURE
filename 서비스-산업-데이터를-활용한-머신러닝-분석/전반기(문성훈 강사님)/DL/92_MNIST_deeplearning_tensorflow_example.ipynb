{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소프트웨어 측면 - 인공지능(AI)\n",
    "- 하드웨어 측면 - 양자이론, 양자컴퓨터\n",
    "\n",
    "### 빅뱅의 시작을 1년으로 잡으면 \n",
    "- 인류의 탄생은 2일 전 (호모사피엔스)\n",
    "- 산업혁명은 2초 전 \n",
    "- 기술의 발전 속도는 기하급수적으로 증가하고 있음 \n",
    "- 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올 것이라고 예측할 수 있다! \n",
    "- 이 시점을 특이점(singlurarity)라고 한다.\n",
    "- 조만간 특이점이 올 것이라고 많은 사람들이 생각하고 있고 그 시점을 사람들이 예측해보면 2045년도일 것...\n",
    "- 많은 학자들 중 일부는 특이점이 오는 시기가 인류가 멸망하는 시기라고 예측\n",
    "- 회사들은 안전하다고 말함 > AI가 선 악을 구분할 수 있을 것이다. 그래서 사람을 죽이거나 하지 않을 것이다. \n",
    "- 프로그래밍을 통해 AI를 제어할 수 있음\n",
    "\n",
    "###  뇌과학자 => AI가 개발이 되면 인간과 학습하는 능력이 비슷할 것인데, 속도는 차이남.\n",
    "- 인공지능은 전자회로의 속도로 학습을 하는 것이고 사람은 생화학적 회로로 학습\n",
    "- 전자회로의 속도가 약 100만배정도 빠름\n",
    "- MIT의 AI 개발자들이 만약 인공지능을 만들어 내면 인공지능이 1주일동안 할 수 있는 일을 MIT AI 개발자들이 2만년이 걸린다... > 인공지능이 우리를 미개하게 볼 것 > 사람을 죽이지 말라는 프로그래밍 코드를 삽입해도 의미없음\n",
    "\n",
    "### 현 시점에 가장 빠른 슈퍼컴퓨터: 미국-IBM이 만든 서밋(summit)\n",
    "- 농구코트의 2배정도 되는 크기에 캐비넷 깔아놓고 그 안에 컴퓨터를 채워넣음 \n",
    "- 기후예측, 화성탐사선 시뮬레이션(우주개발 시뮬레이션) 등을 수행함\n",
    "\n",
    "### 작년 10월 23일 구글이 네이처지에 양자컴퓨터 개발에 대한 논문 발표\n",
    "- 서밋이 1000년동안 해야 하는 일을 3.5초만에 해결했다고 발표 -> 사실과는 다름 \n",
    "- IBM의 반격: 서밋이 2.5일 정도 되는 일을 3.5초만에 해결 (그래도 빠르긴 함)\n",
    "- 양자컴퓨터는 모든 산업에 적용되지 않고 특정 분야에만 적용 가능 - 인간의 게놈 지도, 양자연구 등\n",
    "- 기존의 보안 체계에는 문제 없을 것! \n",
    "\n",
    "\n",
    "\n",
    "# 인공지능(AI)\n",
    "\n",
    "- CS에서 궁극의 목표 중 하나 \n",
    "- 문제도 많음\n",
    "- 직업적인 문제가 생길 수 있음\n",
    "- 20년 뒤에는 직업의 절반 가량이 없어지고 10년 더 지나면 모든 직업이 사라질 것이라는 예측이 있다. \n",
    "- 사람은 무엇을 하며 살아야 하는가?\n",
    "\n",
    "- 인간의 뇌를 연구하기 시작\n",
    "- perceptron: 사람의 뇌처럼 모델링 한 것\n",
    "- 1958년에 perceptron을 모델링한 기계를 실제로 구현\n",
    "- 뉴욕타임즈에 기사가 실림 \n",
    "    - 조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만들 수 있다.\n",
    "- AND/OR에 대한 logistic regression => perceptron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost값은:0.6766015291213989\n",
      "cost값은:0.3063524663448334\n",
      "cost값은:0.20591184496879578\n",
      "cost값은:0.1560397893190384\n",
      "cost값은:0.12551918625831604\n",
      "cost값은:0.10480726510286331\n",
      "cost값은:0.08982474356889725\n",
      "cost값은:0.07849374413490295\n",
      "cost값은:0.06963463127613068\n",
      "cost값은:0.06252612173557281\n"
     ]
    }
   ],
   "source": [
    "# tensorflow로 logistic 구현해보자!\n",
    "# 진리표를 이용해서 학습할 것\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# AND 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [1]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "# W & b\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "# H\n",
    "logit = tf.matmul(X,W)+b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0]]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost값은:0.8159719109535217\n",
      "cost값은:0.7017427682876587\n",
      "cost값은:0.693906307220459\n",
      "cost값은:0.6932177543640137\n",
      "cost값은:0.6931538581848145\n",
      "cost값은:0.6931477785110474\n",
      "cost값은:0.6931472420692444\n",
      "cost값은:0.6931471824645996\n",
      "cost값은:0.6931471824645996\n",
      "cost값은:0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "# W & b\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "# H\n",
    "logit = tf.matmul(X,W)+b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0]]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "#### Perceptron(logistic)으로 AND/OR은 구현 가능, XOR(Exclusive OR)는 구현 불가!\n",
    "- 많은 사람들이 XOR를 어떻게 Perceptron으로 구현할 수 있을까? 고민 \n",
    "\n",
    "#### 1969에 마빈 민스키라는 사람이 논문 하나 발표\n",
    "- MIT AI lab 창시자 \n",
    "- XOR는 한 개의 perceptron으로 학습이 불가능하다는 것을 수학적으로 증명\n",
    "- MLP(multi layer perceptron)으로는 가능하다\n",
    "- MLP는 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수 없음 \n",
    "    - ->1970년대 AI의 침체기\n",
    "\n",
    "#### 1974년도 Paul이라는 박사과정 학생이 Backpropagation 방법을 고안 \n",
    "- 그러나 이미 배는 떠났음.. 아무도 안함... 묻히게 됨\n",
    "- 1982년도에 다시 한 번 논문을 발표\n",
    "\n",
    "#### 1986년도에 Hinton 교수가 논문 발표(Backpropagation 내용 가지고)(명망 높은 사람이라 사람들이 관심 가지기 시작)\n",
    "- AI가 다시 각광받기 시작\n",
    "\n",
    "#### 1995년쯤에 Backpropagation 방식이 안되는 건 아닌데 복잡한 문제는 해결 불가능하다.\n",
    "- 이 시기에 다른 여러가지 알고리즘이 나타나기 시작\n",
    "- SVM, 나이브 베이지언, Decision Tree \n",
    "- LeCUN => 다른 알고리즘이 더 우수하다는 것을 증명 \n",
    "- 다시 침체기에 진입\n",
    "\n",
    "#### 1990년도쯤 캐나다가 국책 연구기관을 설립\n",
    "- Canadian Institute For Adanced Research(CIFAR)\n",
    "- 1987년에 Hinton교수가 캐나다로 건너가서 AI연구를 지속\n",
    "- 2006, 2007년도에 Hinton교수가 2개의 논문을 발표 \n",
    "    - 망했던 이유를 수학적으로 증명함 \n",
    "    - 2006년도 => 이유: W와 b의 초기값을 randomㅡ로 주면 안된다는걸 알아냄.\n",
    "    - 2007년도 => 초기값 증명에 대한 논문. layer를 더 많이 사용할수록 복잡한 문제를 해결할 수 있다는 논문 발표\n",
    "- 사람들의 반응이 차가움..양치기 소년...\n",
    "- 신분세탁(rebranding) => Deep Learning\n",
    "\n",
    "#### 가장 많이 쓰이는 분야: 추천서비스\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2개짜리 logistic (deep learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# W & b & layer\n",
    "W1 = tf.Variable(tf.random_normal([2,2]),name=\"weight1\")  # 앞쪽logistic  # 뒤의 logistic이 계산 수행하려면 x 두개 수행해야 하므로 결과 2개 출력해줘야 함                                                    \n",
    "b1 = tf.Variable(tf.random_normal([2]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]),name=\"bias2\")\n",
    "\n",
    "# H\n",
    "logit = tf.matmul(layer1,W2)+b2\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n",
    "\n",
    "# predict\n",
    "predict = tf.cast(H>0.5,dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[1,1]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer를 늘려보자! - 다계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 연산\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# W & b & layer - 100개,256개의 logistic\n",
    "W1 = tf.Variable(tf.random_normal([2,100]),name=\"weight1\")  # 앞쪽logistic  # 뒤의 logistic이 계산 수행하려면 x 두개 수행해야 하므로 결과 2개 출력해줘야 함                                                    \n",
    "b1 = tf.Variable(tf.random_normal([100]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([100,256]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([100,256]),name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([256]),name=\"bias3\")\n",
    "# H\n",
    "logit = tf.matmul(layer2,W3)+b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels = Y))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,\n",
    "                                                   Y:y_data})\n",
    "    if step%3000==0:\n",
    "        print(\"cost값은:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
