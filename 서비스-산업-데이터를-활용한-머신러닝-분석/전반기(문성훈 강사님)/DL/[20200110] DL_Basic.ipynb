{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 딥러닝 환경설정\n",
    "\n",
    " 지금까지 사용한 tensorflow는 cpu 버전이다. 딥러닝에서는 더 많은 수학적 연산(matrix 연산)을 해야 하기 때문에, GPU를 이용해야 한다. GPU를 이용해야 더 빠르게 많은 계산을 할 수 있다.\n",
    " \n",
    " 새로운 가상환경을 만들어 tensorflow GPU 버전을 이용한다.\n",
    " \n",
    " ```\n",
    " conda create -n gpu_env python=3.6 openssl\n",
    " activate gpu_env\n",
    " conda install nb_conda # nb_conda 설치\n",
    " python -m ipykernel install --user --name=gpu_env --display-name=[GPU_ENV]\n",
    " ```\n",
    " \n",
    " 이후 GPU를 사용하기 위해 그래픽카드 비디오 드라이버를 설치한다. NVIDIA 비디오 드라이버를 버전을 맞춰서 설치해 주자. 이후 행렬 연산을 위해 CUDA를 설치한다.\n",
    " * NVIDIA 비디오 드라이버 설치\n",
    " * CUDA 설치\n",
    " * cuDNN 압축 풀어서 덮어 쓰기(파일 3개를 NVIDIA GPU Computing Toolkit v10 안에 덮어 씌우자)\n",
    "\n",
    "\n",
    "\n",
    " 그리고 필요한 모듈을 설치하자.\n",
    "\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "pip install tensorflow-gpu==1.15\n",
    "```\n",
    "\n",
    " Tensorflow 1.15 gpu 이용해서 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*딥러닝*\n",
    "\n",
    " 소프트웨어측면에서 인공지능(AI)은 궁극적 목표 중 하나이다. 그러나 그만큼 문제도 많다.\n",
    " \n",
    " > 빅뱅의 시작을 1년으로 잡으면,\n",
    " > 인류 탄생은 2일 전, \n",
    " > 산업혁명은 2초 전.\n",
    " \n",
    " 기술의 발전속도는 기하급수적으로 증가하고 있다. 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올거라고 예측되는데, 그 순간을 **특이점(singularity)**이라고 한다. 사람들이 조만간 특이점이 올 거라고 생각하고 있는데, 그 시점을 예측해 보건대 2045년 즈음이 될 것 같다고.\n",
    " \n",
    " \n",
    " AI가 개발이 되면 인공지능은 전자회로의 속도로 학습을 하고, 사람은 생화학적 회로로 학습한다. 전자회로의 속도가 약 100만 배 정도 빠르다! MIT의 AI개발자들이 만약 인공지능을 만들어 내면 인공지능이 1주일 동안 할 수 있는 양은 MIT AI개발자들이 2만 년이 걸리는 양이다.\n",
    "\n",
    " 현 시점에 가장 빠른 슈퍼컴퓨터가 미국 IBM이 만든 `summit`. 농구코트 2배정도 되는 크기에 캐비넷 깔아놓고 컴퓨터를 채워놓은 형태이다. 기후 예측, 우주개발 시뮬레이션 등등의 태스크를 수행하는데.\n",
    " \n",
    " 2019년 10월 23일 구글이 네이처 지에 양자컴퓨터 개발에 대한 내용을 발표했다. 그런데 summit이 1000년 동안 해야 하는 일을 3.5초만에 해결했다(...)고. 실제로는 2.5일 정도 필요한 일을 3.5초만에..\n",
    " \n",
    " 어쨌든! 딥러닝은 1960년대부터 꾸준히 연구되고 개발되어 온 분야. 인간의 뇌를 연구하고, 1958년에 perceptron을 모델링한 기계가 실제로 구현되었다. 뉴욕타임즈에, \"조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만들 수 있을 것이다.\"라는 기사!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XOR 문제\n",
    "\n",
    " Exclusive OR 문제가 제기되면서, perceptron으로 logistic regression을 구현하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# module import\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AND / OR 에 대한 logistic regression => perceptron\n",
    "\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "X = tf.placeholder(shape=[None,2], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit,\n",
    "                                                                 labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X : x_data,\n",
    "                                                       Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(f\"Cost값은 : {cost_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "# sess.run(predict, feed_dict = {[0,0]})\n",
    "\n",
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict = {X:[[1,0]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론과 딥러닝\n",
    "\n",
    " Perceptron(logistic)으로 AND, OR 문제는 구현할 수 있다. 그러나 XOR 문제는 perceptron으로 구현이 안 된다.\n",
    " \n",
    " 1969년에 마빈 민스키(MIT AI Lab 창시자)가 XOR 문제를 1개의 퍼셉트론으로 학습시킬 수 없다는 논문을 발표했다. Multi Layer Perceptron을 사용해야 학습이 가능하다. 그러나 학습이 어려워서 AI가 망했다.\n",
    " \n",
    " 1974년 Paul이라는 박사과정 학생이 back propagation 방법을 고안했다. 그러나 이미 배가 떠났고, 1982년도에 논문을 또 발표했다. 1986년에 Hinton 교수가 살을 붙이고 추가해서 논문을 다시 발표했고, AI가 활황이 되었다.\n",
    " \n",
    " 1995년, back propagation 방식에서도 복잡한 문제는 역시나 해결할 수 없다는 문제가 있었고, 여러 가지 다른 알고리즘이 나타나기 시작했다. 예컨대, SVM, 나이브 베이지안, Decision Tree 등. LeCun이 다른 알고리즘이 더 우수하다는 것을 증명했고, 딥러닝이 다시 침체기에 들어섰다.\n",
    " \n",
    " 이후 캐나다에서 Canadian Institute for Advanced Research(CIFAR)라는 국책 연구기관을 발표했다. 이후 Hinton 교수가 1987년에 캐나다로 건너가서 AI 연구를 지속했다. 2006년, 2007년 즈음 2개의 다른 논문을 발표했고, 그 논문에서 기존의 딥러닝이 좋지 못한 성능을 보인 이유를 증명해 냈다.\n",
    " \n",
    " 2006년 Hinton교수가 W, b의 초기값을 random하게 설정하면 안 되는 이유를 규명했고, 2007년도에 초기값 증명에 대한 논문을 발표했다. layer를 더 많이 사용할 수록 복잡한 문제를 해결할 수 있다고 했다.\n",
    " \n",
    " 이렇게 퍼셉트론이 신분을 세탁해서 딥러닝이 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 1.262202262878418\n",
      "Cost값은 : 0.001196566503494978\n",
      "Cost값은 : 0.0006161608034744859\n",
      "Cost값은 : 0.0004337542341090739\n",
      "Cost값은 : 0.00033550671651028097\n",
      "Cost값은 : 0.00027329649310559034\n",
      "Cost값은 : 0.0002302827633684501\n",
      "Cost값은 : 0.00019878038438037038\n",
      "Cost값은 : 0.00017470816965214908\n",
      "Cost값은 : 0.00015574952703900635\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]]\n",
    "y_data = [[0],\n",
    "         [1],\n",
    "         [1],\n",
    "         [0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32)\n",
    "\n",
    "# Weight & bias 1\n",
    "W1 = tf.Variable(tf.random_normal([2,100]),name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([100]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# Weight & bias 2\n",
    "W2 = tf.Variable(tf.random_normal([100,256]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "# Weight & bias 3\n",
    "W3 = tf.Variable(tf.random_normal([256,1]),name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([1]),name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit,\n",
    "                                                                 labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습해보기\n",
    "\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={X : x_data,\n",
    "                                                       Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(f\"Cost값은 : {cost_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = tf.cast(H > 0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict = {X:[[1,1]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 고려할 사항\n",
    "\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    " 정해져 있는 값은 없다. 0.01 정도를 기준으로 cost값을 보고, 이후 learning rate를 조절한다. 만약 learning rate가 크다면, 과적합(overshooting) 현상이 발생한다. 반면 learning rate가 너무 작다면, local minimum 현상이 발생한다. 따라서 학습률을 조절하며 cost 값을 기준으로 커스터마이징해야 한다.\n",
    " \n",
    " \n",
    "## Preprocessing\n",
    "\n",
    " feature engineering을 포함해서, 각 feature 간 데이터 범주와 크기를 살펴본다. minmax scaling 등을 통한 정규화(normalization), 평균과 표준편차를 이용해서 -1과 1 사이의 값으로 scaling하는 표준화(standardization) 방식이 있다.\n",
    " \n",
    "## Overfitting\n",
    "\n",
    " 과적합으로, 모델이 학습 데이터에 너무 잘 들어맞는 방식으로 학습하는 경우를 의미한다. 실제 데이터를 적용할 때 결과값 예측이 잘 되지 않는다.(새로운 데이터)\n",
    " \n",
    " 과적합 현상을 피하기 위해서는, 가능한 많은 training data set이 있어야 하고, feature 개수를 가능한 줄여야 한다.\n",
    " 필요 없거나 중복되는 column을 삭제하는 방법도 생각해 본다.\n",
    " \n",
    "## Train\n",
    "\n",
    " 일반적으로 딥러닝 학습 과정에서는 train data set의 크기가 굉장히 크다. 따라서 1에폭을 수행하는 시간이 오래 걸린다. 따라서 주로 batch 처리를 이용해서 학습을 진행한다.\n",
    " \n",
    "## Accuracy\n",
    "\n",
    " raw data set을 얻은 후, train set과 test set으로 분리한다. 보통 그 비율은 `7:3`, `8:2` 정도로 설정한다. 테스트 셋은 평가 전까지 절대 들여다보지 않는다. train set 안에서도 검증을 위해 n-fold coss validation을 하기도 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
