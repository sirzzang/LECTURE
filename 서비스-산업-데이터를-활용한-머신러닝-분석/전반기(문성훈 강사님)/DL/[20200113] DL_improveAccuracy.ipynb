{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 정확도 높이기\n",
    "> MNIST dataset에 ReLU, Xavier 초기화, Dropout 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\")     # warning 출력 제한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset 탐구\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-384fce364529>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\",\n",
    "                                 one_hot = True)\n",
    "# MNIST data set 불러들인 후, data 폴더 안에 4개의 압축파일 생성.\n",
    "# x : 픽셀 데이터, 약 55000개의 이미지. 28*28 pixel.\n",
    "# y : x 데이터에 onehot encoding 처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c3b5bcef98>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X data 탐구.\n",
    "## 각각의 데이터 별로 0과 1 사이로 변환된 진하기가 들어 있다.\n",
    "print(mnist.train.images[0]) \n",
    "\n",
    "# label 탐구\n",
    "# 첫 번째 이미지는 multinomial label을 본 결과, 숫자 \"7\"이다.\n",
    "print(mnist.train.labels[0])\n",
    "\n",
    "# 그림 그려보기\n",
    "# 28*28 사이즈로 reshape 후 그림 그리기.\n",
    "pix = mnist.train.images[0].reshape(28,28)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pix, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "---\n",
    "> single layer의 x ~ y에서 \"~\"에 해당하는 단계에 layer 중첩\n",
    "---\n",
    "* 1) 여러 개의 weights, bias 설정.\n",
    "    * 단순 logistic model에서는 1개의 weight, bias만 지정.\n",
    "    * 그러나 딥러닝에서는 여러 계층을 만들어 신경망을 만들고, 각 계층에서 logistic model을 여러 개 구현.\n",
    "* 2) 적절한 수의 perceptron 설정.\n",
    "    * 계층이 너무 많으면 depth 문제로 시간이 오래 걸리고, 정확한 연산이 오히려 안 될 수 있음.\n",
    "    * 각 계층별 weight와 bias의 shape에 주의할 것\n",
    "* 3) epoch, batch : 각 학습의 단계별로 data의 개수를 나누어 진행.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기존의 방식으로 진행\n",
    "* 이 단계에서는 일단 ReLU 적용 전이므로, sigmoid나 softmax나 상관 없이 그냥 사용.\n",
    "* 처음에 강사님 H에 sigmoid 설정했음 -> Q) multinomial에 대해 딥러닝 진행하니까, softmax를 써야 하는 것 아닌가? -> A) 그렇게 하자! sigmoid나 softmax나 크게 차이는 없다. 어차피 operation function은 나중에 바뀌어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 값은 : 0.9665764570236206\n",
      "cost 값은 : 0.6756019592285156\n",
      "cost 값은 : 0.47195690870285034\n",
      "cost 값은 : 0.26232409477233887\n",
      "cost 값은 : 0.22157913446426392\n",
      "cost 값은 : 0.25711145997047424\n",
      "cost 값은 : 0.25551527738571167\n",
      "cost 값은 : 0.24147453904151917\n",
      "cost 값은 : 0.09975516051054001\n",
      "cost 값은 : 0.13791650533676147\n",
      "학습 끝\n",
      "정확도는 91.64000153541565%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # 그래프 초기화\n",
    "\n",
    "# 1. placeholder\n",
    "## 입력 파라미터 받은 후 학습 진행 + 정확도 구하고 + 예측하는 tensor.\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) # 784 = 28*28.\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "# 2. weight, bias 설정\n",
    "## layer1 -> layer2 -> layer3: 각 계층의 결과를 받아서 다음 계층으로 넘김.\n",
    "## 256은 강사님이 임의로 설정한 수. 관용적인 용법 존재하지 않음.\n",
    "\n",
    "## X data 각각에 대해서 256번 학습 진행.\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "## 256번 학습한 걸 다시 돌려서 256번 학습 진행.\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "layer2= tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "## 최종적으로 10개의 label값에 맞도록 학습 진행.\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "# 3. hypothesis\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# 4. cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "# 5. train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# 6. session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 7. learning\n",
    "## 데이터셋의 개수가 많으면 학습이 어렵기 때문에, batch 통해 큰 사이즈의 데이터를 나눔.\n",
    "\n",
    "num_of_epoch = 30  # 전체 학습 횟수\n",
    "batch_size = 100   # 데이터셋의 개수를 몇 개로 나눌 것인가\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size)  # 한 번의 epoch에 몇 번의 batch를 불러와야 하는가\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # next_batch : mnist.train 안에 내장된 함수로, 몇 개씩 데이터를 불러들일지 정해주는 함수.\n",
    "            # 1) for문 통해 batch 개수만큼 불러와야 하는데, tensorflow의 mnist 예제에는 next_batch로 그 과정이 이미 구현되어 있다\n",
    "            # 2) x, y data로 나눠줘야 하는데, next_batch 함수 쓰면 그 자체에서 x, y data로 나뉘어서 도출됨.\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y})\n",
    "            # iteration 한 번 돌 때마다 cost_val이 나온다.\n",
    "            # batch_1에 대한 cost_val, batch_2에 대한 cost_val, ..., batch_550에 대한 cost_val.\n",
    "\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "             # 3, 6, 9, ...번째 학습 단계마다 cost 값을 찍어라.\n",
    "            # 출력되는 cost값은 마지막 batch에 대한 cost값.\n",
    "            \n",
    "print(\"학습 끝\")\n",
    "\n",
    "# 8. accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    # 예측값\n",
    "correct= tf.equal(predict, tf.argmax(Y,1))   # Y에 test label 넣어서 predict와 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "    # correct인지 아닌지 판단한 것을 실수 값으로 변환 -> 1인 게 예측 맞은 것.\n",
    "    # 평균 냄 -> \"(1의 합) / (전체 개수)\"이므로, 얼마나 맞는지 확인할 수 있음.\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels})\n",
    "    # X에 test용으로 제공된 이미지 넣어서 predict.\n",
    "    # Y에 label으로 제공된 이미지 넣어서 predict와 correct 작업 진행.\n",
    "    # result 변수에 저장.\n",
    "    \n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 정확도 향상\n",
    "---\n",
    "> 딥러닝을 진행하였으나, 기존에 multinomial classification으로 진행했을 때와 비교해서 정확도가 유의미하게 향상되지는 않았다. 다음의 방식을 통해 딥러닝의 정확도를 향상시킬 수 있다.\n",
    "---\n",
    "\n",
    "* 1. ReLU함수 사용 : 딥러닝의 activation function은 전부 ReLU 사용.\n",
    "* 2. 가중치 설정 : (딥러닝 뿐이 아니더라도) Xavier initializer 사용.\n",
    "* 3. optimizer : `GradientDescentOptimzer` 외에 `AdamOptimizer` 사용할 수 있음.\n",
    "* 4. dropout : 과적합 방지하기 위한 dropout 방식 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Activation Function : ReLU함수\n",
    "---\n",
    "> Deep Learning에서 정확도를 향상시키기 위해서는, ReLU(Rectified Linear Unit) 함수를 사용한다.\n",
    "---\n",
    "* 딥러닝은 학습을 `deep & wide`하게 진행하기 위해 layer 추가하고, 각 layer에 많은 perceptron을 추가해서 구현하는 방식이다.\n",
    "* 이러한 multi layer 학습 방식에 sigmoid/softmax 함수를 적용하게 되면, Vanishing Gradient 문제가 발생한다.\n",
    "    * sigmoid가 중첩되면서 W, b 값이 0에 수렴한다(=vanish).\n",
    "    * 즉, sigmoid 함수의 특성으로 인해 y label이 0과 1 사이의 값을 갖는데, 이 값들이 여러 layer를 거치며 sigmoid함수를 여러 번 지나게 되면, 0에 가까운 값들이 계속해서 곱해지면서 굉장히 작은 값을 갖게 된다.\n",
    "    * 이로 인해 layer가 깊어질수록 학습 및 예측이 어려워지는 현상이 나타나는 것이다.\n",
    "* Hinton교수가 고안한 ReLU 함수는, 입력값이 0보다 작을 때에는 non-active를 할당하고, 0보다 클 때에는 그 값을 그대로 반환한다.\n",
    "    * x>0이면 기울기가 1인 직선이고, x<0이면 함수값이 0이 된다.\n",
    "    * x<0인 값들에 대해서는 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점이 존재한다.\n",
    "* activation function만 바꾸면 된다. cost function에서 softmax entropy 함수는 바꿀 필요 없다.\n",
    "```\n",
    "# 기존의 activation function 설정\n",
    "layer1 = tf.nn.softmax(tf.matmul(X, W1) + b1)\n",
    "H = tf.nn.softmax(logit)\n",
    "# ReLU 함수 이용\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "H = tf.nn.relu(logit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "cost 값은 : nan\n",
      "학습 끝\n",
      "정확도는 9.799999743700027%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# model 설정\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) \n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "layer2= tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch = 30  \n",
    "batch_size = 100   \n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size) \n",
    "    cost_val = 0\n",
    "\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y})\n",
    "\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "            \n",
    "print(\"학습 끝\")\n",
    "\n",
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    \n",
    "correct= tf.equal(predict, tf.argmax(Y,1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels})\n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그럼에도 불구하고, cost 값이 `nan`이 나오는데, 이는 모델이 발산한다는 의미이다.\n",
    "* 아래와 같이 다시 진행하면, cost 값이 정상적으로 나온다.\n",
    "* 이는, 초기 weight 값이 어떻게 주어지는지에 따라 운이 좋으면 발산하지 않게 되는 것이다.\n",
    "* 이제는 **weight의 초기값을 어떻게 설정하는지**에 대해서도 고민해야 한다.\n",
    "> 이전까지는 설명의 편의성을 위해 초기 weight값을 `random_normal`을 통해 설정했지만, 좋은 방법이 아니다. 초기값 설정의 중요성!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 값은 : 2.466254472732544\n",
      "cost 값은 : 2.326369285583496\n",
      "cost 값은 : 2.3451919555664062\n",
      "cost 값은 : 2.400777816772461\n",
      "cost 값은 : 2.349839210510254\n",
      "cost 값은 : 2.3939473628997803\n",
      "cost 값은 : 2.304363965988159\n",
      "cost 값은 : 2.3034632205963135\n",
      "cost 값은 : 2.3207085132598877\n",
      "cost 값은 : 2.304262638092041\n",
      "학습 끝\n",
      "정확도는 9.91000011563301%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) \n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "layer2= tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch = 30  \n",
    "batch_size = 100   \n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size) \n",
    "    cost_val = 0\n",
    "\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y})\n",
    "\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "            \n",
    "print(\"학습 끝\")\n",
    "\n",
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    \n",
    "correct= tf.equal(predict, tf.argmax(Y,1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels})\n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. 초기값 설정\n",
    "---\n",
    "> 초기값을 설정하는 방식에 따라 모델의 정확성이 달라진다. 따라서 최적의 초기값을 찾기 위한 연구가 진행중이다.\n",
    "---\n",
    "* 초기값을 무작위로 할당하는 방식은 학습 및 예측에 좋지 않다.\n",
    "* 초기값 설정 방식 : RBM ~ Xavier(2010년 즈음) ~ He's(2015년 즈음)\n",
    "* **Xavier** 방식을 사용한다.\n",
    "```\n",
    "# 기존의 초기값 설정방식\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]), name = \"weight1\")\n",
    "# Xavier 초기값 설정 방식\n",
    "W1 = tf.get_variable(\"weight1\", shape = [784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "cost 값은 : 0.265817791223526\n",
      "cost 값은 : 0.045113854110240936\n",
      "cost 값은 : 0.025331275537610054\n",
      "cost 값은 : 0.11332390457391739\n",
      "cost 값은 : 0.023575926199555397\n",
      "cost 값은 : 0.024161063134670258\n",
      "cost 값은 : 0.00599552970379591\n",
      "cost 값은 : 0.0028295035008341074\n",
      "cost 값은 : 0.000999674666672945\n",
      "cost 값은 : 0.003491774434223771\n",
      "학습 끝\n",
      "정확도는 98.01999926567078%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "# 모델 설정 : Relu + Xavier 초기값 설정\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) \n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\",\n",
    "                    shape = [784, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\",\n",
    "                    shape = [256, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "layer2= tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\",\n",
    "                    shape = [256, 10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch = 30  \n",
    "batch_size = 100   \n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size)  \n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y})\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "print(\"학습 끝\")\n",
    "\n",
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    \n",
    "correct= tf.equal(predict, tf.argmax(Y,1))   \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels})\n",
    "    \n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. AdamOptimizer\n",
    "* GradientDescentOptimizer보다 조금 더 정확하다고 알려진 알고리즘\n",
    "```\n",
    "# 기존의 경사하강법 알고리즘\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "# AdamOptimizer 알고리즘\n",
    "train = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)zer())\n",
    "```\n",
    "* 항상 더 높은 정확도를 보장하는 것은 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 값은 : 0.1014658734202385\n",
      "cost 값은 : 0.01694793999195099\n",
      "cost 값은 : 0.16454263031482697\n",
      "cost 값은 : 0.037584271281957626\n",
      "cost 값은 : 0.08709853887557983\n",
      "cost 값은 : 0.05138076841831207\n",
      "cost 값은 : 0.0625457689166069\n",
      "cost 값은 : 0.05042741447687149\n",
      "cost 값은 : 0.043750762939453125\n",
      "cost 값은 : 0.028329525142908096\n",
      "학습 끝\n",
      "정확도는 96.81000113487244%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "# 모델 설정 : Relu + Xavier 초기값 설정 + learning_rate 0.01(원래 0.1로 했다가 바꿈)\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) \n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\",\n",
    "                    shape = [784, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\",\n",
    "                    shape = [256, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "layer2= tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\",\n",
    "                    shape = [256, 10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch = 30  \n",
    "batch_size = 100   \n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size)  \n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y})\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "print(\"학습 끝\")\n",
    "\n",
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    \n",
    "correct= tf.equal(predict, tf.argmax(Y,1))   \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels})\n",
    "    \n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. drop out\n",
    "---\n",
    "과적합(Overfitting)\n",
    "> * 학습한 model이 training data set에는 \"최적화\"되어 있지만, test data set에 대해서는 잘 들어맞지 않는 상태.\n",
    "> * 일반적으로, 1) 학습한 model이 training data set에는 약 98% 이상의 정확도를 가지지만, 2) test data set에 정확도를 측정했을 대 85% 수준의 정확도가 나오는 상태를 지칭한다.\n",
    "\n",
    "---\n",
    "과적합을 피하기 위해서는\n",
    "> 1. 학습에 사용하는 training data set의 개수가 많아야 함.\n",
    "> 2. feature engineering : 1) 필요 없는 feature 제외, 2) 중복/유사한 feature 단일화.\n",
    "> 3. dropout\n",
    "> * 모든 node/perceptron이 학습, 예측에 참여하기 때문에 과적합이 발생한다는 가정에서 출발.\n",
    "> * 전부 다 이용하지 말고, 일정 비율의 node의 기능을 상실시킨다.\n",
    "\n",
    "---\n",
    "* dropout rate를 placeholder로 지정 : 기능 상실시킬 비율을 조정하기 위해 parameter 형성.\n",
    "\n",
    "```\n",
    "# placeholder 설정 : 인자이고 실수이므로, shape 지정하지 않아도 됨.\n",
    "dout_rate = tf.placeholder(dtype = tf.float32)\n",
    "# layer 2개 설정: 하나는 기능 상실 전, 하나는 기능 상실용.\n",
    "_layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(_layer1, rate = dout_rate)\n",
    "# feed_dict : dout_rate 지정\n",
    "feed_dict = {X : batch_x,\n",
    "             Y : batch_y,\n",
    "             dout_rate : 0.3})\n",
    "```\n",
    "\n",
    "* test 단계에서 accuracy 측정할 때에는 모든 데이터를 사용한다. 즉, 최종적인 정확도를 계산할 때에는 모든 node를 켜야 한다.\n",
    "\n",
    "```\n",
    "# result에서는 dropout 0으로 설정\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels,\n",
    "                               dout_rate : 0})\n",
    "```\n",
    "* tensorflow version에 따라 인자가 바뀐다.\n",
    "    * 1.5.0 : keep_prob(몇 퍼센트를 남길 것인지)\n",
    "    * 1.15.0 : rate(몇 퍼센트를 drop할 것인지)\n",
    "* dropout이 더 높은 정확성을 보장하지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 값은 : 0.3003239333629608\n",
      "cost 값은 : 0.18545137345790863\n",
      "cost 값은 : 0.07099945098161697\n",
      "cost 값은 : 0.17450176179409027\n",
      "cost 값은 : 0.11239916831254959\n",
      "cost 값은 : 0.10157787054777145\n",
      "cost 값은 : 0.09994788467884064\n",
      "cost 값은 : 0.19651824235916138\n",
      "cost 값은 : 0.03572039678692818\n",
      "cost 값은 : 0.04375575855374336\n",
      "학습 끝\n",
      "정확도는 98.18999767303467%입니다.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "# 모델 설정 : Relu + Xavier 초기값 설정 + dropout\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32) \n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "dout_rate = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\",\n",
    "                    shape = [784, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "layer1 = tf.nn.dropout(_layer1, rate = dout_rate)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\",\n",
    "                    shape = [256, 256],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, rate = dout_rate)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\",\n",
    "                    shape = [256, 10],\n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                labels = Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch = 30  \n",
    "batch_size = 100   \n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iteration = int(mnist.train.num_examples / batch_size)  \n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = {X : batch_x,\n",
    "                                                          Y : batch_y,\n",
    "                                                          dout_rate : 0.3})\n",
    "    if step % 3 == 0 :\n",
    "        print(f\"cost 값은 : {cost_val}\")\n",
    "print(\"학습 끝\")\n",
    "\n",
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)    \n",
    "correct= tf.equal(predict, tf.argmax(Y,1))   \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "result = sess.run(accuracy,\n",
    "                  feed_dict = {X : mnist.test.images,\n",
    "                               Y : mnist.test.labels,\n",
    "                               dout_rate : 0})\n",
    "    \n",
    "print(f\"정확도는 {result * 100}%입니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[gpu_env]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
